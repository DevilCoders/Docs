# Вежливость при обходе сайтов и порядок их обходе

## Вежливость (politeness)

Поисковой робот Яндекса придерживается правил вежливого обхода сайтов. Это означает, что качающий кластер имеет некоторые гарантированные ограничения на количество запросов к любому внешнему серверу в единицу времени. Допустимое количество запросов зависит от внешнего хоста/IP и выбрано таким образом, чтобы не слишком сильно нагрузить внешний сервер и, следовательно, не вызвать негодование вебмастера. С помощью экспертной оценки и некоторого накопленного опыта выбрана следующая дефолтная квота на скачивание с конкретного хоста - 1 запрос в 2 секунды. Для крупных хостов, тех, которые могут держать большую нагрузку, квота на скачивание превышает дефолтную, но обычно не превышает 10-15 запросов в секунду. Также при обходе учитывается, как отвечает сайт: если при обращении будет отдаваться большое число кодов ошибок 5XX или 429, робот воспримет это как сигнал о том, что хост испытывает затруднения с нагрузкой, и число обращений будет снижено. 

Обратите внимание, что с отдельными хостами могут существовать дополнительные договоренности по скорости обхода. К примеру, небольшие хосты, неспособные держать большую нагрузку, иногда обращаются с жалобами на число запросов, и количество обращений к ним может быть урезано до 1 запроса в 10-15 секунд. Крупные хосты также могут иметь дополнительные договоренности: к примеру, VK.com достаточно чувствителен к числу одновременных подключений, поэтому резко поднять число запросов к этому хосту не получится. 

При обходе сайтов учитываются указания в robots.txt сайтов. Это означает, что если та или иная страница запрещена в robots.txt, при попытке скачать ее Zora получит http-код ответа 1003, и скачать документ не получится. 

## Зачем это нужно?

Ресурсы принимающей стороны, то есть хостов, не бесконечны, и, как правило, очень сильно уступают возможностям наших роботов их индексировать. Если включить обход "на полную", это будет мало чем отличаться от DDOS-атаки. В таком случае у владельца сайта не останется другого выбора, кроме как использовать более жесткие методы блокировки, при которых установить соединение с сайтом не удастся совсем. К тому же, чрезмерно агрессивное индексирование может создать плохую репутацию ботам Яндекса, и большее число сайтов может заблокировать наших роботов совсем. 

## Как понять, что ограничения установлены на стороне сайта

1. **Запрет в robots.txt** (ошибка 1003)
Для проверки директив robots.txt можно использовать внешний инструмент [Анализ robots.txt](https://webmaster.yandex.ru/tools/robotstxt/). Введите адрес сайта и проверьте нужную страницу. Если страница запрещена, проверка покажет директиву, которая запрещает страницу. Для использования инструмента нужно быть залогиненым в Яндекс.ID.

{% cut "Пример проверки" %}

[![robots.txt_check](https://jing.yandex-team.ru/files/oddeye/browser_2V0swtNOCE.gif)](https://jing.yandex-team.ru/files/oddeye/browser_2V0swtNOCE.gif)

{% endcut %}

2. Стандартная ошибка HTTP

В отличие от запрета в robots.txt, ошибка HTTP может быть настроена только для роботов, или даже только для роботов Яндекса, которых можно определить по User-Agent, или, например, автономной сети. В первую очередь при получении такой ошибки стоит проверить страницы просто в браузере: если страницы не открываются или сообщают об ошибке при попытке посетить их "вживую", то вполне возможно, что они и правда были отключены на сайте. 

{% note info %}

По возможности, стоит проверять не из сетей Яндекса, т.к. бывают ситуации, когда все наши сети блокируются целиком, включая и сети пользователей Яндекса.

{% endnote %}

Если страницы открываются, но на запросы отдают код ошибки, стоит протестить, когда именно доступ недоступен. Например, можно при помощи **CURL** потестить, на какие запросы отдается ошибка. Зачастую блокировка совершается по User-Agent. В таком случае проверить можно, к примеру, при помощи инструмента [Bertal](https://bertal.ru/). Проверьте страницы с User-Agent Yandex и проверьте код ответа. После этого проверьте ту же страницу с другими User-Agent. Если код ответа меняется с 4XX/5XX на 200 OK при проверке других User-Agent, то это блокировка именно по User-Agent, и сайт сознательно блокирует наши запросы. 

3. **Ошибки 1001 и 1010**

4. **Ошибка DNS** (ошибка 1006)

## Можно ли обойти ограничения?

Проигнорировать указания, которые напрямую дает сайт, мы не можем. Если сайт сознательно отдает код ошибки или запрещает страницы к индексированию, посетить их не удастся. В такой ситуации, если у вас есть договоренности с сайтом, стоит обратиться к владельцам ресурса с просьбой разблокировать нужные страницы. Если договоренностей нет, в Роботе есть отдельный процесс для контактов с хостами с просьбами разбана. Чтобы сделать такую заявку, можно завести задачу через [форму для очереди ANTIBAN](https://st.yandex-team.ru/createTicket?queue=ANTIBANN&_form=75290). В задаче очень желательно указать, какую пользу получит сам сайт от такой разблокировки (например, больше трафика, больше продаж и т.д.). Нужно при этом учитывать, что успех такого контакта гарантировать, к сожалению, не получится: решение остается на усмотрение пользователя. Практика показывает, что шансы на успех не очень большие. 

{% note warning %}

Некоторые источники Zora позволяют посещать страницы с игнорирированием указаний в robots.txt. Пользоваться этим стоит с осторожностью и по отдельной договоренности с самим сайтом. Обратиться с таким запросом можно в [чат Zora Telegram](https://t.me/joinchat/CfwpTT8thXxCIp5TQi-CtQ)
Метод не рекомендованный, если есть договоренности, лучше просить разбанить страницы. 

{% endnote %}

## Как разбанить роботов

Если хост готов разблокировать наших ботов для индексирования, но не знает, как это сделать, у нас есть [пользовательская документация](https://yandex.ru/support/webmaster/robot-workings/check-yandex-robots.html), где перечислены все User-Agent наших роботов, а также методы, по которым их можно проверять, включая автономные сети, из которых робот совершает запросы: AS13238 и AS208722. 

Если хост не умеет определять роботов по этим данным, а требует список IP-адресов для добавления их в белый список, то запрос на список IP можно принести в таск [Zora-1696](https://st.yandex-team.ru/ZORA-1696). 

{% note warning %}

Это способ рекомендуется только как "крайнее средство", поскольку мы не можем гарантировать, что список наших сетей не изменится. 

{% endnote %}


