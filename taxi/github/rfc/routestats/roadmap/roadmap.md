# Развитие routestats

## Что за routestats?

Ручка клиентского приложения Go `/3.0/routestats` дергается с главного экрана
(экран с пином) и экрана саммари (экран с тарифами и кнопкой "Заказать").

## В каком сервисе находится ручка?

Верхнеуровневая схема работы есть на
[доске](https://miro.com/app/board/o9J_lBXgDYU=/) в Miro. Схема не
окончательная и будет меняться/дополняться.

Если коротко, то есть ручка в api-proxy, которая ходит в ручку `/v1/routestats`
сервиса routestats (uservices), а тот, в свою очередь, ходит в ручку
`routestats-internal` сервиса protocol (backend-cpp).

![image](static/workflow.png)

В ручке протокола происходит запрос цен, ETA, формирование основного и
альтернативных офферов, рендеринг ответа. В ручке нового сервиса ответ
кастомизируется с помощью системы плагинов:
[wiki](https://wiki.yandex-team.ru/taxi/backend/client-product/routestats-service/vvedenie-v-plaginy/).

## Зачем что-то развивать?

В текущей системе есть проблемы:
- Много нового функционала в приложении затрагивает экран саммари и, как
  следствие, ручку `routestats`. Не все фичи можно реализовать в текущей схеме
в новом сервисе, т.к. многое (в том числе, создание оффера) происходит все еще
в протоколе. Из-за этого новая функциональность добавляется в deprecated
репозиторий backend-cpp со всеми вытекающими: сложный, сильно связный код,
долгое тестирование, выкатки, проблемы старого фреймворка `fastcgi` и т.д.
- Критичная функциональность (об этом далее) не полностью отдельна от
  некритичной. Система плагинов позволяет игнорировать исключения в некритичных
плагинах, однако неаккуратный код в плагине может уронить весь сервис в корку,
что приведет и к недоступности критичного функционала. К слову, эта проблема
актуальна не только в проде, но периодически блокирует и тестирование, когда
расчет офферов недоступен из-за поломки какого-то нового, непротестированного
функционала.
- Нет понятных фоллбэков на все критичные источники (прайсинг, кандидаты,
  сурж). Есть фоллбэк для работы без прайсинга по нефиксе, но нужно его
допиливать.
- Высокие тайминги протокольного роутстатс могут объясняться не всегда
  оптимальным распараллеливанием затратных по времени операций похода по сети и
в БД.

## Что предлагается

В этом разделе предложения по работе в нескольких направлениях:
- Явное выделение критичной части routestats
- Выбор схемы взаимодействия критичной и некритичных частей

Предложения по фоллбэкам прорабатываются в отдельном разделе.

### Что является core-частью

Ключевой задачей является выделение критичной (core) части работы `routestats`.

Верхнеуровнево core-часть включает в себя:
- Поход в прайсинг за ценами
- Сохранение основного оффера в БД
- Формирование ответа для основного оффера

Предполагается выселение core-части в независимый сервис, см. [секцию про
core-часть и плагины](#взаимодействие-core-части-и-плагинов).

К некритичному функционалу, который можно вынести в плагины, предлагаю отнести
все остальное:
- Поход в driver-eta за ожидаемым временем подачи машины (ETA)
- Рассчет платной подачи
- Альтернативные офферы:
  - ExplicitAntisurge
  - Combo
  - PlusPromo
  - AltPins
- Создание пина в суржере (обсуждал с эффективностью - это ок вынести из core-части)
- Сохранение факта платной дороги для оффера
- Логика для корпов при походе в driver-eta
- Посылка события в антифрод (aka `NotifyAFS`)

#### Связь альтернативных офферов
В целом, альтернативные офферы не сильно зависят друг от друга, поэтому их
обработку можно будет распараллелить через независимые плагины.

Точки связи сейчас такие:
- Некоторые альтофферы не создаются, если уже созданы другие альтофферы. Тут
  стоит подумать, и может быть будет не сильно дорого создавать офферы, которые
потом не попадут в ответ. Но это, вероятно, позволит улучшить тайминги за счет
параллельной обработки. При этом часть лишней работы выполняется уже и сейчас в
протоколе, например, есть походы в прайсинг по альтпинам, при этом альтоффер
альтпинов может не попасть в ответ, потому что уже есть альтоффер другого типа.
- Создание пина в функции `CreatePin`. Сейчас эта функция принимает
  идентификаторы всех офферов. Но если пин будет создаваться в core-части после
создания основного оффера, то айдишники альтофферов нужно будет "досылать" в
суржер. Здесь стоит поговорить с эффективностью, насколько здесь важны
айдишники альтофферов. **Upd.** Договорились с эффективностью, что можно
создание пина вынести в плагин и не держать его в core-части.

#### Сервис офферов

Для того, чтобы создавать офферы из разных мест (core-часть и плагины), нужно
будет создать сервис офферов, который закроет за собой коллекцию
`dbprocessing.order_offers`. Сейчас в протоколе используется прямой поход в
базу + запись в файл для отправки данных оффера через LogBroker.

![image](static/uroutestats-plugins-protocol-offers-service.png)

Такой сервис также позволит делать фичи, которые будут читать данные из оффера.
До этого приходилось делать вспомогательные решения, например, передавать
данные оффера через redis для промоплашек tariffs-promotions.

Здесь же, вероятно, имеет смысл перевезти монгу офферов в облако. Иногда растут
тайминги на запись и ручка routestats отвечает очень долго (время в мс):
```
Time				total_time	create_offer_mongo_insert_time	stopwatch_name
Jul 16, 2021 @ 14:45:15.408	4,479.43	4,260.084			handle_routestats
Jul 16, 2021 @ 14:45:15.411	1,794.591	1,498.897			handle_routestats
Jul 16, 2021 @ 14:45:15.411	2,730.781	2,515.363			handle_routestats
Jul 16, 2021 @ 14:45:15.412	3,796.176	3,505.857			handle_routestats
Jul 16, 2021 @ 14:45:15.413	3,358.996	3,013.434			handle_routestats
Jul 16, 2021 @ 14:45:15.416	3,418.817	3,007.763			handle_routestats
Jul 16, 2021 @ 14:45:15.418	3,454.787	3,034.69			handle_routestats
Jul 16, 2021 @ 14:45:15.421	2,312.885	2,005.908			handle_routestats
Jul 16, 2021 @ 14:45:15.425	3,159.766	2,939.099			handle_routestats
Jul 16, 2021 @ 14:45:15.426	4,512.507	4,225.13			handle_routestats
Jul 16, 2021 @ 14:45:15.427	1,616.988	1,009.815			handle_routestats
Jul 16, 2021 @ 14:45:15.427	1,574.36	1,420.894			handle_routestats
Jul 16, 2021 @ 14:45:15.429	3,272.14	2,931.939			handle_routestats
Jul 16, 2021 @ 14:45:15.429	3,125.443	2,502.998			handle_routestats
Jul 16, 2021 @ 14:45:15.429	4,338.663	4,010.625			handle_routestats
```

#### Корпы и driver-eta

Перед походом в сервис driver-eta для корпоративного метода оплаты протокольный
routestats собирает дополнительную информацию, которая, судя по всему,
используется для дополнительных фильтров при поиске кандидатов.

Там две фичи: виртуальные тарифы и логистические контракты. Последние делались
в [этом ПР](https://github.yandex-team.ru/taxi/backend-cpp/pull/12178). И есть
тикет на то, чтобы это, вероятно, выпилить:
[CARGODEV-3187](https://st.yandex-team.ru/CARGODEV-3187).

Сейчас, кажется, эта информация вообще никогда не передается: [ссылка на
логи](https://kibana.taxi.yandex-team.ru/goto/551412eafd3b67cb6bf563d4083ef12a).

А сама фича с логистическими контрактами не работает из-за ошибок в сервисе
`corp-integration-api`: [ссылка на
логи](https://kibana.taxi.yandex-team.ru/goto/ff489237fbee370a2b8185b46bbb4ef8).

Можно договориться о выпиле, либо о переносе этой логики в сервис driver-eta.
Кажется нелогичным, что каждый потребитель сервиса driver-eta должен
самостоятельно собрать мету для фильтров кандидатов.

#### Посылка события в антифрод
Передаются айдишники пользователя и факт похода в ручку. Можно вынести в
плагин, либо на уровень `api-proxy`, но тогда потребуется лишний поход в сервис
`user-api`.

### Взаимодействие core-части и плагинов

Схематично взаимодействие основного обработчика ручки `/v1/routestats`, который
ходит в сервис protocol за core-частью ответа, и плагинов в сервисе routestats
сейчас выглядит следующим образом.

![image](static/uroutestats-plugins-protocol.png)

Видно, что есть набор хуков, через которые основной обработчик запроса общается
с плагинами. Данные могут передаваться в обе стороны.

В настоящий момент core-часть по сути находится в сервисе protocol, но будет
вынесена в сервис на фреймворке `uservices`. Самый простой и, на первый взгляд,
логичный вариант - перенести core-часть в сервис routestats. Сначала будем
предполагать, что так и сделали.

Далее рассмотрим, какие достоинства и недостатки есть у текущей архитектуры
взаимодействия core-части и плагинов и какие у альтернативных подходов.

#### Core-часть и плагины в сервисе routestats (текущий вариант)

Текущий вариант взаимодействия плагинов с core-частью. Общение происходит через
вызов функций в C++ коде.

![image](static/uroutestats-plugins-same-service.png)

Плюсы:
- Есть готовый фреймворк с работающими примерами.
- Плагины можно переиспользовать для разных ручек (?)
- Нет удаленных вызовов (RPC) для взаимодействия с плагинами.
- Есть документация:
  [ссылка](https://wiki.yandex-team.ru/taxi/backend/client-product/routestats-service/).

Минусы:
- Один плагин может коркнуть весь сервис. Невозможно запустить код плагина в
  неком sandbox-окружении, который не сможет уронить весь процесс сервиса. В
других языках, вероятно, было бы проще с ловлей всех исключений, но везде
какие-то C-модули/unsafe и т.д., так что абсолютной защиты никто предоставить
не сможет.
- Вся логика плагинов в одном сервисе - много тестов. Но пытаемся поддерживать
  правильную пирамиду тестирования с преобладанием юнит-тестов, чтобы тесты
проходили быстро.

Итого: основная проблема здесь - некритичные плагины могут сложить критичный
функционал.

#### Core-часть и плагины в сервисе routestats, плагины в отдельном процессе

Можно решить проблему влияния некритичного функционала на критичный, разделив
обработку запросов к ручке `/v1/routestats` на два отдельно живущих процесса: в
одном живет core-часть, в другом - плагины.

В таком случае нужно реализовать общение между core-частью и плагинами через
межпроцессорное взаимодействие. При этом во вспомогательном процессе нужно либо
создавать долгоживущие корутины с контекстом, который сохраняется на протяжении
всего времени обработки исходного запроса в routestats, либо в основном
процессе накапливать контекст и передавать его целиком в плагин в каждом хуке,
тогда вызовы хуков могут порождать независимые корутины.

![image](static/uroutestats-plugins-separate-process.png)

Плюсы:
- Изолируем критичную часть routestats от плагинов на уровне процессов.

Минусы:
- Нужно писать межпроцессное взаимодействие (через unix-сокеты?), придумать
  способ поддержания контекста обработки запроса в процессе с плагинами.
- Нет изоляции на уровне ресурсов (CPU, etc.). Например, если процесс с
  плагинами загонит CPU в полку, то и основной процесс с core-частью будет
страдать.

Итого: основная сложность, на мой взгляд, в написании взаимодействия между
процессами. Скорее всего, усложнит разработку самих плагинов.

#### Плагины во внешних сервисах + http

Для полной изоляции core-части от плагинов можно выселить плагины в отдельный
сервис или сервисы, общение с плагинами будет осуществляться http-запросами по
сети. Флоу взаимодействия core-части c плагинами (на какие хуки подписан
плагин) можно будет описать, например, на языке AGL.

![image](static/uroutestats-plugins-separate-services.png)

Плюсы:
- Изолируем критичную часть routestats от плагинов полностью.
- Не нужно писать взаимодействие через сокеты, используем привычный интерфейс в
  виде http-запросов.

Минусы:
- Нет фреймворка в виде кода, взаимодействие через API.
- Не для всех плагинов понятно, в какой сервис их отселить.
- Добавляем latency из-за сетевых походов.
- Большое число сетевых походов при обработке одного запроса routestats: в
  худшем случае (N x M), где N - число плагинов, M - число хуков.
- Нужно передавать накопленный контекст во всех запросах - большие тела
  запросов и ответов, увеличиваем потребление сети.

Итого: большие риски увеличения таймингов ручки routestats, невозможности
масштабирования из-за большого числа сетевых походов.

#### Core-часть во внешнем сервисе

Выносим логику core-части в отдельный (новый) сервис, например, routestats-core.

Этот сервис умеет отдавать ответ ручки routestats в готовом для клиентов виде,
так же как это сейчас делает протокол. В таком случае можно использовать этот
core-сервис в качестве фоллбэка, если сломался сервис routestats с плагинами.

![image](static/uroutestats-plugins-core-service.png)

Плюсы:
- Изолируем критичную часть routestats от плагинов полностью.
- Получаем фоллбэк "из коробки".
- Не нужно писать взаимодействие через сокеты, используем привычный интерфейс в
  виде http-запросов.

Минусы:

- Есть latency на сетевой поход из сервиса routestats в routestats-core.

#### Выбранный вариант

Предлагаю распил протокольного routestats разбить на два этапа.

На первом этапе предлагаю остановиться на последнем описанном варианте -
выделении core-части в отдельный сервис, который можно будет использовать в
качестве фоллбэка, если сломался основной сервис routestats.

Этот вариант имеет большой плюс в том, что систему плагинов сильно переделывать
не придется (кроме хуков про создание оффера, но здесь в любой случае что-то
нужно будет придумывать).

У этого подхода, однако, есть минус в том, что если ломается основной сервис
routestats, то потенциально отключаются все плагины, которые не в core-части.
Такая деградация функциональности может негативно восприниматься пользователями
и создает единую точку отказа многих фич.

Поэтому на втором этапе предлагаю попробовать вариант с вынесением плагинов в
отдельные процессы. Это позволит сделать плагины более изолированными, отдельно
отключаемыми, потенциально позволит писать плагины не только на C++. Для этого
нужно будет сделать прототип - вынести один из плагинов в отдельный процесс и
посмотреть, насколько легко с таким плагином будет работать (разработка, логи,
мониторинги, автофоллбэки, производительность). Если прототип будет успешным -
выносить все плагины в изолированные процессы.

## Фоллбэки

### routestats-fallback
Сейчас есть фоллбэк, включающийся при недоступности основной ручки
`/v1/routestats` и протокольной ручки `/internal/routestats`. Подробнее о нем
[на
вики](https://wiki.yandex-team.ru/taxi/backend/client-product/routestats-service/fallback-routestats/).

Данный фоллбэк не задействует прайсинг, а занимается расчетом минимальной цены
самостоятельно. Также учитывает сурж. Вероятно, стоит забирать минимальные цены
из `pricing-fallback`.

Нужно подумать про дальнейшее развитие этого фоллбэка после выделения
core-части и плагинов.

В core-части будет две точки отказа: прайсинг и сервис офферов.

### Сурж в легковесном routestats

Сейчас легковесный routestats зависит от значения суржа. Если сервис(ы) суржа
не отвечают - ручка будет 500тить. Есть вариант сделать фоллбэк в виде карты
суржа (через кэш). Значение суржа будет не таким точным, но это лучше, чем
вообще не отвечать на запрос в ручку routestats.

## Что делать с estimate (int-api)

Ручка `estimate` - аналог ручки `routestats` для внешних интеграций, которые
создают заказы Такси. Сюда относится все, что не является нативным мобильным
приложением:
- колл-центр
- корпоративные заказы
- турбоапп
- алиса
- ...

Ручка живет в сервисе integration-api, который сильно связан с протоколом. Флоу
обработки запросов на создание оффера там менее богатый, чем в routestats.
Какие-то фичи из коробки не работают, если их отдельно не продублировали для
протокола и int-api.

Предлагается для таких потребителей создавать отдельные ручки в сервисе
routestats, а в этих ручках переиспользовать плагины основной ручки, которой
пользуются нативные мобильные приложения. В таком подходе можно будет выборочно
подключать плагины, необходимые для конкретного потребителя и при этом
кастомизировать логику формирования ответа.

Для переиспользования плагинов между ручками внутри сервиса routestats
потребуется доработка системы плагинов.

## Roadmap

Далее диаграмма Ганта по различным задачам с примерными сроками и учетом
зависимостей между задачами.

Задачи распараллелены без учета доступных ресурсов разработки, а просто из
соображения блокирующих зависимостей.

Для описания стрима по переносу потребителей int-api на сервис routestats
колл-центр (КЦ) выбран в качестве примера.

![image](static/roadmap-gantt.png)
