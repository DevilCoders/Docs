## Цель

Создать сервис, который позволит разгрузить polling-ручки или вовсе избавиться от них.



## Требования
### Водительские приложения
**Как может использоваться сервис:**
* **coord-control**. Отдельный кэш для etag в микросервисе contractor-orders-polling, инкрементальная ручка для обновления кэша etag + внутренняя ручка для получения данных. Заменяется хорошо.
* **reposition**. 3 прямые ручки на 30к+ RPS, вызываемые напрямую из driver-protocol -> contractor-orders-polling. Можно поддержать event-based, если реализовать события про логин-разлогин и подписаться на них в reposition
* **airport-queue**. TBD, Егор
* **driver-ui-profile**. TBD, Дима
* Части /driver/polling/state
  * представляющие собой старые конфиги (не очень важно)
  * представляющие собой кэши на базами.
    * Кэш над базой, на основании которого создаётся состояние, можно перевести на событийную модель, подписавшись на апдейты в БД 

**Функции сервиса:**
* Сервис должен принимать данные от других сервисов, предназначенные определенному Клиенту
* Сервис должен принимать данные от других сервисов, предназначенные всем Клиентам/Клиентам в определенном разрезе (примеры в рамках Такси -- парк?геозона?). Опциально для реализации в MVP, но стоит предусмотреть в протоколе. Как вариант -- разные топики для отправки сообщений в них.
* (отказались) Сервис должен быть format-agnostic, т.е. уметь хранить и выдавать данные независимо от формата (предполагается, что основной -- json)
* Сервис должен быть driver-agnostic, т.е. не иметь явных завязок на текущие каналы доставки данных в рамках партнерки/Такси.
* Сервис должен надёжно хранить принимаемые данные. Варианты политики хранения:
  * До подтверждения получения их таксометром
  * _(если не реализован предыдущий вариант)_ Х минут
  * _(отказались)_ N последних "сообщений"
  * _(отказались)_ Возможные кейсы, когда нужны данные - перезапуск таксометра, получение последнего опубликованного сообщения, фоллбэк на поллинг
* Сервис должен обеспечивать дедупликацию сообщений на своей стороне, чтобы избегать лишнего трафика до таксометра
* Сервис должен поддерживать приоритет и TTL сообщений
* Сервис должен доставлять до "Таксометра" получаемые данные с небольшой задержкой (определить SLA)
* Сервис должен иметь метрики/статистику по хранению данные в разрезе каждого отдельного поставщика
  * Предполагается, что конечные сервисы будут отправлять сервис законченное сообщение для Таксометра, и оно может быть, в целом, любых размеров

**Что не хочется видеть в сервисе по доставке данных:**
* Хочется сфокусироваться на хранении и доставке данных, сократив количество внешних зависимостей и возможных точек изменения сервиса
* Например, не хочется добавлять в него "агрегированную ручку" / "службу одного окна" для проксирования запросов в низлежащие сервисы
* Также, не хочется, чтобы он сам ходил в какие-то продуктовые сервисы за данными. 

**Что нужно будет сделать в будущем:**
(не в рамках текущего RFC)
* (отменилось) Доставка событий до других сервисов (пример: если у водителя поменялся статус, то уведоми об этом сервис режимов работы) - _не будет делаться в сервисе, оставил для истории_


### Клиентские приложения
**Как может использоваться сервис:**
Есть несколько polling-ручек, которые можно улучшить:
* rps до нескольких тысяч
* цель - разбить большие ручки по области ответственности, по критичности, по частоте изменения данных.
* ручки будут распилены на несколько, клиентам будет сообщаться в каких ручках какие данные можно забрать.
* будет единая поллинг-ручка, которая сообщит в каких ручках произошли изменения. Клиенты будут ходить в нее, а затем в остальные ручки
* пушинг как альтернатива не рассматривался
* хранить state в едином месте не планировалось, каждый сервис сам отвечает за свой state


Подробнее:
https://github.yandex-team.ru/taxi/rfc/blob/master/protocol/totw_decomposition/overview.md
https://github.yandex-team.ru/taxi/rfc/blob/master/superorders/stage_1.md


### Ресторанные приложения
**Как может использоваться сервис:**
* в ресторанном бэке есть несколько (3-6) polling-ручек, rps порядка 10-100. Из них минимум 2 можно улучшить, планируется сделать это с помощью etag'ов
* есть back2back ручка экспериментов с rps=200, но ее предположительно будут решать кешем
* была проблемная ручка с заказами (до 100 rps), мобильные клиенты при уходе в background не могли ее дернуть и до них не доходили websocket-пуши. Полечили переходом на нативные пуши, стало 10 rps
* ответ ручек может быть достаточно большим (10Кб и больше)
* бэкенд умеет генерировать события, эти события можно слушать чтобы понять что в ручках изменился ответ. События кладутся в RabbitMQ.

Подробнее: https://st.yandex-team.ru/EDADEV-38993


### Другие b2b
**Как может использоваться сервис:**
По информации от Димы Курилова, в b2b есть процессы на основе ProcaaS, которые инициируются клиентом, работают асинхронно (в ProcaaS), возвращают клиенту результат.
У них те же проблемы:
* Результат на клиента доставляется поллингом
* Надо каждый раз писать поллинг-ручку и обвязку к ней
* Есть задержка в доставке результата

Пример: сервис штрафов https://wiki.yandex-team.ru/taxi/backend/architecture/taxi-order-fines/


## Архитектура
Термины:
* _client-events_ - название разрабатываемого сервиса.
* _клиент_ - конкретная установка клиентского приложения (таксометр, яндекс.go и т.д.).
* _топик_ - группа клиентских приложений.
* _channel_ - клиент или топик, обозначает получателя.
* _state_ - какие-либо данные, которые надо передать клиенту.
* _etag_ - версия state, представляет собой uuid4-строку
* _событие_ - сигнал о том, что изменился state. Состоит из `{event, event_id, channel, ttl, state}` (не более 4 кб). 
* _pull_ - передача данных от сервера к клиенту по инициативе клиента. Реализуется, например, через HTTP-запросы.
* _push_ - передача данных от сервера к клиенту по инициативе сервера. Реализуется через каналы FCM/APNS/HMS (нативные пуши) и WebSocket (ws-пуши).

![Архитектура](https://jing.yandex-team.ru/files/v-belikov/client-events%20%281%29.svg)

Пояснения к диаграмме:
1. Сервисы генерируют _события_ и отправляют их в _client-events_

2. _client-events_ вставляет (или обновляет) в СУБД запись события `{event, event_id, channel, state, etag}` (ключ - `{event, event_id, channel}`).

3. Если у _события_ включена форсированная доставка изменений клиенту, _client-events_ отправляет изменения через _push_ с новым _state_.

4. _клиент_ приходит в _client-events_ чтобы получить список _событий_, их текущие _etag_ и _state_. Получая этот список, клиент обрабатывает каждый его элемент так же, как пуши.
_Клиент_ ходит в эту ручку на случай если какие-то _события_ не дошли либо чтобы получить события, отправленные на топик (не все транспорты пушей поддерживают топики).
_client-events_ возвращает _state_ только тех событий, у которых отличается _etag_ (либо которые отсутствуют в запросе). По умлочанию клиент ходит в ручку редко (например, раз в минуту), но может уменьшит интервал поллинга если обнаржут проблемы:
    * Последний пуш приходил больше N1 минут назад
    * WebSocket-соединение к Xiva разорвано более N2 минут назад
    * Xiva не присылала heartbeat через WebSocket-соединение более N3 минут назад
    * (только для Android) Не приходило heartbeat через специальный FCM-топик более N4 минут назад

5. _client-events_ читает список _событий_, доступных _клиенту_, и возвращает их.

6. _client-events_ удаляет из СУБД информацию о _событиях_, у которых истек TTL


## Дальнейшее развитие
1. Сервис _client-events_ при получении _события_ может извлекать _state_ запросом в специальную _state-ручку_, а не вместе с событием. Это может быть полезно в двух случаях:
    * В месте, где генерируется событие, может не быть всех необходимых данных чтобы сформировать _state_. Например, может потребоваться сходить в другие сервисы, в кеши и т.д.
    * Клиент может использовать _state-ручку_ как фолбэк если сервис _client-events_ недоступен. Может быть полезно для особо критичных данных.

2. Дефолтные значения для событий. Клиент может потребовать, чтобы событие всегда существовало. Если оно не было сгенерировано для клиента, может потребоваться какой-то дефолт. Удобно задавать дефолты на бэкенде.

3. Зависимые события. При генерации одного события, может потребоваться изменить другие. Сервис _client-events_ может вычислить изменившиеся события и что-то с ними сделать: удалить последний _state_, извлечь _state_ из _state-ручек_.

4. Буферизация событий. Можно указать, что пуш отправляется клиенту не чаще, чем раз в N секунд. В течение этого времени события буферизуются. Это позволит избежать шторма событий, когда одно событие (например, изменение параметров заказа) может порождать множество других, в результате чего клиент делает много запросов вместо одного.

5. Изменение протокола для общения с клиентом чтобы передавать _state_ больше 4кб и/или делать это менее энергозатратно:
    * Можно отправлять клиенту пуш только с `{event, event_id, etag}` (без _state_) чтобы клиент знал, что событие изменилось и немедленно сходил в _pull_.
    * Можно сжимать _state_ или передавать только diff с предыдущим чтобы увеличить максимальный размер _state_
    * Можон перейти на бинарный протокол: в _pull_ на grpc или подобный, в _push_ можно сделать собственную реализацию вместо Xiva.

6. Получение ack на пуши от клиентов чтобы измерять процент и скорость доставки

7. Логирование в YT событий и фактов доставки.

8. Ручка для удаления события. Пример: водитель разлогинился - надо удалить все сохраненные. Можно сделать через зависимые события (см. п.3).



## Рассмотренные проблемы

1. Стоит ли долгосрочно хранить последний присланный state и отдавать его клиенту вместо специальной state-ручки?

   _Достоинства:_
      * Будет только один нагруженный polling-сервис вместо множества

   _Проблемы:_
      * Допустим, есть редкое событие. Выкатили версию сервиса, который генерирует это событие. Версия содержит ошибку, из-за которой структура события заполняется неправильно и клиент крэшится. Сервис откатили, но кривые события остались в базе и отдаются клиентам. Как их удалить? Кто должен этим заниматься?
      * Похожая ситуация: что делать если нужно изменить структуру события обратнонесовместимым образом?

   _Решения:_
      * Сервис не должен заниматься долговременным хранением state'ов
      * (Алексей Яговкин): Тут только принудительное перемещение курсора по timestamp на то время, после которого не было поставки плохих событий. Можно конечно развить эту тему в сторону простановки стримов поставки и уметь пропускать записи конкретного поставщика до указанного timestamp.



2. Стоит ли хранить историю событий чтобы отдавать ее клиентам?

   _Достоинства:_
      * Может пригодиться в каких-то кейсах (история чатов)
      * Таких кейсов на практике сейчас нет

   _Проблемы:_
      * Проблемы те же, что и в предыдущем вопросе
      * В целом делать одно хранилище для множества сервисов - неразумная идея: будет фейлиться "все или ничего" (например, новые версии клиента начали активно выкачивать историю одного события и завалили доставку других событий), сложно управлять квотами на ресурсы хранилища и т.д.

   _Решения:_
      * Сервис не должен хранить историю

   Отказались от требования т.к. у заказчиков не нашлось кейсов.


3. Шторм событий

   _Проблемы:_
      * разные события могут приводить к хождению в одну и ту же state-ручку и генерировать неадекватную нагрузку.

   _Решения:_
      * Можно агрегировать связанные события на бэкенде в течение короткого времени, отправлять на клиент только одно итоговое событие
      * Можно сделать "период остывания" для ручки: не присылать обновления чаще, чем раз в N секунд


4. Должен ли клиент получать от бэкенда всегда полный state или он должен вычислять state самостоятельно на основе событий?

   Допустим, есть заказ, у заказа есть свойства:
      * статус
      * точка подачи
      * и т.д.

   Надо ли при изменении любого поля передавать весь заказ или только одно поле?

   _Достоинства:_
      * Логика на бэкенде упрощается

   _Проблемы:_
      * Где-то надо собрать весь state. Если события содержат только измененные данные, то это делается на клиенте. Если все поля, то на бэке.
      * Вариант собирать на фронте сложнее отлаживать и дорабатывать

   _Решения:_
      Не давать клиенту собирать state из кусочков, отдавать полный state с бэкенда.
      В событиях передавать либо новый state полностью, либо передавать state через ручку, а в событии только название и аргументы к ручке.


5. Нужна ли гарантия упорядоченности событий?

   Обсудили, что требование не актуально.


6. Стоит ли клиентам всегда брать данные из событий или нужно делать событие + state-ручку?

   _Проблемы:_
      * Что делать если события какого-то типа еще нет? Откуда клиенту взять данные этого типа?
      * Если из state-ручки, то зачем нужно дублировать данные в событии? Клиенту придется учиться брать данные сразу из двух источников, нет гарантии что у них совпадает структура


7. Какой должен быть максимальный размер события?

   _Проблемы:_
      * В нативные пуши вмещается не более 4Кб данных. Если хочется доставлять данные события пушами, то желательно влезть в этот лимит.

   _Решения:_
      * Посылать в пуше только информацию об изменившейся ручке (без контента ручки)
      * Сжимать содержимое
      * Присылать только измененные данные


8. Холодный старт таксометра

   _Проблемы:_
      * При холодном старте таксометр ходит во множество сервисов, нагружая их. 

   _Решения:_
      * Игнорировать проблему - она не слишком насущна
      * Хранить на такосметре кеш ответов, читать что изменилось через polling-ручку
      * Хранить все последние ответы ручек в polling-сервисе и отдавать из него же, разгружая другие сервисы


9. Можно ли использовать Xiva WebSockets + Xiva Reverse Proxy в качестве транспорта пушей и для получения ack?
   Скорее всего, можно


10. Стоит ли использовать бинарный протокол для общения с клиентом?
    * В первой версии точно не стоит т.к. это большая доработка.
    * Бинарный протокол можон использовать в ручке /pull, но надо доказать что это что-то улучшит. Можно провести тест на следующих этапах.
    * Xiva, FCM, APNS, HMS не поддерживают передачу бинарных данных, а если кодировать в base64, то теряется смысл. Можно сделать замену xiva/websocket, но только если будет подтверждено что есть заметное улучшение.