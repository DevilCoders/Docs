## Хранение разметки аудитории

Сейчас получение публикации для конкретного пользователя выглядит так:
1. С клиента приходит запрос в `inapp-communications`.
2. В `inapp-communications` есть кэш коммуникаций. В нём мы ищем, какие коммуникации подходят пользователю.
3. Подошедшие коммуникации возвращаются на клиент.

Если мы не будем использовать механизм экспериментов, нам могли бы подойти такие варианты:

1. Использовать сервис тегов. 

Можно связать с каждой коммуникацией тег и узнавать у сервиса тегов, что подходит данному пользователю.

Скорее всего, не будем использовать такой вариант, потому что сервис тегов нужен для других сценариев (обычно для тегов, 
используемых в нескольких сервисах. Например, тег, что пользователь - фродер). Для использования разметки в одном сервисе 
есть другие решения.
Также нам бы потребовался отдельный инстанс сервиса, чтобы при слишком больших раскатанных кампаниях 
никому не помешать. Поднять отдельный инстанс может быть не быстрее, чем написать простой подобный сервис.
В сервисе тегов может не хватать гибкости для нас: есть то, что мы не будем использовать, 
и может не быть того, что потребуется.

2. Хранить в сервисе. Не хватит места для наших объёмов аудитории, не рассматриваем.

3. Попробовать использовать `feeds`, где похожая задача решалась. В `feeds` решение пока для меньших объёмов аудитории, переезд сложен. 
Не рассматриваем.

4. Использовать базу, в которой будет храниться связь идентификатора или идентификаторов пользователя с идентификатором кампании. 
База будет заполняться при создании кампании.

Пока этот вариант выглядит предпочтительным.

### Возможный размер аудитории

В декабре 2021 года недельное количество активных пользователей - около 20 млн.
С расчётом на то, что 50% наших поездок к 2024 году должна составить международка - считаем, что недельная аудитория сервиса - 40 млн активных
пользователей. 
Посчитаю, что большая часть маркетинговых кампаний займёт время не больше недели, скорее, меньше. И точно не должна занять больше месяца. 
На аномально долгие кампании сделаю запас в 10 млн пользователей.

Всего максимальная аудитория кампаний - 50 млн пользователей.

Посчитаем, что в момент, когда мы уже научились запускать все коммуникации через CRM, на каждого пользователя будет запущена витрина. 
Плюс предположим, что в один момент дополнительно запущена одна большая кампания (максимальная аудитория фуллскрина из недавнего времени - 
20 лет Такси, около 22 млн пользователей) и до 10 маленьких и средних кампаний. Если локальные кампании - до 5 млн человек (с запасом, скорее всего,
размер кампаний будет меньше), то суммарно будет 50млн + 25млн + 5млн * 10 = 125 млн активных коммуникаций. 

Заложу на 20% больше на случай, если что-то не учла, тогда всего ожидается до 150 млн активных коммуникаций одновременно.

В размер базы и размер операций над базой преобразуется так:

150 млн * 150б(размер строчки) = 20Гб - ожидаемый размер базы.

Максимальный размер INSERT: около 25 млн записей (по размеру самой большой кампании);
Максимальный размер UPDATE, DELETE: около 25 млн записей;
Ожидаемый размер SELECT - до нескольких записей (выбор по идентификатору пользователя).
Нужен индекс по используемым идентификаторам пользователя.

Другой подход к расчёту:

Попробую посчитать максимальный размер базы. Буду исходить из предположения, что аудитория мало выбирается специально, 
старые коммуникации, раскатанные на долгий период, висят до года, и на каждого активного пользователя сервиса одновременно раскатано по 10 маркетинговых коммуникаций.

Сейчас в год на главный экран заходит около 75 млн пользователей. Допустим, что это число увеличится в 2 раза 
(за счёт роста сервиса и международки, хоть международка и в yango). Если на каждого активного пользователя в любой момент раскатано 10 коммуникаций, 
то количетсво записей в базе - около 1,500 млн. Тогда ожидаемый максимальный размер базы - 200Гб.

### Выбор базы для хранения разметки аудитории

Предполагаются такие операции над данными:

- добавление аудитории для новой публикации. Может происходить долго, это не проблема, так как кампания может не публиковаться мгновенно.
Это процесс, который планируется заранее;
- получение идентификатора публикации по идентификатору пользователя. Чем быстрее, тем лучше;
- снятие публикации с части аудитории. Может происходить как обновление либо как создание новой аудитории с новым идентифиикатором. 
Время, за которое это должно происходить, уточняется;
- удаление неактуальной аудитории. Может происходить долго, но важно не мешать другим операциям.

Какие базы рассматриваем:
1. Postgres.

Плюсы:
- умеем с ней работать;
- отличная поддержка в Такси.

Минусы:
- update и delete медленные и влияют на работу базы, а нам точно придётся часто их делать.
- сильная сторона базы - сложные транзакции - в наших сценариях не нужна. 
У нас очень простая схема базы, простые операции. Возможно, для нас это неоптимальный вариант.

Текущие объёмы (до 5 млн одновременных пользователей) не должны вызывать проблем.
Если объёмы вырастут, например, до 100 млн - под вопросом.

2. YDB

Плюсы:
- просто и почти неограниченно масштабируется, проверено на  больших проектах в Яндексе (Алиса)
- быстрые запросы по ключу (до 10мс при стандартных настройках, 1-2мс при ослаблении гарантий);
- нет проблем при удалениях большого количества данных, в отличие от Postgres.

Минусы:
- нет опыта успешного использования в клиентском продукте Такси;
- не умеем с ней работать, например, следить за состоянием базы;
- слабая поддержка в uservices;
- возможно, качество продукта пока не идеальное.

Пример использования в Такси: сервис `contractor-mentorship` (в команде Pro).
Пример кода, использующего базу: https://a.yandex-team.ru/arc/trunk/arcadia/taxi/uservices/services/contractor-mentorship/src/db/mentorships.cpp?rev=r8889060#L34 

Есть serverless режим, рекомендован для тестов и экспериментов. Может пригодиться. Для прода он не рекомендован.

3. YT

Близко по плюсам и минусам к YDB, но поддержка в uservices лучше, тайминги хуже.

Предварительно выбрали YDB.
