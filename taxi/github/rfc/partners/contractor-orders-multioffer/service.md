**Сервис contractor-order-multioffer**

Он же `c-o-m`, `COM`

**Какую продуктовую проблему решает сервис?**

* Для водителей из Убера нет штрафов в Активность за пропуски заказов, поэтому водители гораздо больше
 перебирают, в итоге низкий Acceptance Rate (`AR`) и плохая эффективность (потому что это приводит к реордеру и 
 повышению времени на поиск исполнителя)
* Сейчас не назначаются заказы на водителей в статусе "Занят".
Мы хотим уметь такие заказы раздавать, чтобы снижать горение и сурж. Конечно, за отказы мы не штрафуем Активностью, 
поэтому `AR` в таких заказах очень низкий, и эффективность тоже плохая.

Суммарно -- мы хотим для той части заказов, на которой подбираются "плохие" по `AR` исполнители, 
использовать мультиоффер-подбор исполнителя.

**Почему нельзя решить эту задачу без разработки нового сервиса существующими решениями?**

Все текущие сервисы диспатча работают в концепции "один заказ - один водитель".

Тажке, в них нет и не будет взаимодействия с водителями -- т.е. отправки запросов на участие в борьбе за заказ и сбор подтверждений/отмен.

Также, хочется сделать изолированный механизм, не связанный с текущими -- это подходит под логику диспатчей, когда они вызываются по цепочке.

**Как именно сервис будет решать поставленные перед ним задачи?**

Сервис интегрируется в механизмы `lookup + dispatch` и представляет собой отдельный вариант назначения водителей.
Также, сервис занимается взаимодействием с водителями - рассылка пушей + сбор ответов от водителей. 
* **Часть диспатча** (мультиоффер-диспатч)
  * Запускается из `lookup` в зависимости от настроек (для определенных зон итд)
  * Мультиоффер-диспатч в списке диспатчей находится выше буферного и прямого, поэтому потенциально через него будет проходить много заказов
  * Мультиоффер-диспатч получает список кандидатов на заказ и сортирует их
  * Мультиоффер-диспатч работает в режиме быстрой проверки - проверяет по сортированному списку кандидатов, что заказ ему 
  интересен, а иначе возвращает управление в лукап для перехода к следующему диспатчу
  * Мультиоффер-диспатч запускает отдельный процесс для `розыгрыша заказа` и возвращает управление в лукап
* **Часть розыгрыша заказа**
  * Запускается из `мультиоффер-диспатча`
  * Рассылает мультиофферы выбранным водителям
  * Для них будет открываться экран, очень схожий с экраном нового заказа
  * Собирает оки/отказы в борьбе за заказ
  * После сбора всех оков или истечения таймаута выбирает лучшего кандидата
  * Возвращает (заказ, кандидат) в lookup через callback

Для Таксометра/Убера выставляются внешние ручки (водительская авторизация):
* `/driver/v1/order-multioffer/multioffer/accept` -- принять запрос на участие в мультиоффере
* `/driver/v1/order-multioffer/multioffer/decline` -- отклонить запрос на участие в мультиоффере

Для Таксометра/Убера будет новый пуш:
* запрос на мультиоффер -- в содержимом будет информация о заказе (сеткар) + дополнительная информация про мультиоффер

Внутренние ручки:
* `/v1/contractor-for-order`: принять заказ в мультиоффер - вызывается из `lookup`

**Разрабатывается один сервис или система?**

Разрабатывается сервис.

**С кем взаимодействует сервис?**

* Таксометр/Убер -- пуши + новые ручки
* lookup, candidates, driver-ordering - для мультиоффер-диспатча
* driver-orders-builder - для сборки сеткара для отправки пушей
* communications - для рассылки пушей

Сервисы:
1. Сервиc `lookup`
Взаимодействие в рамках поиска кандидата на заказ.

Как используется?
Сервис `lookup` вызывает `contractor-orders-multi`, передавая ему заказ для назначения.

Как гарантируется идемпотентность при запросах?
Будет использоваться токен идемпотентности, подробнее знают разработчики сервиса (выходит за рамки данного документа).

Как поддерживаются отказы?
Если сервис `contractor-orders-multi` не отвечает или отвечает очень долго, то со стороны лукапа будет фоллбек на следующий алгоритм назначения заказа.

Как организованы ретраи?
Прописаны в конфиге клиента, точные цифры будут указаны в процессе разработки и согласования с ответственными
за сервис.

2. Сервис `candidates`
Взаимодействие в рамках поиска кандидата на заказ.

Как используется?
Сервис `contractor-orders-multi` вызывает `candidates`.

Как гарантируется идемпотентность при запросах?
Будет использоваться токен идемпотентности.

Как поддерживаются отказы?
Если сервис `candidates` не отвечает или отвечает очень долго, то будет возвращена ошибка, и далее со стороны лукапа будет фоллбек на следующий алгоритм назначения заказа.

Как организованы ретраи?
Прописаны в конфиге клиента.

3. Сервис `driver-ordering`

Как используется?
Вызывается для сортировки кандидатов на заказ.

Как гарантируется идемпотентность при запросах?
Будет использоваться токен идемпотентности.

Как поддерживаются отказы?
Если сервис `driver-ordering` не отвечает или отвечает очень долго, то будет возвращена ошибка, и далее со стороны лукапа будет фоллбек на следующий алгоритм назначения заказа.

Как организованы ретраи?
Прописаны в конфиге клиента.

4. Сервис `driver-orders-builder`

Как используется?
Вызывается для формирования сеткаров (информации о заказе) для передачи на клиент (Таксометр).

Как гарантируется идемпотентность при запросах?
Будет использоваться токен идемпотентности.

Как поддерживаются отказы?
Если сервис `driver-orders-builder` не отвечает или отвечает очень долго, 
то будет осуществлён повтор формирования сеткаров средствами STQ. 
В случае более продолжительной недоступности со стороны лукапа будет фоллбек на следующий алгоритм назначения заказа.

Как организованы ретраи?
Прописаны в конфиге клиента.

5. Сервис `communications`

Как используется?
Вызывается для рассылки пушей про мультиоффер.

Как гарантируется идемпотентность при запросах?
Будет использоваться токен идемпотентности.

Как поддерживаются отказы?
Если сервис `communications` не отвечает или отвечает очень долго, то будет осуществлён повтор отправки пушей средствами STQ. 
В случае более продолжительной недоступности -- будет фоллбек на доставку сеткара через `driver/polling/order`.

Как организованы ретраи?
Прописаны в конфиге клиента.

Клиенты (водители):
Таксометр+Убер
* Принимают пуши с сеткаром или получают новый сеткар через поллинг. Механизмы стандартные, здесь ничего не меняется.
Будет добавлен новый код пуша.

* Вызывают ручки для того, чтобы:
  * принять участие в мультиоффере
  * отклонить запрос на мультиоффер

Ручки будут находиться в самом сервисе `contractor-order-multioffer`

**Какие периодические процессы?**

Периодический процесс очистки БД (PG) от старых данных. Предварительно -- храним месяц, настраивается по конфигу.

**Прикрепите схему того, где этот сервис находится в текущей инфраструктуре**


1. Как технически проект влияет на цикл заказа.

На цикл заказа нет влияет, влияет на назначение водителей на заказ.

2. Кто будет потребителями сервиса.
Таксометр+Убер, `lookup` 

**Какие базы использует? Какие данные и по какой схеме сервис будет хранить в базе?**

Используются:
* СУБД `PostgreSQL` для хранения оперативных данных и короткой истории (1-30 дней).
* репликация в YT для хранения полной истории операций, для аналитики

Концептуально нужно хранить в моменте информацию вида:
* заказ, на котором был розыгрыш мультиоффера
* по каждому водителю из заказа
  * score, который мы получили от `driver-ordering`
  * время отправки пуша водителю
  * ответ (да/нет/не получен) на мультиоффер
  * время, когда мы получили ответ (если получили)
  
Для аналитики/эффективности также нужно отправлять эту информацию в YT.

Суммарно, участвуют следующие сущности:
* мультиоффер
  * id
  * даты (создания, обновления)
  * метаинформация, по которой мы решили, что это мультиоффер
  * информация о заказе (как минимум -- его id)
* запрос водителя на мультиоффер
  * driver_profile_id, park_id
  * score
  * request_id
  * push_sent_at
  * contractor_answer (yes, no, -)
  * answer_received_at

**Какие операции над данными заложены? Какой объем данных будет храниться и какой объем будет изменяться в единицу времени?**
Создание мультиоффера и привязка к нему водителей, которые участвуют в розыгрыше
Обновление информации по водителю при получении его ответа на мультиоффер
Поиск всех водителей на мультиоффер для рассылки результатов / отправки водителя в лукап.
  
В рамках MVP нужно хранить не более, чем все запросы.
Это 15 rps * 120 байт (одна запись, на глаз -- три строчки по 32 символа + две даты + число) * 2 (индексы итд)
* 60 * 60 * 24 (в день) -> ~300MB.
За месяц это 9Гб. Предлагается хранить не больше месяца, если оценка верна, или меньше, в случае большей нагрузки.
Нижний лимит хранения -- день.

В целом, возможно разделение на горячее (PG) + холодное (YT) хранилища, если сделать одинаковый формат хранения и формат аналитических данных.

**Есть ли какой-то стейт в памяти, как он обновляется и валидируется?**

Работа происходит напрямую с базой.

**Какая нагрузка ожидается?**

По всему такси сейчас 240 rps на подбор водителей. 
На MVP ориентируемся на Москву -- это 30 rps.

Наша логика диспатча должна работать не всегда, а лишь на доли заказов. Будем считать долю == 10% (скорее всего, в реальности число окажется ниже).

Таким образом, на запуске мы имеем 3 RPS на выполнение пмультиофферного диспатча.

Введём параметр -- среднее кол-во водителей на заказ == 5. Будем считать, что все они что-то присылают в ответ (на самом деле, они могут игнорировать, но следует рассчиывать, в отсутствии других данных, нагрузку для этого случая).
Таким образом, ожидается 15 RPS на взаимодействие с водителями

Рассчитаем суммарную нагрузку.

* Входящий RPS на `/v1/contractor-for-order` для `lookup` -- 30 (MVP) / 240 (при запуске на всю Россию, не раньше Q4). Дальнейшие расчёты -- для MVP.
* Кол-во исходящих запросов в `candidates` и `driver-ordering` == 30
* Кол-во STQ-тасок на дальнейший процессинг == 3
* Кол-во исходящих запросов в `driver-orders-builder` (из предыдущей таски) == 3 (в одном запросе будет один заказ и все водители -- это балковое создание сеткаров)
* Кол-во STQ-тасок на отправку пушей == 3 * 5 == 15 (пушей на отправку мультиоффера)
* Кол-во исходящих запросов в `communications` (из предыдущей таски) == 15

* Суммарный RPS на ручки `/driver/v1/order-multioffer/multioffer/{accept, decline}` -- 15 

Нагрузочное тестирование будет проводиться.

**Какие фолбеки предусмотрены на сам этот сервис?**

Если сервис не отвечает, то `lookup` будет использовать следующий алгоритм диспатча.

**Какие фолбеки предусмотрены внутри этого сервиса на взаимодействие с другими сервисами?**

Поведение при отказе соседних сервисов описано выше.

Какие особенности будут при отключении ДЦ?
Никаких.

Как будет тестироваться отказоустойчивость?
Поведение при отказе сервисов уже описано выше.
при отказе базы возвращается 500 Internal Error, и дальше используется фоллбек.

**Какие возможности масштабируемости закладываются?**
Для всех процессов, которые не должны возвращать ответ сразу -- используется STQ, она масштабируется с количеством машин.

Чтение из БД -- масштабируется по репликам со стороны PG и врядли будет узким местом. Если будут места, где можно будет отдавать данные из кэша -- то мы это сделаем, но пока что выглядит, что таких мест нет.
Потенциально узкое место -- на запись/обновление в PG, тк масштабируется сложнее всего.
Запись будет идти:
* из ручек `/driver/v1/order-multioffer/multioffer/{accept, decline}` ~ 15RPS 
* STQ-тасок на дальнейший процессинг ~ 3RPS
Суммарно это ~20rps, что не является существенной нагрузкой. 
Рост в х10 тоже выглядит нормально, при раскатке на всю РФ.

Поскольку данные сервиса не шардируются, при росте количества машин, сервис будет равномерно распределять
нагрузку между всеми машинами. На каждой машине должны жить данные отписанные выше.
Чем больше машин, тем меньше нагрузка на каждую машину.

`PostgreSQL` - расширяем.

**Какие точки отказа есть в сервисе?**

1) Соседние сервисы.
2) PostgreSQL.

Фоллбэки описаны выше.

**Укажите ключевые продуктовые метрики сервиса, за которыми планируете следить. Укажите технические метрики.**

И для продукта, и технчески необходимо мониторить:
1) Среднее время реакции водителей на предложение мультиоффера
2) Процент заказов, на которых мы получили хотя бы один ответ
или обратная метрика -- процент запросов к водителям, когда мы от них не получили ни одного ответа
3) Процент запросов, на которых мы нашли водителя (хотя бы один водитель согласился)

Технически необходимо мониторить:
1) Стандартные метрики в uservices по части клиентов, STQ-тасок, RPS/таймингов ручек, ошибок в них итд

Все графики с указанными выше показателями будут жить в [графане](https://grafana.yandex-team.ru).

**Какая функциональность ожидается в сервисе в будущем?**
Будет ясно после реализации первого этапа и после аналитики при запуске.

**Какое изменение нагрузки планируется?**
Вопрос рассмотрен выше -- после MVP возможна раскатка на всю Россию.
Скорее всего, в этот момент в сервис будут вноситься изменения для лучшей интеграции с буферным диспатчем, что снизит нагрузку на сервисы `candidates` + `driver-ordering`.

**Активно ли будет изменяться сервис?**
После реализации MVP будет анализ метрик и других ключевых показателей, и решение будет приниматься после этого.

**Как раскатываем?**
1. Выкатываем новый сервис.
2. Настраиваем конфиги таким образом, чтобы в любом случае розыгрыш мультиоффера не выполнялся. 
3. Перенаправляем часть запросов от лукапа на мультиоффер-диспатч - проверяем эту связку в режиме фоллбека.
4. Перенаправляем все запросы, на которых планируется постоянная работа, на мультиоффер-диспатч - проверяем, что мы сможем отключить функционал, если что.
5. Включаем конфиг для работы мультиоффер-диспатча на отдельной геозоне.
6. Убеждается, что метрики в проде показывают адекватные значения и обсуждаем с аналитиками дальнейшие шаги.
7. Включаем конфиг для работы мультиоффер-диспатча на всю Москву.
8. Отдаем логи мультиоффер-диспатча аналитикам, и ждём резюме от них.
9 (опционально). Меняем конфиги диспатча (вместе с аналитиками).

**Авторизация**
Внешние ручки, для таксометра - через driver-authproxy
Сервисы - TVM2.0 

**Не забыть**
1. Заказать дырки:
- тестинг
- прод
2. Сделать графики и мониторинги
