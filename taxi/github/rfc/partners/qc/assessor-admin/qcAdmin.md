**Название сервиса**

qc-pools

**Какую продуктовую проблему решает сервис?**

Предоставить заказчикам сервиса удобный инструмент для налаживания и управления процессом проверки фотоконтроля. Предоставить все необходимые продуктовые метрики и основные мониторинги для контроля над ситуацией. 
Сделать возможным итеративное усложнение процесса с добавлением новых этапов проверки без привлечения разработчиков из группы Фотоконтроля.

**Почему нельзя решить эту задачу без разработки нового сервиса существующими решениями?**

Сейчас асессорские проверки фотоконтроля живут в монолите c#, добавление новых проверок или изменение флоу требует первыкатки монолита, логика не отделена от UI. 
Проверки масок, термопакетов и пр. живут в нирване, логика принятия решения и истории этого решения вынесена из сервиса во внешние скрипты.
Проверки контроля качества ходят в свои ручки и выносят вердикт в соответствии со своей логикой.

Размазанная логика усложняет трассировку шагов выполнения проверок и не дает возможности гибко менять их флоу. 
Нет возможности проводить А/Б тестирование UI, заведение новых флоу проверок требует разработки с правкой монолита.
Сильно усложнен процесс сбора общих метрик

**Как именно сервис будет решать поставленные перед ним задачи?**

PeriodicTask получает из сервиса quality-control все новые проверки и раскладывает их по пулам в соответствии с настройками конфигов по экзаменам.
Каждая проверка, помимо типа экзамена имеет настраиваемый произвольный набод дополнительных данных (метаданные), которые нужны экзекуторам для выполнения своих проверок.

Экзекуторы внешних систем обращаются к сервису за новыми порциями проверок. После обработки в соответствии с внутренней логикой на своей стороне, они присылают проверку обратно с обогащением метаданных проверки.
В идеале, экзекуторы должны решать небольшую локальную задачу: распознать, проверить на дубликаты, отверифицировать данные распознавания и т.п.

В соответствии с настройками конфигов и имеющейся meta-data, сервис либо выполняет resolve вроверки в сервисе quality-control, либо выполняет 
перекладывание проверки в следующий пул для следующего экзекутора.

При появлении новых экзекуторов или изменения порядка их работы, перенастройка пулов осуществляется правкой конфигов без перевыкатывания сервиса.

Если проверка зависла в пуле она разрешается асинхронным обходчиком по истечении времени, отведенном на проверку.

Отдельная джоба собирает метрики по всем пулам для выведения на сводный дашборд

**Этапы разработки**

====Этап 1. Инфраструктура====
* Проработка БД
* Проработка АПИ
* Заведение нового сервиса
* Заведние конфигов и логика работы с ними
* Джобы импорта и протухатора.
* Настройка метрик
* Настройка репликации логов

====Этап 2. Первая проверка====
* Завести на новый сервис простую проверку.
* Выкатить параллельно со старым механизмом на процент
* Сравнить метрики, исправить ошибки
* Написать howto и sample проверку для заведения любых новых экзаменов.

====Этап 3. Перевод ассесорской админки C#====

Далее - перевод всех проверок на новую админку

**Разрабатывается один сервис или система?**

Один сервис + несколько проверок через nirvana, для демонстрации флоу проверки

**С кем взаимодействует сервис?**

 - `quality-control` для получения и разрешения проверок
 - `experiments3` для хранения логики маршрутизации

**Какие базы использует?**

Вводится pg база, в которой хранится:
 * таблица с настройками пулов
 * таблица с проверяемыми pass'aми
 * таблица с метаданными, связанными с проверкой, в том числе и проверяемые данные
 * таблица с активными проверками
 * таблица истории проверки

**Какие периодические процессы?**

UpdatePeriodicTask получения данных из quality-control - непрерывно, пока есть данные. В случае их отсутствия слип (из конфига)

ExpirePeriodicTask разрешения просроченных проверок - раз в 1 минуту(из конфига) - проход по отслеживаемым пулам для просроченных проверок - выполнение ветки "expire"

MetricsPeriodicTask сбора статистики по пулам.

На таблицу Logs будет настроена репликация для отгрузки истории проверок в YT

При получении данных от экзекутора стартуется таска на сохранение данных и перекладыване проверки в следующий пул.

**Планируется обработка персональных данных**

Нет, персональные данные передаются потребителям в виде идентификаторов сервиса ПД. Обмен скрытых данных на сырые выполняется на стороне экзекуторов (например, скрипты nirvana) в момент выполнения конкретных действий по проверке данных.

**Какие данные и по какой схеме сервис будет хранить в базе?**

В сервисе будут хранится текущие проверки из QC, после разрешения в QC, они будут удалятся. После перекладывания проверки в пул, из предидущего пула проверка удаляется. История хранится в отдельной таблице, после репликации, удаляется.
Ниже приведена примерная схема таблиц.

Passes - таблица с проверяемыми pass'aми (проверки из quality-control)
| field | type | comment |
| ------ | ------ | ------|
| id     | TEXT unique | qc pass id
| type | TEXT | Тип qc-сущности, нужно, чтобы строить метрику
| exam | TEXT | qc exam
| created | TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP| 
 

Pools - очереди проверки, pool_id для идентификации и настройки в конфигах
| field | type | comment |
| ------ | ------ | ------|
| id| TEXT unique | имя пула
| description| TEXT | Описание
| author| 	 TEXT | кто создал пул
| created| TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP, 

Checks - таблица с активными проверками (нахождение pass в pool)
| field | type | comment |
| ------ | ------ | ------|
| id| BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
| pass_id| uuid REFERENCES qc-pools.pass(id) ON DELETE CASCADE,
| pool_id| uuid REFERENCES qc-pools.pools(id) ON DELETE RESTRICT,
| created| TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
| expired_at|   TIMESTAMPTZ,
| locked_till|  TIMESTAMPTZ
 

Meta - таблица с метаданными, связанными с проверкой, в том числе и проверяемые данные
| field | type | comment |
| ------ | ------ | ------|
| id| BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
| pass_id| uuid REFERENCES qc-pools.pass(id) ON DELETE CASCADE,
| key| TEXT NOT NULL,
| value| TEXT NOT NULL,
| editable| BOOLEAN NOT NULL DEFAULT TRUE,
| hidden| BOOLEAN NOT NULL DEFAULT TRUE
 

Logs - таблица с событими, которые проходит проверка. Таблица реплицируется в YT с удалением реплицированных
|  field | type | comment |
| ------ | ------ | ------|
| id| BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
| timestamp  | TIMESTAMPTZ,
| pass_id| uuid REFERENCES qc-pools.pass(id) ON DELETE CASCADE,
| pool_id| uuid REFERENCES qc-pools.pools(id) ON DELETE RESTRICT,
| status| TEXT NOT NULL, | [pending, locked, finished] 
| process_id|   TEXT NOT NULL, | идентификатор процесса, вызвавшего событие
| text| 	TEXT | описание, если оно необходимо (например, смена метаданных)
  

**Какой объем данных будет храниться и какой объем будет изменяться в единицу времени?**

Объем будет сильно зависить от экзаменов, переведенных на сервис
Сейчас основные проверки это:
 * Биометрия (до 300К проверок в день, время жизни 1 проверки < 10 минут) 
 * Медицинские маски (до 300К проверок в день, время жизни 1 проверки < 10 минут) 
 * Проверки из старой админки асессоров (до 200К проверок в день, время жизни 1 проверки < 6ти часов)

одна проверка, которая сейчас хранится в монге, занимает около 1Кб

Т.о. если все проверки перевести на сервис, за день будет прокачиваться около 1Гб, единовременно в системе будет до 300 Мб 
+ лог, которы будет жить некоторое время до репликации (м.б. настроить оптимальный период, на старте - 2 дня) ~ 500 Мб/день


**Какие операции над данными заложены?**

Passes - добавление, удаление
Checks - добавление, удаление
Meta - добавление, модификация, удаление
Logs - добавление

**Есть ли какой-то стейт в памяти, как он обновляется и валидируется?**

Нет

**Какая нагрузка ожидается?**
 
На все проверки qc сейчас около 200 rpm, с учетом балковых ручек и небольшого количества экзекуторов, нагрузка ожидается небольшая.
 

**Какие фолбеки предусмотрены на сам этот сервис?**

Внутри QC, в случае, если резолюции проверок не будут поступать от сервиса, они будут разрешаться автоматически, по истечении времени указанного в настройках каждого экзамена. В зависимости от настроек это может быть, как автоматический ОК, так и просьба водителя послать фотографии еще раз.

**Какие фолбеки предусмотрены внутри этого сервиса на взаимодействие с другими сервисами?**

В случае отказа поставщика данных (QC), новых проверок поступать не будет, UpdatePeriodicTask будет ждать восстановления.

В случае отказа любого из экзекуторов, ExpirePeriodicTask будет переводить проверки на дефолтный пул (из конфига).

**Какие возможности масштабируемости закладываются?**

Сервис масштабируется горизонтально - rps растёт пропорционально количеству проверок и исполнителей.

**Какие точки отказа есть в сервисе?**

Внешние походы в `quality-control`
БД
experiments 3.0

**Укажите ключевые продуктовые метрики сервиса, за которыми планируете следить**

На каждом этапе при изменении состояния в grfana отгружаются метрики по каждому из пулов: 
 * количество проверок
 * принятые решения
 * время, проведенное проверкой в пуле (SLA)
 * метрика на полное время проверки по всем пулам (с разделением по экзаменам).

**Укажите технические метрики**

стандартные - rps, тайминги ручек, размер БД.

**Какая функциональность ожидается в сервисе в будущем?**

распараллеливание проверок
Перераспределение по пулам в зависимости от нагрузки/динамики роста sla и eta резолюции

**Какое изменение нагрузки планируется?**

x2, пропорционально росту сервиса

**Активно ли будет изменяться сервис?**

При стабильной работе - не планируется

## Архитектура

### API

POST api/v1/pass/{pool}/retrieve выдать проверку экзекутору, возможно повесить лок на проверку (в зависимости от настроек пула)
POST api/v1/pass/{pool} вернуть проверку с резолюцией экзекутора, выполнить действие в соответствии с логикой конфига, удалить проверку из пула

GET api/v1/pass/state вернуть историю проверки из таблицы Logs (по pass_id), либо восстановить историю проверки из YT
POST api/v1/pass/ разрешить конкретную проверку в сервисе QC (для селективной резолюции из UI админки)

### Клиентская логика

Сервис исключительно межсервисный

### Технические ограничения

На первом этапе исключительно последовательные проверки. SLA ограничивается выгрузкой данных из QC и периодичностью запуска экзекуторов

### Как раскатываем

После настройки окружения плавно переключаем проверки на новые рельсы

### Безопасность

Все межсервисные ручки прикрыты tvm, изменения конфигов через драфт, POST ручка для админки - под аудитом админки без кода.



