# order-metrics


## Для разработчиков продукта

Я решил написать РФЦ по шаблону архитектурного описания сервиса, т.к. наш шаблон тут не очень подходит.

Переход будет осуществлен следующим образом: графитовые метрики вида

```
'orders.moscow.multiclass_selected_classes_count.num_3'
'orders.allcities.price_modifier.ya_plus.source.alice'
```

Превратятся в соломоновские сенсоры

```
{
    'sensor': 'order_proc_stat_legacy',
    'metric_legacy': 'orders.moscow.multiclass_selected_classes_count.num_3'
}

{
    'sensor': 'order_proc_stat_legacy',
    'metric_legacy': 'orders.allcities.price_modifier.ya_plus.source.alice'
}
```

Это облегчит переход на соломон, т.к. пользователям нужно будет только изменить источник данных в графане, получение метрик вайлдкардами останется тем же.

В дальнейшем пользователи получат возможность эмитить такие метрики:

```
{
    'sensor': 'order_proc_modifiers',
    'source': 'alice',
    'modifier': 'ya_plus',
    'city': 'moscow',
    'tariff': '...',
    ...
}
```

Это позволит сократить число сенсоров и получить более "гибкие" срезы по статистике.

**Важно:** order_proc_statistics не будет отправлять данные в графит.


## План по переходу

Первым этапом переводим существующие метрики из папок `created` и `finished` на uservices/order-metrics и отправку в Solomon, убеждаемся, что все ок (значения метрик в графите и соломоне совпадают), с этого момента добавлять новые метрики в графит нельзя (для этого `backend-product-team-2` добавляется в CODEOWNERS taxi_maintenance/order_proc_statistics).

Вторым этапом переводим существующие метрики из папки `active`. Для них скорее всего понадобится дополнительное хранилище.
**(TODO: проработать решени с active)**

Продуктовым команды могут переводить существующие дашборды на Соломон (переход не должен быть трудным, все авторы по git-blame будут уведомлены), после перевода последнего order_proc_statistics удаляется из taxi/backend.


## Проблемы

1) В будущем хотелось бы перейти на получение данных о заказах из очереди сообщений (логброкер?)

> @iHelos: мне кажется логброкер должен быть не заменой, а дополнением к существующему способу сбора метрик
например кол-во текущих активных заказов кажется удобно собирать по базе, а вот кол-во успешно завершенных за период времени - по логброкеру (потому что есть проблема с получением успешных заказов из монги (непонятно по какому полю итерироваться - updated_at может меняться по другим причинам))

**TODO: оценить по каждой проверке реализуемость её без запросов в mongo.**
Будем делать в [таблице](https://wiki.yandex-team.ru/users/sapunovnik/Perevod-proverok-orderprocstatistics-na-uservices/)


# Заявка на сервис

## Задачи сервиса

### Название сервиса

order_statistics

### Какую продуктовую проблему решает сервис?

Сбор продуктовых метрик по заказам (например, количество активных заказов с мультитарифом, или количество созданных заказов через Алису за последнюю минуту)

### Почему нельзя решить эту задачу без разработки нового сервиса существующими решениями?

Данная проблема уже решена в taxi/backend/taxi_maintenance/stuff/order_proc_statistics

однако

1) Код написан на python 2, а он депрекейтед (надо на перевести на питон 3 / плюсы)
2) Код шлет данные в graphite, а он депрекейтед (надо слать в Соломон)
3) Код лежит в "общем" месте taxi_maintenance

### Как именно сервис будет решать поставленные перед ним задачи?

Метрики из taxi/backend/taxi_maintenance/stuff/order_proc_statistics/metrics будут переписаны на userver и доставаться будет из топика LogBroker.


## Взаимодействие

### С кем взаимодействует сервис?

- **LogBroker**. Топики:
    - /taxi/processing/unstable/order-events
    - /taxi/processing/testing/order-events
    - /taxi/processing/production/order-events

- **Solomon** — write only (send metrics)

### Какие базы использует?

Будет создана новая база для этого сервиса для хранения курсора LogBroker.
В перспективе в этой базе будет хранится промежуточный стейт для вычисления active метрик.

### Какие периодические процессы?

После реализации логики обработки active событий потребуется очищать базу.


## Хранение и обработка данных

### Какие данные и по какой схеме сервис будет хранить в базе?

На первом этапе сервис будет хранить дистлоки LogBroker и его же офсеты

```sql
-- distlocks table
CREATE SCHEMA order_statistics;
CREATE TABLE order_statistics.distlocks(
  key TEXT PRIMARY KEY,
  owner TEXT,
  expiration_time TIMESTAMPTZ
);


-- logbroker schema and offsets table
CREATE SCHEMA lb;
CREATE TABLE lb.offsets (
	topic_partition TEXT PRIMARY KEY,
   	offsets BIGINT
);
```

### Какие операции над данными заложены?

\-

### Есть ли какой-то стейт в памяти, как он обновляется и валидируется?

\-

## Отказоустойчивость и масштабируемость

### Какая нагрузка ожидается?

Сервис анализирует
* активные на момент запуска заказы (status ==  assigned), <200k документов
* заказы созданные за последнюю минуту (time() - 60 <= created <= time()>), <50k документов
* заказы в статусе (status in [finished, cancelled]) за последнюю минуту, <50k документов

### Какие фолбеки предусмотрены на сам этот сервис?

Нет фолбеков.

### Какие фолбеки предусмотрены внутри этого сервиса на взаимодействие с другими сервисами

\-

### Какие возможности масштабируемости закладываются?

На данный момент сервис обрабатывает сто тысяч активных заказов за 10 секунд (здесь заложено время на получение данных из Монги и их анализ Питоновским кодом). Это значит, что

1) Существующее решение может пережить десятикратное увеличение числа заказов без изменений
2) Бесконечно масштабироваться нынешнее решение не может.

Возможные пути улучшения:

1) Увеличение периода сбора метрик
2) Выполнение параллельных запросов к монге и параллельная аггрегация
3) Заведение отдельного хранилища под сбор статистики

### Какие точки отказа есть в сервисе?

\-

## Метрики

### Укажите ключевые продуктовые метрики сервиса, за которыми планируете следить

Продуктовых метрик у сервиса нет.

## Укажите технические метрики

Будем следить за
1) Временем выполнения каждой из периодических задач (а также сделаем алерт "время выполнения близко к периодичности запуска")
2) Будем следить за количеством анализируемых документов по каждой из периодических задач

## Планы роста

### Какая функциональность ожидается в сервисе в будущем?

Для расчета дополнительных метрик список получаемых из заказа полей может увеличиться (NB: не влияет на количество выгружаемых из базы строк).

### Какое изменение нагрузки планируется?

Со временем будет увеличиваться число документов для анализа

### Активно ли будет изменяться сервис?

За последние девять месяцев в "оригинальное" решение было добавлено суммарно 17 метрик по 9 признакам (например, если order.source содержит значение из конфига ORDER_SOURCE_METRICS, посчитать по нему активные и созданные заказы), т.е. добавляется по ~1 признаку в месяц.
