Есть необходимость реактивного запуска расчетов подвязанных на внешние 
логфеллерные таблицы. Допустим аппметрики. Решили завязывать реактивность на ctl.
Однако, в нашем ctl нет информации касательно наличия этих самых таблиц. 
Поэтому эти данные нужно прогрузить.

**Как мы грузим эти данные:** отдельной таской, которая ходит с определенной 
частотой в реактор и собирает данные об обновлении таблиц за определенное время. 
Таску запускаю в eda_etl и taxi_etl. т.к. в обоих сервисах есть таски полагающиеся 
на логфеллерные таблицы. И при добавлении отслеживания нового лога будет 
достаточно выкатить только один их этих etl.

**Особенности логфеллерных таблиц**: Они публикуются в двух скейлах. В дневном 
и получасовом (эти два скейла в msk). Дневной обладает большим лагом. Поэтому, 
при расчете данных за "вчера" по utc мы берем дневные и получасовые таблицы 
(только на дневных пришлось бы ждать дополнительные сутки или больше). Нет 
гарантии, что таблицы будут опубликованы по порядку, т.к. возможен случай, 
когда получасовые скейлы приедут в порядке "1:00 2:00 1:30".

**Алгоритм расчета last_load_date для внешней таблицы**:
1. запрашиваем дневные скейлы и получасовые скейлы. 
2. Добавляем к таблицам скейла дельту равную скейлу (т.е. для получасового скейла 
 транформация будет [1:00, 1:30] -> [1:30, 2:00] - 1:00 таблица содержит данные
 до 1:30; дневная таблица `2020-02-03 00:00:00` содержит данные по `2020-02-04 00:00:00`).
3. удаляем все после "дырок" для обоих скейлов (т.е. для 30m, если есть 
последовательность "1:00 1:30 2:30 3:00" мы обрезаем последовательность до "1:00 1:30"
4. проверяем что дневные скейлы и получасовые пересекаются. Для этого достаточно
 проверить, что максимальная дата в дневном скейле присутствует в массиве получасовых.
5. берем максимальную дату из двух скейлов.
6. берем сохраненную дату из ctl и сравниваем новое значение с ней - если есть 
изменение, перезаписываем.