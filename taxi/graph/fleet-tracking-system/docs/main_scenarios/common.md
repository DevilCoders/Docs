### Сценарий улучшения алгоритма.

Представим что мы разработали новый алгоритм, который улучшает качество притягивания на 11 секунд согласно нашим внутренним оффлайн метрикам. Теперь мы хотим его раскатить по всей системе постепенно, с проведением внутреннего эксперимента (который следит за нашими внутренними метриками).

Тут сразу вопрос - а правда ли что мы можем так описать метрики предсказания, что они будут хорошим контрактом для всех наших потребителей ? Не обнаружится ли что новый алгоритм лучше по всем нашим метрикам, но какой-то потребитель так использует данные, что ему от нового алгоритма хуже? Может быть нам стоит разрешить пользователям выбирать алгоритм предсказания, а мы будем всего лишь их уведомлять:
Вместо maps-predict-v1.97 мы разработали maps-predict-v2.04 котоырй лучше на 7% согласно метрике А, 9% по метрике Б и т.п. 
В этом варианте сценария, как пользователю аккуратно проверить какое предсказание лучше на своей стороне ? (опять таки, тут можно и нужно использовать комбинацию наших настроек и настроек у них)


### Сценарии протокола, простой
Протокол хочет получить последнюю позицию водителя с источником Verified и ее притянутый вариант (если контрактор притягивается)

