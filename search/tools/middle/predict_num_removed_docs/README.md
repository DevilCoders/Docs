Утилита для подбора формул из search/meta/fml_extradocs.cpp, предсказывающих
верхнюю оценку числа групп, которые удалят переранжирования на AfterMerge
или AfterFetch, по числу запрошенных групп и bgfactors.

Формула имеет вид одного matrixnet-слагаемого: есть k условий вида
"`фактор` > `число`", они организованы в полное двоичное дерево с 2^k листьями,
в каждом листе написан предикт, который следует вернуть в качестве результата
при попадании в лист. Собственно matrixnet не получилось завести в условиях, когда
ошибка в минус намного критичнее ошибки в плюс.

На вход принимает логи прода, выгружаемые каждый день в
[hahn://home/eventlogdata/MsuseardataJupiterTier0/*](https://yt.yandex-team.ru/hahn/navigation?path=//home/eventlogdata/MsuseardataJupiterTier0).

Первая стадия — агрегация логов в пул. За один день вызывается командой
`./predict_num_removed_docs process-eventlog //home/eventlogdata/MsuseardataJupiterTier0/20200000 <pool table 1>`
В результирующем пуле по одной записи на запрос. Рекомендуется собрать данные
за несколько дней, но так, чтобы суммарно пул влезал в память. Текущие формулы
подбирались по пулу размера ~600000 записей. Плюс дополнительная тестовая таблица
за ещё один день для контроля переобучения.

Вторая стадия — собственно подбор формулы.
```
./predict_num_removed_docs learn \
  --target {merge|fetch} \
  --learn-table <pool table 1> --learn-table <pool table 2> \
  --test-table <pool table 3> | tee learn.log
```
При желании можно покрутить дополнительные ручки `./predict_num_removed_docs learn --help`,
их умолчальные значения соответствуют тому, как подбиралась текущая формула.
Крутилка `--allowed-extramerges-ratio` задаёт долю записей лёрна,
на которых разрешено занизить оценку, и управляет балансом между суммарным
количеством "лишних" документов и долей дозапросов. Остальные ручки технические.
В stdout пишутся подобранные формулы на каждой итерации и их статистики:
```
6176715	\
GroupsRequested:43,RandomLogQueryAvgIsIndexPage:0.0484146115,DssmRandomLogQueryAvgTextLike:0.26470589,DssmRandomLogQueryAvgQueriesAvgCM2:0.123529415,QueryToTextAllAvg:0.634348225,QueryWordDistanceToWordRassrochka:0.073597298	\
14,16,15,15,11,14,10,17,6,7,5,4,8,8,7,7,25,25,9,11,21,24,12,25,9,4,3,4,16,16,8,15,1,1,6,3,8,8,6,22,5,4,3,3,7,7,6,6,15,2,2,5,14,18,8,7,7,4,6,0,13,14,5,7	\
2758	3484496
```
по одной строке для формулы, через табуляцию. Первая колонка — сумма предиктов
по всем строкам лёрна, минимизируемая функция. Вторая и третья описывают собственно
формулу, во второй колонке набор факторов и границ, в третьей значения в листьях.
Четвёртая и пятая дают результаты на тесте (если тестовый пул не задан, там будут нули),
соответственно число ошибок на тесте, где предикт оказался меньше реального
значения из пула, и сумму предиктов по всем строкам теста. Если оба начинают
ползти вверх, нужно либо брать пул побольше, либо не последнюю итерацию в качестве результата.

Принцип подбора формулы. Факторы бинаризуются по квантилям. Для фиксированного
набора бинарных факторов вычисляются значения в листьях так, чтобы число ошибок
в минус не превосходило заданного параметра обучения N (aka
`--allowed-extramerges-ratio * <размер лёрна>`), а сумма предиктов была
по возможности меньше; для этого проходим по пулу и запоминаем top-N таргетов
в каждом листе, после чего вычисляем таблицу `aggregated[i,j]` = "минимальный скор,
если учитывать первые `i` листьев и разрешать `j` ошибок в минус" итеративно по `i`;
срез для одного листа это просто копия top-N таргетов, при добавлении `i+1`-го листа
нужно вычислить оптимальное разбиение `j` на число ошибок в первых `i` листьях
и во вновь добавленном (делается банальным перебором); итоговый результат — последний
элемент таблицы. Обладая вычислителем скора по набору бинарных факторов, запускаем
их подбор по одному жадным образом, на каждой итерации фиксируем набор факторов
от предыдущей итерации и перебираем все варианты для ещё одного; перебор
распараллелен на YT. Для чуть лучшего качества от предыдущей итерации
берётся не один топовый набор, а 16 (`--num-candidates`).
