### Введение

Данный проект предназначен для тестов мониторинга большого брата. Базово эти тесты предполагают следующее:

* Поднятие базы для darkspirit с накаткой всех необходимых миграций.
* Поднятие образа большого брата, работающего с этой базой.
* Проверка того, что мониторинги реально работают.


В данном проекте используются подмодули. Гипотетически этого можно избежать в случае большого
брата (просто прописав его в requirements.txt), но не в случае базы, которую надо собрать с нуля, пригодную для накатки. Соответственно здесь приходится взаимодействовать через подмодули.

### Состав образа

Данный образ предполагает свое построение следующим образом. Из корневого каталога
нужно запустить следующую команду:

```shell script
cd containers
docker build --network=host -f docker/simplebrother/Dockerfile .
````

Первая строка образа намекает, что в основе лежит версия образа trusty. Почему не 
xenial либо bionicle? Потому что часть пакетов, используемых в зависимостях, не была
добавлена в репозитории для этих версий, поэтому придется посидеть на trusty.

Затем у нас идет вызов вида:

```shell script
apt-get update && apt-get install -y 
```

Что здесь нужно  и зачем? python+pip нужны по очевидным 
причинам, поскольку иначе собрать проект на питоне будет сложновато. git потребовался
затем, что часть зависимостей собирается из нашего собственного репозитория. oracle instant client
нужен затем, что мы взаимодействуем в ряде мониторингов с базой на Oracle, и это взаимодействие
не заработает иначе. В общем и целом здесь ставится только то, что нельзя потом поставить через
pip.

Остался вопрос с dpkg. А он нужен зачем? А он нужен затем, чтобы можно было поставить
yandex-balance-common-conf-dev. Этот пакет содержит кучу конфигурационных файлов
xml, которые прописываются куда-то в /etc/yandex, и без которых большого брата будет очень сложно
запустить. Этот пакет ставится довольно странно, и без dpkg самостоятельно не встанет, поэтому его 
придется поставить.

Далее идут уже манипуляции c pip. Мы его сначала апгрейдим, доставляем setuptools, затем ставим зависмости
для продакшена (requirements.txt) и зависимости, которые уже есть на наших грид машинках, но отсутствуют
по умолчанию в trusty (requirements-dev.txt). Мы решили не тащить это в итоговую сборку deb пакета,
чтобы не сломать чего-нибудь на железе. 

Затем копируется каталог deploy. Что он в себе содержит? oraprofile, задающий определенный набор переменных окружения. Этот файл затем 
пользуется в entrypoint.sh, поскольку иначе система не будет знать, где искать so
файлы бинарного клиента Oracle. Помимо этого, прописывается еще путь, где искать tnsnames.ora (в переменной
TNS_ADMIN), чтобы потом найти все, что нужно. И подгрузиться к базе.

Также копируется docker-servant.cfg.xml в качестве основного конфигурационного файла. Его состав будет рассмотрен
в отдельном разделе ниже. 

Также создаются парочка вспомогательных каталогов, пути, куда пишутся файлы логов и прочее. Это
сделано просто для того, чтобы приложение внутри образа не сильно отличалось по составу файловой
системы от уже имеющихся железных машин, чтобы не было вопросов вида - почему на железке логи хранятся
по пути /var/log/yb, а в Я.Деплой (а у меня есть идеи, как потом этот образ развернуть там) храняться по пути
/var/log/simplebrother? 

Что касается entrypoint.sh, то он собирает из исходников проекта большого брата нужный бинарник. Возникает резонный вопрос - почему requirements.txt большого брата копируется и собирается в образе, а это вынесено в запуск? А потому что установка requirements.txt каждый раз при запуске будет приводить к нехилым тратам трафика сетевого. Логика здесь такова, что requirements.txt меняется существенно реже остального кода проекта simple-brother. Поэтому зависимости подтягиваются при создании образа, а вот код собирается в библиотеку при запуске. Это дает существенную экономию, поскольку при старте проект собирается моментально.

### Содержимое конфигурационного файла

Давайте посмотрим на этот файл. До секции MonitorSet ничего интересного, в общем-то, нет. Чисто указываются
настройки логирования, профиль запуска, а также где хранится pid процесса - это нужно в связи с тем,
видимо, что изначально на железках была масса процессов на питоне, и как-то за ними надо следить.
Для докера это неактуально, но переписывать код родительского большого брата не с руки.

В MonitorSet сначала идет Schema, в которой прописывается базовая таблица моинторинга. Это позаимствовано без изменений
с других конфигурационных файлов. Далее идет секция Connection, которая представляет наибольший интерес. Она 
описывает соединения, с которыми можно работать. Надо заметить, что названия хостов - это то, что записано в tnsnames.ora, 
описанном в предыдущем разделе - а не реальные названия хостов баз. 

Далее идет секция обработчиков Handlers. В терминологии большого брата это то, что будет использовать 
sqlalchemy для взаимодействия с базой в тех мониторингах, где это реально необходимо. Здесь указывается название соединения
из предыдущей секции.

Далее идет секция расписаний запросов на мониторинг Scheduler. Здесь используется планировщик из данного проекта,
который и содержит необходимые запросы на мониторинги.

Далее идет секция Senders - то, что отправляет результаты мониторинга. Отправлять можно много куда, мы используем
самописный DockerSender, который просто записывает в лог json, который ему прислали. Этот DockerSender не входит в проект самого большого брата, и докидывается в образ из данного проекта тестов.

Далее идет набор MonitorLogic и прочие разделы, связанные с настройками глобальных блокировок. Отключить что-либо из
этого нельзя, т.к. код большого брата рассчитан на наличие всех этих настроек. Собственно, в них произведено 
нацеливание на единственную базу - которая должна быть поднята также в докере.

Далее идет секция zookeeper с описанием хостов. Зачем она нужна, см. в соответствующем разделе.

Далее идет секция http, которая поднимает интерфейс этого большого брата локальный, чтобы можно
было посмотреть на него. Этот интерфейс, как правило, не используется.

Далее идет секция mpool, устанавливающая настройки пула потоков, для запуска нескольких запросов
параллельно. Отказаться от этой секции нельзя, т.к. большой брат ее ожидает и в противном случае просто валится.

### Запуск docker-compose

Для начала стоит запустить накатку миграций:

```shell_script
docker-compose -f containers/docker-compose.yml up db-migrate
```

Этот скрипт поднимет базу, дождется того, когда она проинициализируется, и накатит:

* Сначала миграции самого darkspirit.
* А затем создаст таблицу, в которую записывают результаты мониторинга - эта таблица нужна сугубо большому брату. На проде она создана в рамках схемы bo, однако здесь в рамках схемы даркспирита и тестов для него нет никакого резона пытаться поднять такую базу с нуля, пусть и в образе, а можно просто создать таблицу такой же структуры, чтобы мониторинг в нее писал.

После того, как база создана и заполнена, можно запустить самого большого брата:

```shell script
docker-compose -f containers/docker-compose.yml up simple-brother
```

У simple-brother две переменные окружения задаются - одна указывает, где хранится файл окружения. Вторая указывает большому брату, какие именно запросы надо запускать.

### Образ зукипера

Очевидный вопрос - а зачем нам Zookeeper для локальных тестов, и зачем он вообще нужен?

А это нужно для ситуации, когда много разных инстансов координируют свои усилия по мониторингу, чтобы
не дублировать друг друга. А можно ли это отключить для тестов. Краткий ответ - НЕТ. Более длинный ответ заключается
в том, что в коде корневого simple_brother допущена одна небольшая ошибка:

```python
# Сначала у нас есть такая функция
@contextlib.contextmanager
def global_lock_if_needed(name, lock_acquire_timeout, zoo_settings):

# А затем мы используем ее вот так

global_lock_settings = parse_settings(self.cfg, 'MonitorSet/GlobalLock/Setting')
zoo_in_use = global_lock_settings.get("lock_acquire_timeout") is not None
with global_lock_if_needed(zoo_settings=zoo, **global_lock_settings):
```

Фокус заключается в том, что если не указать в настройках конфигурационного файла настройки лока, 
то будет кидаться ошибкой из-за того, что при вызове global_lock_if_needed не передали ожидаемое количество
параметров. Если же указать в настройках эти параметры, то будет ожидаться, что есть zookeeper. Т.е. здесь
пытались сделать возможность запуска без zookeeper, но не осилили до конца. Править корневую библиотеку, которая
используется не только лишь здесь, мы не стали, а просто добавили инстанс zookeeper.

### Тестовая база

Здесь все просто - наш docker-compose по сути создает такую же базу, как и docker-compose в проекте darkspirit, и использует тот же самый код через подмодули. Просто и со вкусом. 

В скриптах конкретно для тестов содержатся две вещи:

* Создание таблицы мониторинга, поскольку в продакшене она берется вообще из другой базы (коммунальная база, блин).
* Заполнение данных для того, чтобы запросы метрики вообще отрабатывали с непустым результатом - создано несколько ФН.

### Организация тестов

Тесты написаны на Java, с помощью [фреймворка testcontainers](https://www.testcontainers.org/). Конечно, это выглядит странным решением, но, как оказалось, версия testcontainers 
для питона является очень сильно урезанной. Поскольку речь идет о наборе тестов, и не затрагивает исходные проекты, то такое решение выглядит обоснованным. 

В качестве тестовых ресурсов для проекта Gradle выступает каталог containers, за исключением того содержимого,
которое представляет из себя символические ссылки. Это происходит в случае, когда разработчик пытается 
поднять какой-то из сервисов вручную, при этом запуская его сборку, которая, в свою очередь,
приводит к появлению символических ссылок. 

Можно заглянуть в ComposeTest.java и увидеть, что обращение к файлу docker-compose.yml производится следующим образом:

```java
    private static File getFile(String filename) {
        val loader = ComposeTest.class.getClassLoader();
        return new File(loader.getResource(filename).getFile());
    }
```

Т.е. по этой ссылке мы получим путь, куда были распакованы в рантайме тестовые ресурсы, и рядом будут необходимые каталоги из darkspirit и simplebrother, чтобы compose сумел ими воспользоваться.
