# 5. Архитектура сети внутри датацентров
Традиционно, сети датацентров строились по классической иерархической модели, унаследованной от кампусных и энтерпрайзных сетей.

Для модулей, построенных по данной модели можно выделить следующие свойства:
1. Высокая переподписка.
2. L2 внутри модуля. L2-коммутатор в каждой серверной стойке. Проекты изолируются друг от друга с помощью VLAN-ов:
   каждый проект живёт в своём VLAN-е (иногда в нескольких). В каждую очередь устанавливается файрвол – сущность,
   через которую проходит весь трафик очереди, а так же обеспечивающая доступы из одного проектного VLAN-а в другой.

![Old-school DC](./images/dc_arch_inside/img1.svg)

Логическое развитие данной модели - L3 на стойку и разделение проектов по VRF.

Стоечный коммутатор выполняет функции маршрутизатора и терминирует на себе сети из проектного VLAN-а. Существенно сокращена область распространения broadcast-пакетов и изменена маршрутизация, что позволяет применять ECMP для балансировки. Однако, трафик между проектами по-прежнему проходит через файрволы.

Рост Machine-to-Machine трафика, привел к необходимости отсутствия переподписки в рамках одного ДЦ, что логично вылилось
в новый виток развития сети Яндекса, который можно охарактеризовать следующими тезисами:
1. Переосмысление физической топологии сети в ДЦ и строительство фабрики с использованием Clos-топологии
2. Трафик между серверами Яндекса вместо выделенных серверов с файрволами обрабатывается HBF прямо на хостах

![New-school DC](./images/dc_arch_inside/img2.svg)

Для примера, фабрика VLA1 собрана следующим образом:
> **NOTE**
> Часть линков не отображена для облегчения иллюстрации

![Пример VLA](./images/dc_arch_inside/vla1-fabric-2.svg)

Теперь немного усложним картину и схематично отобразим недостающие элементы, упомянутые выше - балансировщики,
декапсуляторы, файрволы, тунеляторы и NAT64:

![](./images/dc_arch_inside/Scale-out+service.svg)

## Маршрутизация и стек протоколов

Масштабируемость IGP в условиях Яндекса невозможна, и пожалуй, единственная альтернатива (исключая экспериментальные протоколы) в наших условиях - это BGP.

В текущей схеме:
- TOR коммутаторы агрегируют адресное пространство серверов (про L3 на стойку и выделение адресного пространства - ниже)
- Устанавливают eBGP-сессии в VRF Hbf с вышестоящими коммутаторами уровня Spine1 (D)
- Коммутаторы Spine1 включены в OSPF/MPLS домен. В качестве протокола распространения меток используется LDP

![](./images/dc_arch_inside/Domain+proto.svg)

- Коммутаторы Spine1 устанавливают eBGP-сессии с сервисными коммутаторами (A), часть из которых находится в Vrf Hbf,
  другая - в глобальной таблице маршрутизации GRT.
- Коммутаторы Spine1 устанавливают iBGP-сессии с RR нескольких типов

![](./images/dc_arch_inside/Proto_sessions.svg)

- Стоечные коммутаторы анонсируют 4 серверные подсети, маркируя их с помощью BackBone или FastBone BGP community
- Коммутаторы Spine1 терминируют Vrf HBF и анонсируют агрегированные IPv6 сети ДЦ сервисным коммутаторам (A), в свою очередь получая маршрут по умолчанию, адреса балансеров, nat64, тунеляторов и IPv6 anycast-адрес декапсуляторов
- Коммутаторы Spine1 получают маршруты по умолчанию от Border-ов и передают его сервисным коммутаторам (A)
- Коммутаторы Spine1 анонсируют агрегированные IPv6 сети ДЦ
- Сервисные коммутаторы возвращают коммутаторам Spine1 в таблице GRT агрегированные IPv6 сети ДЦ, а также анонсируют адреса балансеров, nat64, тунеляторов и IPv6 anycast-адрес декапсуляторов

![](./images/dc_arch_inside/Serviceleaf.svg)

[Список BGP рефлекторов](./https://wiki.yandex-team.ru/noc/reflectors/)
