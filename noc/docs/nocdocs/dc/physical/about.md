# Физический дизайн ДЦ Яндекса

## Введение. Общая картина
Отвечая на высказанное при опросе общественности пожелание рассказать о физическом устройстве сети, предлагаю в этом посте погрузиться в увлекательный мир кабелей, стоек, странных коробок с мигающими лампочками и тому подобных атрибутов ~~вычеркнуто цензурой~~ NOCовской магии.

В качестве эпиграфа:
```
Dmitry (DAY) Ershov, [21.03.19 16:03]
кажется нам поломали электричество в КЦ

Peter Popov, [21.03.19 16:04]
Кц это что?

Tokza, [21.03.19 16:04]
Нок рум

Олег, [21.03.19 16:05]
что такое нок рум?

Ivan Lysogor, [21.03.19 16:05]
Ядро сети дата-центра
```

Даже если вам понятны все слова в диалоге, все равно в посте вы найдете массу интересного.
В общем виде наша сеть выглядит так:

![img_01.png](_images/img_01.png)

На общей схеме представлены основные функциональные блоки, которые сеть связывает между собой. Это датацентры, магистральная сеть и ядро, точки присутствия, внешние связи. Чтобы разобраться, как все устроено физически и на что похоже, мы рассмотрим устройство всех промежуточных слоев по пути трафика от сервера в датацентре в интернет:

![img.png](_images/img_02.png)

По этому пути мы пройдем с самого начала, от серверного порта. Но сперва – немного про сами датацентры.

## Кратко про здания ДЦ

Нельзя рассказать о физическом устройстве сети без рассказа о том, где и как все это расположено, т.е. о самих помещениях ДЦ, поскольку здесь топология и размеры определяют почти всё.

Здания датацентров бывают разными, тут есть два варианта: то, что в строительстве называется Brownfield – ДЦ внутри существующих зданий и сооружений, для этого изначально не предназначенных, и Greenfield – ДЦ в специально спроектированном здании. Из-за этого конфигурация помещений ДЦ может заметно отличаться.

Наши ДЦ традиционно имеют трехсимвольные обозначения, сеть и ITDC используют общую кодировку: IVA, MYT, SAS, MAN, VLA. Везде, где будут ссылки на конкретный ДЦ, используются именно эти обозначения. ДЦ, обозначенные как Brownfield - IVA, MYT, SAS, представляют собой одно большое здание, заводской или фабричный цех в котором возводятся помещения для размещения оборудования и персонала ДЦ. Обозначенные как Greenfield - MAN,VLA построены по нашим проектам и разделены на несколько отдельных зданий. В обоих таких ДЦ сейчас по одному зданию, в VLA идет полным ходом строительство второго, а всего зданий в новых проектах может быть до 4-х. С точки зрения сети есть три отдельных зоны – зона серверов (Whitespace), зона сетевого оборудования уровня здания -коммуникационный центр(КЦ, Comm. Center), зона ввода внешних кабелей (Fmr).

Пространство, где находятся стойки с серверами называется whitespace и разделено на модули ДЦ, которые раньше называли по-строительному очередями, т.к. строили обычно их по одному. Сейчас строят минимум по два, так что это модули, или как их обозначают инженеры ДЦ IT-модули. Модуль в наших проектах ДЦ содержит от 32 до 360 серверных стоек, собранных в изолированные помещения. В эпоху газового пожаротушения комнаты были маленькие, но с ростом размеров ДЦ оно закончилось и теперь в одном помещении в Сасово можно и 360 стоек поставить.

На плане ДЦ все это обычно выглядит так:

![img.png](_images/img_03.png)

Если открыть противопожарную дверь в модуль,

![img.png](_images/img_04.png)

внутри будут блоки стоек, с изолированным дверями пространством между рядами. Это пространство называется горячим коридором и предназначено для эффективного удаления горячего воздуха (улетающего наверх), выдуваемого серверами, чтобы он не смешался с подающимся снаружи (падающим сверху) холодным воздухом. Это самая распространенная схема и в старых и в новых ДЦ, но в двух из них – IVA и SAS1 есть помещения, где вместо горячего коридора – коробчатый воздуховод к каждой стойке индивидуально.

![img.png](_images/img_05.png)

Над стойками, в два яруса, расположены лотки (cable trays), по которым прокладываются кабели и шинопроводы (bus bars), по которым к стойкам попадает электричество. Современные [«Cтойки 3.0»](https://wiki.yandex-team.ru/users/aksyonovv/rack3.0/) подключаются двумя трехфазными вилками к отводным коробкам с автоматами на bus bar.

В некоторых ДЦ электрические кабели идут прямо к распределительным панелям с розетками в каждой стойке и логически шинопровод заменяет электрощитовая с отдельными щитками, распределяющими питание по этим индивидуальным кабелям.

Серверов в наших стойках бывает от 10 до 72, в зависимости от модели.

Как это все связано между собой сетью и где что размещается, рассмотрим на примере самого современного ДЦ «Владимир», но все сети всех ДЦ состоят из одних и тех же логических блоков:

![img.png](_images/img_06.png)

На логической схеме видно, что часть сетевого оборудования находится в модулях, часть – в отдельном помещении коммуникационного центра (КЦ), которое в ДЦ незатейливо называют просто NOC.

Вся инженерная часть – электроснабжение, охлаждение IT-модулей в значительной степени независима, но сеть связывает модули вместе и с внешним миром. Чтобы работа сети в одном модуле не зависела от неполадок с электричеством или охлаждением в другом - общие элементы «выносятся за скобки». Кроме того, внутри модулей производятся масштабные монтажные работы, например, добавление/удаление стоек с серверами в сборе, каждая из которых весит 1.5 тонны (да, у них есть колёсики и они ездят), что может подвергнуть риску оборудование и кабели, так что мы стараемся максимально исключить зависимость одного модуля от другого.

Здания новых датацентров (начиная с Владимира) сделаны полностью модульным, т.е. можно его строить по частям и быстрее вводить сервера в эксплуатацию, а значит части сети, связывающие модули с внешним миром должны заработать уже тогда, когда некоторые модули еще строятся. Поэтому их мы ставим в отдельное помещение.

В ДЦ множество разнообразных инженерных систем, у которых есть масса разных датчиков, есть система видеонаблюдения, офисная сеть для сотрудников ДЦ, WiFi, телефонная и видеосвязь. Эта технологическая сеть независима от сети, в которой располагаются сервера, и подключается к сети Яндекса по отдельным каналам, которые обычно еще и резервируются через каналы, арендованные у различных операторов связи. Таким образом, даже если сервера в ДЦ полностью останутся без связи, операторы ДЦ и оборудование мониторинга будут связаны между собой, с центральным офисом и будут иметь выход в интернет чтобы эффективно решать возникшие проблемы. Это оборудование также должно работать еще до того, когда будут построены и включены модули с серверами, поэтому его тоже удобнее ставить в коммуникационный центр.

Подобнее про офисную и технологическую сети в ДЦ будет рассказано в третьей части.

Но наши ДЦ устроены по-разному, и во второй части будут приведены логические схемы для всех ДЦ. Таким образом, из схемы с путем прохождения пакета от сервера в интернет непосредственно в ДЦ находятся следующие слои сети:

**Стойка (ToR) - Фабрика ДЦ (S1&S2) - Граница ДЦ - DWDM**

## Фабрика ДЦ

Фабрика ДЦ у нас состоит из трех уровней, условно обозначенных на схеме “Стойка (ToR)” или S0 (он же Leaf), и двух слоёв интерконнекта, обозначенных “S1” и “S2”. В ToR как раз подключаются сервера.

### Сервера

Рядовой сервер в сети Яндекса имеет подключение к сети в виде одного интерфейса 1/10/25G. Поскольку разнообразных моделей серверов у нас немало, сетевых карт тоже имеется изрядный зоопарк форматов, но вот количество чипов для них и, соответственно, драйверов не особо велико. Рассмотрим все варианты.

#### 1G

Начнем с 1G – сетевые карты это практически 100% Intel onboard, т.е. Intel i350 и i82576 с разными буквами. Снаружи это всем известный разъем [RJ45](https://h.yandex-team.ru/?https%3A%2F%2Fru.wikipedia.org%2Fwiki%2FRegistered_Jack), в который включен обычный [UTP](https://h.yandex-team.ru/?https%3A%2F%2Fru.wikipedia.org%2Fwiki%2FВитая_пара) патч-корд. В Ивантеевке, Мытищах 5-6 и Сасово 1-4,6-7 большинство серверов именно такие.

![img.png](_images/img_07.png)

####  10G

Начиная с появления ДЦ MAN и до заказа 2019Q1 основным интерфейсом является конечно 10G, и вот тут царит максимальное разнообразие – 2 вида чипов и штук 5 разновидностей форм-факторов карт для разнообразных типов серверов в наших стойках. Чипы представлены следующими:
- [Intel 82599](https://h.yandex-team.ru/?https%3A%2F%2Fwww.intel.ru%2Fcontent%2Fwww%2Fru%2Fru%2Fproducts%2Fnetwork-io%2Fethernet%2Fcontrollers%2F82599es-10-gigabit.html), самый массовый
- [Mellanox ConnectX-3](https://h.yandex-team.ru/?http%3A%2F%2Fwww.mellanox.com%2Fpage%2Fproducts_dyn%3Fproduct_family%3D162%26mtag%3Dconnectx_3_pro_en_card)
Сами карты в серверах либо обычные PCI, либо разнообразные mezzanine-cards для разных платформ (Gigabyte, Quanta), либо карточки [формата OCP](https://h.yandex-team.ru/?https%3A%2F%2Fwww.opencompute.org%2Fwiki%2FServer%2FMezz) в новых серверах в стойках нашего собственного дизайна. Чтобы было понятно, на что эти карты похожи – загляните в [wiki инженеров ДЦ](https://wiki.yandex-team.ru/dorofeevdv/1/components/#setevyekarty). Наружу у всех этих карт смотрит специальный разъем [SFP+](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSmall_form-factor_pluggable_transceiver%23SFP%2B) и подключаются они специальными кабелями SFP+/SFP+, либо 4xSFP+/QSFP, они на фото ниже.

|   |   |
|---|---|
| ![img.png](_images/img_08.png) | ![img.png](_images/img_09.png)

Это медные кабели известные как [DAC-кабели](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTwinaxial_cabling), они обеспечивают качественную передачу сигнала на расстояние до 5м, т.е. для уровня стойки их более чем достаточно.

{% cut "Почему не витая пара, есть же замечательный стандарт 10GBaseT?" %}

Дело в том, что у 10G по витой паре есть пара серьезных недостатков, делающих его нишевым продуктом в ДЦ. У него значительно больше latency (задержка распространения сигнала) и энергопотребление. Плохая энергетика мешает делать на нем сетевое оборудование и портит экономику большим ДЦ, т.к. более низкая цена серверного кабеля не компенсирует потерю электричества за время жизненного цикла сервера. Сетевое оборудование более дорогое из-за тепловыделения, сильно меньше его выбор. Поэтому 10G по витой паре за пределами корпоративных серверных на три стойки не находит применения.

{% endcut %}

### 25G

Начиная с заказа 2019Q1 все сервера закупаются с интерфейсами 25G, до этого такие карточки покупали только для кластера [YT "Арнольд"](https://yt.yandex-team.ru/arnold/), но они там в 10G режиме пока работают. Карточки формата OCP на базе чипа [Mellanox ConnectX-4](https://h.yandex-team.ru/?http%3A%2F%2Fwww.mellanox.com%2Frelated-docs%2Fprod_silicon%2FPB_ConnectX-4_Lx-EN_IC.pdf). Разъем SFP28 визуально от SFP+ не отличается, кабели тоже, но они имеют куда более высокие характеристики, позволяющие поднять линейную скорость до 25G. Пока 25G в проде есть только на кластере [YT “Landau”](https://yt.yandex-team.ru/landau/), серверах видеотрансляций и L3-балансерах.

Как все вышеперечисленное выглядит в жизни, можно увидеть на фото:

|   |   |   |
|---|---|---|
|![img.png](_images/img_10.png) | ![img.png](_images/img_11.png) | ![img.png](_images/img_12.png) |

На фото слева как раз представлены стойки с 1G «полунодами» из Сасово, в стойке 72 сервера – максимальная плотность на стойку у нас. Современные «Cтойки 3.0» с 10G подключениями из Владимира, где серверов 36 –на среднем фото. Ну и справа обычная серверная стойка с необычными GPU-серверами Tesla, которых в стойке совсем мало. 25G и 10G подключения отличаются только некоторой разницей в цене, по фото этого не видно.

Возможно для кого-то будет новым факт, что сетевые карты, устанавливаемые в серверах – область ответственности серверного R&D и закупок серверного оборудования, а затраты на них включаются в стоимость сервера и в серверный бюджет.

## ToR – коммутатор уровня стойки, S0 (Leaf)

Сервера подключаются внутри стойки в самый нижний уровень сети – уровень доступа, в коммутатор, который называется ToR-свитчом, который соединяет сервера в стойке. ToR значит Top-of-rack, но в Яндексе никогда свитчи в верхний юнит стойки не монтировались. В совсем старых, уже закрытых ДЦ, где перегревы не были редкостью, свитчи ставили в самые нижние юниты стойки, поскольку из-под фальшпола подавался холодный воздух и свитч при перегреве умирал последним, а остывал первым. Когда это стало не актуально, свитчи переехали в середину стойки – это экономит длины кабелей и инженерам с ними гораздо удобнее работать. Т.е. у нас были Bottom-of-Rack свитчи, а стали Middle-of-Rack, но конечно все используют общепринятую терминологию.

Посмотрим поближе:

![img.png](_images/img_13.png)

Вот он, ToR в 23 юните, в который приходят DAC-кабели от серверов, с синими «ручками». Конкретно на фото изображен [Huawei CE6870](https://h.yandex-team.ru/?https%3A%2F%2Fe.huawei.com%2Fen%2Fmaterial%2FonLineView%3Fmaterialid%3De0d3997fe2ba485eaac87f2ec88d08f1), самый массовый наш 10GE свитч на чипе [Broadcom Qumran](https://h.yandex-team.ru/?https%3A%2F%2Fwww.broadcom.com%2Fproducts%2Fethernet-connectivity%2Fswitching%2Fstratadnx%2Fbcm88370%2F).

{% cut "А что за коммутатор, внизу, со связкой плоских кабелей?" %}

Это коммутатор отдельной сети управления серверными портами IPMI, о которой будет рассказано в третьей части. В новых стойках этот свитч встроен в карту управления стойкой в виде модуля.

Cписок моделей ToR-свитчей:
- 1G Huawei CE5850 (48х1G RJ45, 4x10G SFP+, 2x40G QSFP+)
- 10G Huawei CE6850 (48x10G SFP+, 4x40G QSFP+)
- 10G Huawei CE6870 (48x10G SFP+, 6x100G QSFP28)
- 25G Mellanox MSN2410 (48x10G SFP28, 8x100G QSFP28)
- 25G Huawei CE6865 (48x10G SFP28, 8x100G QSFP28)
- 40G Cisco N3K-C3132Q (32x40G QSFP+)
- 40G Huawei CE7850 (32x40G QSFP+)
- 100G Mellanox MSN2100 (16x100G QSFP28)
- 100G Huawei CE8850-32CQ (32x100G QSFP28)
В скобках приведена т.н. портовая формула – количество и тип интерфейсов в коммутаторе.

{% endcut %}

{% cut "Почему тут есть свитчи с 40G и 100G портами, а у серверов только 10 и 25?" %}

Потому, что 40G и 100G образованы четырьмя отдельными каналами (lane), 4х10G и 4x25G, т.е. любой такой порт можно разделить на 4 отдельных линка с помощью специального разветвительного (fan-out) кабеля (жаргонное наименование «Гидра»), что мы и делаем, подключая в такой коммутатор в один порт сразу 4 сервера. Такие коммутаторы позволяют иметь возможность гибко менять количество портов для серверов и для включения стойки в сеть. Из минусов такого решения – один кабель на четыре сервера, что усложняет его замену и вводит дополнительные зависимости.

Именно по портам ToR коммутатора проходит граница зоны ответственности между владельцем сервера и сетью. Обслуживает сервера, сетевые карты и кабели серверов команда ITDC, но поскольку NOC не может сам принимать решение об отключении сервера от сети или питания для починки, ответственность за работу кабеля и сетевой карты лежит на команде, владеющей сервером.

Cерверные подключения, называются Downlink, или ToR access. Cостояние всех портов в виде графиков загрузки и разных типов потерь, видимых в виде счетчиков на свитчах, можно посмотреть в Grafana и Solomon, куда специальный сборщик статистических данных НОКа – Grad их экспортирует. Т.к. портов серверов сотни тысяч, а линки серверов находятся в зоне ответственности владельцев железных машин - в НОК создан специальный сервис, отслеживающий проблемы на них – LinkEye.

Также должно броситься в глаза жуткое, с точки зрения обычных enterprise-сетей, безобразие в виде единственного свитча и порта в сервере. Но здесь масштаб диктует свои законы – резервирование сети на уровне ToR удорожает сеть где-то на 2/3 и делает ее стоимость сравнимой со стоимостью серверов. Так что приходится смириться с отсутствием возможности обслуживать ToR свитч без отключения стойки и терять целую стойку в случае аварии коммутатора.

Из ToR выходят сиреневые оптические кабели, уходящие куда-то наверх – как не трудно догадаться, это кабели в сторону следующего уровня сети, соединяющего стойки между собой, уровня коммутационной фабрики ДЦ. В терминологии такой кабель – Uplink. Кабель этот оптический, называется он патч-кордом (patch cord), на фото - MPO12 MM OM4.

- MPO12 – Тип разъема, вот такой

![img.png](_images/img_14.png)
- MM – Тип оптического волокна - многомодовое ([Multi-mode](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMulti-mode_optical_fiber))
- OM4 – Характеристика волокна – [полоса пропускания](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FModal_bandwidth) – 3500 MHz/km @ 850nm – необходима для передачи 100G сигнала на расстояние до 100м.

{% endcut %}

Цвет кабеля определяет его тип, но для каждого типа кабеля в стандарте используется несколько цветов. Поскольку кабелей мы закупаем очень много, плюс много различных длин, отслеживание конкретных цветов сильно усложняет закупки и логистику, так что в одной стойке легко могут оказаться кабели двух-трех цветов, обозначающих один и тот же тип. Это портит красоту на фотографиях, но больше ни на что не влияет. В таком кабеле 12 оптических волокон и сигнал передается сразу по 4-м в одну и по 4-м в другую сторону, каждый lane в 25G по своей паре волокон. В этом ДЦ каждый такой линк стандарта [100GBaseSR4](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2F100_Gigabit_Ethernet), 100-гигабитный. Линков таких 4, т.е. доступная полоса на эту стойку не может превышать 400G.

Включены кабели в трансивер, преобразующий по-разному закодированный и идущий по различным типам кабелей (и требующий разных типов передатчиков/приемников) сигнал в универсальный электрический интерфейс в коммутаторе, здесь конкретно - [QSFP28](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSmall_form-factor_pluggable_transceiver%23100_Gbit%2Fs_QSFP28).

Кабели уходят вверх, в [патч-панель](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fw%2Findex.php%3Fsearch%3Dpatch-panel%26title%3DSpecial%253ASearch%26go%3DGo), которая позволяет не тянуть индивидуальный кабель для каждого порта, а прокладывать их большими жгутами и использовать для подключения специальные кабели стандартной длины, а также легко их менять в случае повреждения. Это собственно и есть так называемая [СКС – структурированная кабельная система](https://h.yandex-team.ru/?https%3A%2F%2Fru.wikipedia.org%2Fwiki%2FСтруктурированная_кабельная_система). Патч-панели для подключения стоек располагаются над ними, на кабельных лотках, т.к. стойки мобильны и их иногда могут двигать, кроме того, сеть собирается и тестируется до момента их приезда в ДЦ, что крайне важно.

![img.png](_images/img_15.png)

С подключением сервера и стойки к сети мы разобрались и плавно переходим непосредственно к устройству датацентровой фабрики.

## Устройство коммутационной фабрики (Spine 1 & 2)

Итак, в каждой стойке уже стоит по коммутатору, теперь нужно их соединить вместе.

Все сервера внутри ДЦ связывает между собой конструкция, называемая коммутационной фабрикой. Это неблокируемая многослойная конструкция, представляющая собой в математическом смысле так называемую [сеть Clos-а](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FClos_network), названную в честь американского математика Чарльза Клоса (Charles Clos), теоретически описавшего подобные сети. Именно так устроена сеть в новых ДЦ MAN 3-5, SAS 5,7-9, VLA 1-5.

В терминологии сети, независимый блок стоек нижнего уровня сети Clos-а называется PoD–Point-of-Delivery. Размер PoD в сети определяется количеством портов, которые можно собрать в один коммутатор, соединяющий стойки, radix. Если объединять стойки большими модульными коммутаторами, у которых сотни портов, можно сделать PoD совпадающим размером с целым модулем ДЦ, что и проделано в MAN в модулях 3-5 и VLA в модулях 1-5. В Сасово, в модулях 8 и 9 из-за размера в 360 стойко-мест - два отдельных PoD. В более старых ДЦ, как в Сасово 1-4, 6-7, где скорости ниже, PoD даже больше одного модуля и может объединять до 700 стоек. В совсем новом проекте, по которому мы будем строиться в этом году, т.е. начиная с VLA 6 размер PoD будет всего 32 стойки.

Проще всего наглядно себе представить логику связей вот таким образом:

![img.png](_images/img_16.png)

Каждый слой фабрики – перпендикулярная плоскость, соединяющая все устройства предыдущего уровня. Чем меньше radix, тем больше слоев требуется, чтобы соединить одно и то же количество устройств уровнем ниже. Поскольку у нас не теоретическое пособие по устройству сетей датацентров, рассмотрим все на примере конкретного ДЦ во Владимире, а во второй части по той же схеме будет описание всех наших ДЦ по отдельности.

Владимир, здание "Альфа"

![img.png](_images/img_17.png)

Нижний уровень – 4 стойки в каждом модуле, к которым подключены все ToR-свитчи внутри модуля – S1. Верхний уровень - 4 блока из 4-х свитчей верхнего уровня (S2), которые находятся в КЦ. В именах устройств свитчи S1 называтся «-D свитчи», а S2 называются «-X свитчи».

Линк от сервера в основном 10GE, за исключением 15 стоек кластера YT “Landau”, где уже 25G.

Все линки между уровнями – 100GE, всего их, как нетрудно догадаться 5 модулей x 180 стойко-мест x 4 аплинка=3600=**360 Терабит**. Как это выглядит в жизни:

|   |   |
|---|---|
|![img.png](_images/img_18.png)|![img_1.png](_images/img_19.png)|

На каждом фото **1/20** часть уровня **S1** в здании, cтойка S1, с модульными коммутаторами, слева - [Huawei CE12812](https://h.yandex-team.ru/?https%3A%2F%2Fe.huawei.com%2Fen%2Fproducts%2Fenterprise-networking%2Fswitches%2Fdata-center-switches%2Fce12800%3Fsource%3Dcorp_comm) и справа - [Juniper QFX10016](https://h.yandex-team.ru/?https%3A%2F%2Fwww.juniper.net%2Fus%2Fen%2Fproducts-services%2Fswitching%2Fqfx-series%2Fqfx10000%2F).

Модульные коммутаторы представляют из себя специальные шасси, в которые спереди вставляются линейные карты с интерфейсами, фактически – отдельные коммутаторы, а сзади, перпендикулярно, – карты фабрик коммутации. Фабрики – тоже отдельные коммутаторы, но с портами, смотрящими только внутрь. По сути модульный коммутатор является такой же сетью Clos-а, но внутри одной коробки. Всеми картами управляет отдельный управляющий модуль, на котором есть CPU и операционная система в которой живут сетевые протоколы.

![img.png](_images/img_20.png)

Во Владимире сеть собрана на оборудовании сразу двух производителей. Всего таких стоек 4шт. в модуле и 20шт. в здании. Над коммутатором – патч-панели в сторону блоков S2 в помещении КЦ. Под ним – патч-панели в сторону серверных стоек.
Эта же стойка сзади:

![img_1.png](_images/img_21.png)

На фото видны электрические кабели двух цветов – красного и синего. Все стойки S1 подключаются сразу к двум отдельным шинопроводам питания, чтобы отключившийся автоматический выключатель в случае аварии не обесточил стойку. Разные цвета нужны, чтобы при коммутации не перепутали блоки питания и не подключили все только к одному шинопроводу, а потом при каких-нибудь работах отключение не вышло бы незапланированным. На фото видно как раз яркие цветные блоки распределения питания – PDU Raritan. Они, кстати, интеллектуальные и позволяют замерять потребляемую мощность и отключать нагрузку удаленно.

Панели на фасаде вверху стоек S1 в сторону S2:

![img.png](_images/img_22.png)

S2 располагается в специальном помещении коммуникационного центра (КЦ). Помещение КЦ имеет дублированные вводы электропитания, от двух отдельных модулей ДЦ, отдельную от остального пространства резервированную систему охлаждения, все сетевое оборудование подключается к двум отдельным панелям распределения. Таким образом, уровень S2 функционирует независимо от серверных модулей и переживает отключение любого из них. Это может выглядеть не очень с точки зрения катастрофоустойчивости, но нашем проекте подразумевалось распределение верхних уровней Spine между отдельными заданиями, а не между модулями.

![img.png](_images/img_23.png)

Уровень S2 собран из 4-х отдельных блоков по 3 стойки, в каждом блоке по 4 модульных коммутатора и отдельная стойка с патч-панелями от всех стоек S1.

Для иллюстрации взята фотография с монтажа, на ней весь блок видно наглядно, подключен только один модуль из 5 и заведены кабели от второго.
А сейчас это же место выглядит вот так:

![img.png](https://jing.yandex-team.ru/files/vdvolovik/img_24.png)

На фото видно блоки панелей (по 8 шт. через заглушку) от всех 5 модулей, кабели от которых расходятся в 4 модульных коммутатора. Всего 4 блока по 3 стойки, итого 12 стоек. Легко заметить, что половина слотов не используется, это резерв для подключения к соседним зданиям в будущем.
S2 целиком:

![img.png](https://jing.yandex-team.ru/files/vdvolovik/img_25.png)

Постепенно мы подобрались к границе датацентра. В эту сторону уходят кабели от всех блоков S2, но на фото их не видно – специальный желтый короб доставляет их в противоположный ряд стоек в КЦ, в которых размещено граничное оборудование ДЦ – собственно свитчи, образующие границу ДЦ и оптический транспорт.

## Граница ДЦ

Граница ДЦ в разных дизайнах сети может выполнять разные роли и очень по-разному делаться. В нашей нынешней сети это две основные компоненты – коммутаторы типа Edge, такие же как в S1 или S2, подключенные к ядру сети, и сервисные стойки. Но прежде чем начать рассматривать магистраль (куда входят как раз ядро и транспортная сеть между ДЦ), посмотрим на сервисные стойки.

### Сервисные стойки
Сервисные стойки предоставляют высокоуровневые сервисы сети нашим проектам, такие как:
- Балансировка нагрузки на 3м уровне – L3 load balancer/Decapsulator
- Файрволл – Firewall (внешний)
- NAT64
- TUN64
Реализуются эти функции на серверах, главным отличием которых является скорость их подключения к сети.

Фактически сервисные - обычные, не предсобранные серверные стойки. Из-за нашего требования иметь двойное подключение питания, которое в Стойке 3.0 не предусмотрено, приходится пока по старинке использовать 1RU сервера. Хоть и подключается Стойка 3.0 двумя отдельными трехфазными кабелями, на самом деле это не резерв, сервера просто распределены по вводам. Сервисная же стойка, как и S1, запитана от двух шинопроводов с разных сторон коридора, на случай срабатывания автоматического выключателя или работ на шинопроводах. Конкретно в ДЦ Владимир сервисные стойки стоят в модулях, по одной в каждом.

Еще одним важным отличием сервисных стоек является резервированное подключение серверов – в них по два ToR-свитча (во Владимире - двух разных производителей).

![img.png](https://jing.yandex-team.ru/files/vdvolovik/img_26.png)

Приглядимся:

![img.png](https://jing.yandex-team.ru/files/vdvolovik/img_27.png)

Обратите внимание на защелки на электрических кабелях – они не позволяют вырвать вилку из гнезда и случайно что-нибудь в этой стойке отключить.

Коммутаторов в сервисных стойках оказывается 4 штуки. Это потому, что нужно еще скоммутировать IPMI-порты для управления серверами, а они здесь есть в каждом сервере, и 1G встроенные (onboard) порты.

Серверы включаются либо 2-4x10G, либо 2x25G или даже 2x100G портами каждый (не забываем, здесь все резервируется), а скоммутированный 1G порт дает возможность не тратить ресурсы на поддержку загрузки по сети через 100G карты. Это дает нам большую гибкость, позволяя быстрее менять производителя карт и чипов.

Для гигабитной сети используются старые 1G коммутаторы Huawei 5300 и Cisco 2960G, которые уже отработали один свой срок в одном из ДЦ и таким образом мы почти не тратим денег на это удобство. Оборудование получает вторую жизнь и приносит пользу на протяжении минимум 10 лет. Два свитча с уже знакомыми DAC-кабелями как раз реализуют сервисный уровень сети, мы их называем «-A» свитчами. Это модели
- 100G Huawei CE8850-32CQ (32x100G QSFP28)
- 100G Juniper QF5200-32C-AFI (32x100G QSFP28)

В сервера мы ставим всегда самые современные сетевые карты, сейчас на чипах ([ASIC](https://h.yandex-team.ru/?https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FApplication-specific_integrated_circuit)) [Mellanox ConnectX5](https://h.yandex-team.ru/?http%3A%2F%2Fwww.mellanox.com%2Fpage%2Fproducts_dyn%3Fproduct_family%3D260%26mtag%3Dconnectx_5_en_card) и [Chelsio Terminator (T6)](https://h.yandex-team.ru/?https%3A%2F%2Fwww.chelsio.com%2Fterminator-6-asic%2F), обычного формата PCI. Пока писался этот текст [Mellanox был куплен NVidia](https://h.yandex-team.ru/?https%3A%2F%2Finvestor.nvidia.com%2Fevents-and-presentations%2Fevents-and-presentations%2Fevent-details%2F2019%2Fnvidia-to-acquire-mellanox%2Fdefault.aspx), так что в жанре сетевых карт для highload мы кажется по-прежнему не будем видеть Intel (они не договорились, хотя тоже пытались купить Mellanox), - зато скорее всего увидим тандем сетевого ASIC и GPU, будет интересно.

Толстые черные кабели с трансиверами QSFP28 на концах – DAC кабели для 100G портов. Да, 100G бывает и медным.

## Граница ДЦ (Edge-свитчи)

Теперь непосредственно про границу ДЦ. В современных ДЦ два свитча edge, подключенных ко всем блокам S2. и несколько стоек с транспортным оборудованием, все это стоит КЦ. Когда-то на границе ДЦ мы ставили большие модульные маршрутизаторы Juniper MX960/MX2010, но сейчас можем использовать такие же свитчи, как и в фабрике ДЦ.

Цена порта маршрутизатора отличается от коммутатора минимум в 10(!) раз и такая разница оправдывала любые жертвы. Как и следует из названия, граничные коммутаторы ДЦ включаются магистральными каналами через оптическое транспортное оборудование в ядро сети датацентров Яндекса и резервируют друг друга. Стойки с ними находятся в КЦ и визуально почти не отличаются от S1, поэтому не буду приводить фото.

![img.png](_images/img_28.png)

## Транспорт, магистральные линии и ядро сети

Яндексу нужны огромные датацентры, для размещения десятков тысяч серверов каждый, а такие объекты потребляют десятки мегаватт электроэнергии, поэтому строятся они в тех местах, где есть дешевая энергия. Подчас это довольно далеко от мест, где концентрируются операторы связи и пользователи. Поэтому нам приходится строить, конечно не своими руками, а привлекая специализированные строительные компании, эти кабели, потому что арендовать или купить их в таких местах обычно не получается. Про строительство ВОЛС (волоконно-оптических линий связи), сами магистральные кабели и все около (кроссы, разъемы и т.д.) я подробно рассказывать не буду, сошлюсь на несколько публикаций на HABR:
- [Как строят оптические линии в Москве](https://h.yandex-team.ru/?https%3A%2F%2Fhabr.com%2Fru%2Fcompany%2Fdataline%2Fblog%2F432220%2F)
- [Строительство ВОЛС глазами сварщика](https://h.yandex-team.ru/?https%3A%2F%2Fhabr.com%2Fru%2Fpost%2F193146%2F)

По некоторым направлениям мы арендуем волокна в чужих кабелях, а в датацентр в Финляндии (MAN) арендуем каналы связи Москва-Хельсинки у нескольких операторов.

Каждый ДЦ подключается к обеим половинам ядра отдельными оптическими кабелями, т.е. в каждый ДЦ приходит минимум два кабеля от разных узлов ядра сети. Кроме того, мы обычно пытаемся построить третий кабель, напрямую между двумя ближайшими ДЦ. Этого требуют, как соображения надежности, так и необходимость минимизировать задержки (RTT, round-trip delay) между соседними ДЦ, как в паре Владимир-Сасово.

Почему двух обычно недостаточно? Многим кажется, что простого резервирования 1+1 должно хватать, но тут надо учесть, что регулярно случаются аварии и плановые работы, выводящие один из кабелей из строя на достаточно долгий период, часов на 8-10, а иногда даже до суток. Риск двойного обрыва весьма велик, особенно если знать, что наши ДЦ нередко находятся на расстоянии многих сотен километров от ядра сети в Москве, а это требует усиливать сигнал по трассе, на необслуживаемых усилительных пунктах через каждые 50-150км. Этим станциям усиления нужно электричество для работы и проблемы с ним тоже могут привести к отключению каналов связи, хотя там конечно и используются источники бесперебойного питания с батареями большой емкости.

Таким образом, в ДЦ приходит минимум два, а обычно три оптических кабеля. Эти кабели приходят к ДЦ с двух сторон, нигде не имея общих участков, входят в здание в разных местах и попадают в специальные помещения для кабельных вводов (FMR, fiber management room). Нужны специальные отдельные помещения по нескольким причинам:

1. Безопасность. Кабель, идущий внутри здания и идущий по улице сильно отличается, т.к. сделан под разные требования. В грунте кабель, например, может быть горючим и защищенным стальной броней из намотанного троса или гофрированной стальной ленты. При этом в здании он должен не поддерживать горение сам, не выделять ядовитых веществ при горении и быть диэлектрическим, чтобы не создавать помех и опасности для персонала ДЦ, где расположено множество мощных электроустановок. Переход кабеля с одного типа на другой делается в FMR, там же заземляются оболочки внешних кабелей. По кабелю может распространиться огонь снаружи, поэтому комнаты изолированные, с противопожарными дверями. С кабелем проводят работы, под час длительные, внешние подрядные организации. Отдельная комната позволяет выполнять такие работы без доступа посторонних в помещения ДЦ. Для всяких вспомогательных задач операторы связи и подрядчики могут устанавливать свое оборудование, которое тоже можно обслуживать, не заходя в другие помещения ДЦ.
2. Надежность. Комнат FMR минимум две, в разных местах здания, чтобы никакие работы и аварии в них не могли затронуть кабели, приходящие с разных направлений и оставить ДЦ без связи.

![img.png](_images/img_29.png)

На фото оптические кроссы, в которые приходят 2 внешних кабеля нашей сети (третий кабель приходит в другой FMR) и один кабель провайдера Megafon, который используется в качестве резервного для сети управления ДЦ. Далее волокна из этих кабелей подключаются в транспортное оборудование, использующее технологию спектрального уплотнения каналов или DWDM, находящееся в КЦ.

Технология позволяет собирать множество потоков с электрических интерфейсов (таких же, как в коммутаторах), кодировать их в оптический сигнал на разных длинах волн, собирать их вместе в один групповой сигнал, усиливать его и передавать на расстояние в тысячи километров по оптоволоконному кабелю:

![img.png](_images/img_30.png)

Транспортные сети могут иметь разнообразную топологию, но мы, в силу специфики соединения больших датацентров, используем схему в виде двойной звезды с центром в Москве. Для DWDM сетей, часто использующихся на трансконтинентальную дальность, наши расстояния до 1000км весьма невелики и все связи между ДЦ в нашем дизайне это линки типа точка-точка, т.е. просто труба пропускной способностью Nx100G. В каждом ДЦ на разных линках в сторону ядра используется оборудование разных производителей, всего используется оборудование трех вендоров. Современное, производства ADVA и Infinera/Coriant и уже немного устаревшее - Ekinops, которое мы постепенно переносим в Ивантеевку, заменяя его в Сасово и Мытищах на более современное.

Как это выглядит:

![img.png](_images/img_31.png) ![img.png](_images/img_32.png)

В центре модуль собственно мультиплексора-демультиплексора, в который сходятся все отдельные каналы, это может быть 100 или 200G, в зависимости от схемы кодирования. Чем меньше расстояние, тем больше можно в один спектральный канал упаковать данных с электрических интерфейсов. Уже появились на рынке модули с пропускной способностью 8x100G в одном спектральном канале (отдельная длина волны). Стандарт определяет сетку из 80 длин волн, что дает теоретически на одной паре волокон 80 каналов по 800G, т.е. 64T. Внизу расположено шасси с транспондерами, модулями, которые стандартные интерфейсы 100G кодируют и отправляют их в виде оптического сигнала на определенной длине волны в мультиплексор (жгут из желтых патч-кордов). В верхней корзине размещаются усилители, регуляторы уровней сигналов, модули управления и прочие вспомогательные вещи.

После мультиплексора собранный уже групповой сигнал усиливается усилителем и отправляется в путешествие по оптическому кабелю через помещение FMR в ядро сети или другой датацентр.

Но ВОЛС несколько сложнее просто кабеля в земле. На расстоянии в сотни километров сигнал затухает и его нужно усиливать повторно, на узлах, которые называются, как уже было сказано «необслуживаемый усилительный пункт». Необслуживаемый, что забавно, у телефонистов означает всего лишь, что на этих узлах нет персонала, а так конечно обслуживать это оборудование необходимо. Усилители располагаются по трассе на расстоянии 50-150 км. друг от друга. Размещаются они либо в наших собственных контейнерах, либо в контейнерах операторов связи, либо на узлах местных операторов в технических помещениях. Контейнеры находятся часто в весьма отдаленных местах, вот например наш контейнер на окраине деревни Колпь на границе владимирской и рязанской областей:

![img.png](_images/img_33.png) ![img.png](_images/img_34.png)

На фото достраивают забор с колючей проволокой вокруг контейнера (мы эти узлы строим по классическим операторским стандартам из 90-х годов, в нашей стране лучше перестраховаться - колючая проволока, клетки для наружных блоков кондиционеров и т.д.). В контейнере размещается электрощит, кондиционеры, стойка с корзиной с оптическими усилителями, ИБП (UPS). На заднем плане видно столб с нашим собственным трансформатором, мы предпочитаем по возможности подключаться к сетям уровнем повыше чем местная сеть 0.4kV, для надежности. Усилительных пунктов по трассе может быть много, для примера устройство трассы M9-SAS после завершения модернизации, 6 усилительных пунктов:

![img.png](_images/img_35.png)

![img_1.png](_images/img_36.png)

На узлах М9 (ДЦ ММТС-9, ул. Бутлерова 7) и STD (ДЦ StoreData, ул.Нижегородская 32) DWDM-транспорт подключается в оборудование ядра сети. Почему на схеме нет датацентра в Финляндии? Дело в том, что через границу нам приходится арендовать все каналы связи у четырех операторов, собственной трансграничной транспортной сети у нас нет. Но в Финляндии есть небольшая отдельная транспортная сеть, связывающая три популярные у операторов связи площадки в Хельсинки с датацентром в г.Mäntsälä.

![img_2.png](_images/img_37.png)

Как всем известно, мы эксплуатируем не один, а целых 5 собственных датацентров. Чтобы они могли передавать данные между собой и в интернет, служит специальная часть сети, т.н. "ядро" (core). Поскольку это самая важная часть сети, проблемы в которой касаются сразу всех пользователей, эта часть сети разделена на две половины, находящиеся в двух разных коммерческих датацентрах в г. Москве между 3-м кольцом и МКАД-ом, конкретно на 10 этаже ММТС-9 (M9) и в StoreData. Выглядит часть ядра (магистральный узел или hub) как несколько шкафов с сетевым оборудованием, коммутирующим и транспортным. На фото ниже – как раз ядро сети и точка присутствия на М9. Пока они размещаются в наших стойках в общем зале на 10 этаже, но мы сейчас начали арендовать отдельную «клетку» внутри, куда будем переезжать ядром и всем прочим.

![img_3.png](https://jing.yandex-team.ru/files/vdvolovik/img_38.png)

Каждый магистральный узел, в свою очередь, тоже делится на две части. Они имеют одинаковую пропускную способность, но собраны на оборудовании разных производителей. Это делается для того, чтобы ошибки и дефекты в оборудовании одного производителя (vendor) можно было обойти, отключив половину узла.

Вообще-то, у нас две сети (ну так-то 4, но про остальные две в третьей части), условно называемые Backbone – обычная сеть, и Fastbone – сеть для выкладки данных. Вторая сеть была порождена в древние времена наличием у нас поиска с его огромной базой и раскладкой индекса на почти все сервера. Вторая сеть имеет сильно более простую топологию и меньше степень резервирования. Поэтому, на магистральных узлах размещено по три устройства (2 ноды backbone, 1 нода fastbone) на каждом из двух.

![img_4.png](https://jing.yandex-team.ru/files/vdvolovik/img_39.png)

На фото половинка одного из магистральных узлов (М9), т.е. 1/4 ядра. Верхний коммутатор относится к backbone, нижний к fastbone. К вот этим узлам двумя независимыми путями и через разное транспортное оборудование подключаются все ДЦ Яндекса и все т.н. "точки присутствия" (PoP, points of presence). На этих точках мы держим оборудование для подключения к операторам связи, чтобы доставлять трафик к нашим пользователям. Таких площадок много и оборудование на них как раз образует границу сети. Чтобы понять, что на них происходит придется немного рассказать про то, на основе чего строится интернет.

## Внешние связи

### Как операторы связи взаимодействуют друг с другом
Интернет по-сути – сложившаяся исторически на основе экономических интересов совокупность связей между сетями операторов связи и/или корпоративными сетями. Когда к оператору приходит клиент, или два оператора решают что взаимного трафика между сетями стало слишком много - появляется экономический интерес завести прямую связь, чтобы этот трафик дешевле обходился. Линк стоит денег, чем он длинней - тем он дороже, и иногда менее эффективен вдобавок. Появляется экономический интерес сделать его подешевле, а значит его надо сделать там, где уже по какой-то причине есть сеть, расширять существующие узлы обычно дешевле строительства новых. Поэтому возникают точки концентрации связей, которые как черные дыры собирают трафик в себя. Исторически они возникают в местах, где были какие-то всем нужные ресурсы и их потребители, как-то:
- Места выхода трансатлантических кабелей на берег
- Финансовые центры, поскольку именно банки и биржи исторически были главными и платежеспособными потребителями передачи данных как таковой
- Международные узлы классической телефонии – первый источник каналов связи для передачи данных
Если смотреть на карту связей интернета на [Telegeography.com](https://h.yandex-team.ru/?https%3A%2F%2Fwww.telegeography.com), становится хорошо видно, что желающие иметь за минимальные деньги максимальное число связей с миром просто не могут миновать основные финансовые центры, в Европе – Amsterdam, Frankfurt, London, Paris. Мы тоже в стороне не остаемся и имеем свои выносы в Амстердам, Хельсинки, Франкфурт в Европе, а также в Ashburn,VA в США (это как раз одна из главных точек выхода кабелей в Европу на побережье, сейчас в итоге этот небольшой городишко просто «страна датацентров»).

![img_5.png](_images/img_40.png)

В РФ такой точкой стала, еще начиная еще с эпохи UUCP и модемов, международная АТС ММТС-9 в Москве, где и cлучился первый в стране и один из крупнейших в мире IXP (Internet-Exchange point) – [MSK-IX](https://h.yandex-team.ru/?https%3A%2F%2Fwww.msk-ix.ru%2Ftraffic%2F). Сейчас это хороший современный коммерческий датацентр, в котором доступны любые мыслимые услуги связи. Выкупивший его Ростелеком на вид не пытается его переварить и насадить свои порядки, так что мы надеемся, что эта площадка останется на долгие годы удобнейшим местом для связи сетей в РФ. Но в каждом городе точек, где удобно стыковаться с операторами несколько, так что у нас организовано по нескольку узлов в нескольких городах. В РФ это Санкт-Петербург (2 узла), Москва (3 узла и несколько мелких выносов), Екатеринбург.

### Типы внешних линков

Линки с различными операторами, в зависимости от того, до каких сетей через них можно добраться, бывают нескольких типов (тип услуги):
- Пиринговый линк (peering) – через него видно только сети оператора и его клиентов
- Транзитный (ip-transit link) – через него видно весь интернет
- Выделенный (private) – канал из точки в точку, например к офису, или к определенному сервису, например к шлюзу IP-телефонии
- Клиентский порт – линк в сторону того, кому предоставляется доступ в интернет, клиент обычно платит за доступ по тарифу

Мы не оператор связи, так что доступ можем предоставлять только безвозмездно, т.е. даром. В качестве клиента у нас выступают, например, ноды корневого сервера имен DNS [L-root](https://h.yandex-team.ru/?https%3A%2F%2Fwww.dns.icann.org%2Fimrs%2F). Мы их держим на своих узлах в Хельсинки, Москве и Екатеринбурге, чтобы помочь Рунету и нашей [Zora](https://wiki.yandex-team.ru/zora/) иметь быстрый и качественный доступ к корню DNS.

Пиринговые порты, которых у нас большинство, это взаимовыгодный обмен трафиком между операторами, часто он не подразумевает оплату. Транзитные порты оплачиваются в зависимости от утилизации за каждый мегабит используемой полосы. Каждый оператор обычно платит вышестоящему оператору (upstream) за услугу IP-transit, и имеет некоторое количество бесплатных пиринговых линков.

Есть некоторое количество операторов, которые образуют ядро интернета, самый верхний уровень – операторы Tier1. Это самые крупные операторы, вырастившие себе магистральные сети по всему миру, подключившие к себе возможный максимум операторов поменьше и отстоявшие свое право никому не платить в «пиринговых войнах» (когда операторы перестают обмениваться трафиком друг с другом напрямую, добиваясь более выгодного положения на рынке).

Нашими upstreams являются следующие компании:
Tier1:
- [Cogent](https://h.yandex-team.ru/?http%3A%2F%2Fwww.cogentco.com%2Fen%2F)
- [Telia International Carrier](https://h.yandex-team.ru/?https%3A%2F%2Fwww.teliacarrier.com)
- [Centurylink (aka Level3)](https://h.yandex-team.ru/?https%3A%2F%2Fwww.centurylink.com)
Подключение к Tier1 хоть и дает максимальные гарантии и, до некоторой степени, помогает меньше страдать от пиринговых войн, имеет минус. Минус в виде сети самого Tier1 по дороге к пользователям,
до которых нет прямой видимости, т.е. более длинный путь. Поэтому у нас есть еще один upstream в виде английской компании (такой же как Яндекс - голландский) [ReTN.NET](https://h.yandex-team.ru/?https%3A%2F%2Fretn.net), который используют в качестве upstream множество российских небольших операторов.

Все типы линков как раз агрегируются и подключаются к нашей сети на точках присутствия. К точкам присутствия могут подключаться несколько выносов (site), обычно это просто отдельный коммутатор, подключенный нерезервированным линком в граничный маршрутизатор. Поскольку в нем обычно оказываются отдельные прямые линки с операторами, резервировать эти соединения не обязательно. Каждый пиринговый линк с отдельным оператором и так получается уже многократно зарезервирован, поскольку сети этого оператора доступны и через линки с теми, кто предоставляет доступ в интернет ему и через операторов уровня Tier1, к нескольким из которых мы тоже подключены. Линки с особо крупными игроками, вроде большой тройки российский мобильных операторов, распределены географически по разным точкам присутствия.
Прямых линков с разными операторами Qrator Labs в аналитическом отчете как-то насчитал у нас 636 штук, самим нам как-то не приходило в голову их считать.

## Точки присутствия

Точка присутствия из себя представляет несколько стоек с оборудованием и серверами. Оборудование это, как обычно транспортное, для подключения к ядру сети, пограничные маршрутизаторы, коммутаторы для агрегации линков от провайдеров, серверные коммутаторы, такие же как ToR в датацентрах. Сервера это либо вспомогательные сервера для нужд сети – управляторы, концентраторы VPN (для удаленного доступа мелких офисов и сотрудников в сеть), телефонные шлюзы, либо сервера CDN и Видеотрансляций.

## CDN – content delivery network/ Video CDN

Упрощенно, эта конструкция предназначена для того, чтобы часть ресурсов сайтов отдавать с минимальной задержкой как можно ближе к пользователю. Состоит из серверов на удаленных площадках (или в сетях конкретных операторов связи), которые подключены туннелем через публичный интернет и на которые разными способами направляются пользователи. На серверах живет мини-платформа на скриптах для запуска контейнеров, которые, например, отдают статику некоторых проектов, раздают браузер пользователям, и тому подобное. Отвечает за это хозяйство в НОКе Traffic-Team.

Особняком стоит похожая на CDN конструкция для раздачи Видео, созданная в тесном сотрудничестве команды Андрея Година и НОК, обеспечивающая трансляцию видео, в том числе массовых мероприятий вроде ЧМ-2018. Это сервера, подключенные непосредственно к граничным маршрутизаторам на крупных точках присутствия в России и Финляндии. Нужны они в таком виде для решения двух проблем. Первая – из-за характерного для видео чудовищного мультипликатора «вход-выход» и огромных объемов (терабиты) внешнего трафика, такие объемы достаточно дорого и не рационально таскать по всей сети. Нужно мультиплицировать потоки как можно ближе к точкам выхода трафика наружу, в идеале – включать свитч с ними прямо в border, что и было проделано к олимпиаде и чемпионату мира по футболу на всех основных точках присутствия. Вторая проблема – опять же из-за объемов трафика его приходится раздавать по всем возможным путям, по которым пользователь виден из нашей сети. Это противоречит базовой идее терминировать трафик пользователя как можно ближе к пользователю и отдавать трафик оптимальным путем, на которую ориентируется остальная сеть. Видеотрафик управляется централизовано сложной логикой, которая учитывает текущее состояние линков наружу, доступность пользователя и состояние раздающих машин. При таком размещении весь этот механизм получается проще.

В итоге, раздающие сервера видеотрансляций размещаются на точках присутствия, примерно вот так:

![img_6.png](https://jing.yandex-team.ru/files/vdvolovik/img_41.png)

Это точка присутствия в ДЦ «Техногород» на Марьиной Роще в Москве. Cлева внизу как раз пограничный маршрутизатор Juniper MX960, над ним DWDM-транспорт Coriant Groove G30 (без шасси с усилителями, на такое расстояние они не нужны) и сервера НОК. Справа – стойка серверов видеотрансляций. Сервера видео сейчас обычно одноюнитовые машины с SSD, которые подключаются в коммутатор интерфейсами 2x25G.

## Пограничные маршрутизаторы (border routers)

Эти устройства как раз собирают на себя трафик внешних сетей, обмениваются с внешними и внутренними сетями маршрутной информацией, т.е. знанием о том, как добраться до конкретного адреса в интернете, применяют разнообразные политики к маршрутной информации и трафику и направляют потоки трафика от серверов во внешний мир. Здесь используются большие операторские устройства вроде Juniper MX2010 высотой 34U и с электропитанием 380V:

![img_7.png](https://jing.yandex-team.ru/files/vdvolovik/img_42.png)
![img_8.png](https://jing.yandex-team.ru/files/vdvolovik/img_43.png)
![img_9.png](https://jing.yandex-team.ru/files/vdvolovik/img_44.png)

Да, кабели так прокладывать обычно нельзя, но в эту стойку уже все равно не поставить другое оборудование из-за трехфазного питания этого монстра. На правом фото - DWDM на стороне ядра сети

Через пограничный маршрутизатор трафик пользователя попадает в нашу сеть и трафик сервисов попадает к пользователю. Путь пакета в нашей сети завершен.

## Заключение

Мы прошли по всему пути от сервера в датацентре до выхода в интернет. Надеюсь этот пост поможет сформировать у вас более правильное представление о нашей инфраструктуре.

Во второй части будет справочник, где описаны все ДЦ с точки зрения отказоустойчивости и характеристик сети, а также инфраструктуры ДЦ. В третьей части будет рассказано про сеть IPMI и офисную (она же в ДЦ технологическая) сеть.
