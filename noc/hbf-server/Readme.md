# Host-Based Firewall

Проект для компиляции legacy-правил файрвола Yandex (*FreeBSD ipfw with m4 templating, CVS repo*) в эффективную по времени lookup пакета древовидную структуру правил iptables для применения на конечных хостах. Правила компилируются и отдаются по http-запросу. Параметр запроса — это IP-адреса хоста, где надо применить файрвол.

## Быстрый старт
### Установка зависимостей и сборка
    pip3 install -r requirements.txt
    python3 setup.py build_ext
    CVSROOT=tree.yandex.ru:/opt/CVSROOT cvs co -d fw_data noc/routers/fw

### Запуск тестов
    python3 -m unittest

### Запуск локально
Запуск однопоточного сервера на aiohttp:

    ./hbf.py

Для вывода debug-лога в stdout и быстрого старта (без парсинга всех правил) можно запустить с ключами `-d`, `-s`:

    ./hbf.py -d -s NOCSRVNETS,DISKNETS

Запуск production-like окружения с gunicorn:

    gunicorn -c gunicorn-example.ini web_app:app

### Deploy в прод
    git clone git@noc-gitlab.yandex-team.ru:nocdev/ansible.git
    cd ansible

Редактируем`playbooks/deploy-hbf.yml`  (установить поле `revision:` на git reference). Затем:

    git commit playbooks/deploy-hbf.yml
    ./deploy-hbf.py -t code {sas,iva,myt,vla,man,prestable,validator} v0

По сути под капотом у этого ansible playbook обычный `git pull`.

## HTTP-ручки
### `/get/IP1,IP2?[&debug][&output][&training]`
Компилирует и возвращает правила iptables для заданных IP.
Параметры:

- `IP`: Список IP-адресов через запятую. Также можно передавать DNS hostname, макросы, сети.
- `debug` - добавляет комментарии к правилам через `iptables -m comment --comment ""`. Также в вывод добавляются версия сервера, немного статистики, info-сообщения.
- `output` - формировать правила не только на INPUT, но и на OUTPUT.
- `training` - принудительно включить режим учений, то есть выдать правила, блокирующие всё, кроме самого hbf

### `/macros/MACRO[&format={json,text}][&trypo_format={trypo,dottedquad,none}]`
Получить содержимое макроса
Параметры:

- `MACRO`: имя макроса, например, `_YANDEXNETS_`.
- `format`: json или new-line-separated текст
- `trypo_format`: используемый синтаксис для project-id сетей. project-id - это 3-й DWORD в IPv6-адресах в битах 64-95, big endian. Варианты:
  - `trypo`: `PID@CIDR`, например, `604@2a02:6b8:c00::/64`
  - `dottedquad`: `2a02:6b8:c00:0:0:604:0:0/ffff:ffff:ff00:0:ffff:ffff:0:0`
  - `none`: `2a02:6b8:c00::/40`. То есть, возвращается весь агрегат, для клиентов, которые не умеют новый синтаксис. Сейчас это Паспорт (Грантушка).

### `/stat/IP1,IP2?[&output]`
Найти и отобразить в plain-text исходные (ipfw) правила, подходящие под запрошенные IP. Рядом с каждым правилом написать его стоимость - сколько различных IP-префиксов (проверок src/dst) привозит с собой это правило. Правила будут отсортированы по убыванию стоимости.

Вызов используется для анализа размера выдачи. Позволяет понять, какие правила дают наибольший вклад в размер выдачи.

### Прочее
Прочие менее важные URI можно найти в `hbf/code.py` в `app_init()`

## Алгоритм работы
Есть две задачи:

- создание индекса правил (перед стартом и периодически)
- обработка запроса

### создание индекса
Суть процесса - пропарсить все правила в каталоге `fw_data` (cvs-репозиторий `noc/routers/fw`) и построить индекс правил по их src и по их dst. Также строится индекс файлов-секций по их scope (макросам).

Переиндексация происходит в `FW._rebuild_index` и запускается при старте программы или при получении новых правил из RT/CVS.

На выходе у этого процесса в Singleton-объекте `FW` должны оказаться заполненными поля:
- `_src_index`
- `_dst_index`
- `_section_index`

Все они - объекты класса `RadixTrypo`, который представляет собой префиксное дерево по IP-сетям с поддержкой project-id (т.е, не вполне префиксное).

Например, есть 3 правила:

    add allow ip from { _A_ } to { _A_ }
    add allow ip from { _B_ } to { _B_ }
    add allow tcp from { _A_ or _B_ } to { _C_ } 443

Пусть макросы заданы так:

- A: 10.0.1.0/24
- B: 10.0.2.0/24
- C: 10.0.0.0/8

Тогда при поиске в `FW._dst_index` по IP `10.0.1.99` мы найдем следующие правила:

- `add allow ip from { _A_ } to { _A_ }` (IP входит в `_A_`)
- `add allow tcp from { _A_ or _B_ } to { _C_ } 443` (IP входит в `_C_`)

### Обработка запроса
Центровая функция, обрабатывающая пользовательский запрос - это `build_iptables_ruleset`. Она делает следующее:

- ищет для каждого переданного target набор правил, применимых к нему, путём поиска в соответствующем дереве:
  - `FW._dst_index` для input-файрвола
  - `FW._src_index` для output

- группирует найденные правила по IP-префиксу, в котором они нашлись. В предыдущем примере первое правило нашлось по `10.0.1.0/24`, а второе - по `10.0.0.0/8`. Сгруппированные правила образуют набор объектов класса `Ruleset`. Ключом этого объекта является пара (`prefix`, `direction`), а значением - набор правил (`Rule`).
- для каждого `Ruleset` вызывается `.render()`, который через цепочку вызовов `FWPrefixTree() -> art.build -> FWPrefixTree.render_node()` делает готовые iptables-правила в виде dict, сгруппированные по цепочкам (chains).

  `art.build` - единственный более-менее сложный алгоритм в этом проекте. `art` расшифровывается как Adaptive Radix Tree, название где-то в инете увидел, не факт, что в оригинале за ним скрывался тот же смысл.

  Суть алгоритма - смешиваем вместе префиксы из всех правил `Ruleset` (src для input, dst для output), строим из них префиксное дерево, в узлах которого будут те самые оригинальные `Rule`. Это дерево и будет в конечном счёте представлено как набор iptables-chain и jump между ними. Но чтобы цепочек и jump не было слишком много, применяется свертка соседних нод в одну, пока число исходящих ребер (jump) в ноде не превысит `art.MAX_SIBLINGS`. Таким образом, hbf гарантирует, что стоимость lookup в одном дереве не превышает `MAX_SIBLINGS * max_depth(tree)`. Внутри одной ноды (цепочки) правила расположены последовательно, в линию.
- затем полученные dict из `Ruleset.render` для различных `Ruleset`, direction (in/out) и IPs попадают в один большой `jinja`-шаблон в `render_iptables_text()`, который и собирает финальный текст ответа.

### Кеширование и multi-worker
В prod-окружении прилетает достаточно высокая нагрузка, поэтому применяется несколько тех. решений для её обработки:

- Кеширование ответа на стороне клиента. Правила файрвола в CVS меняются нечасто (где-то раз в 10 минут), а с по-ДЦ окнами выкладки - и того реже. Поэтому вместе с каждым ответом клиенту отдаётся timestamp последнего коммита в CVS в заголвоке `Last-Modified`, и следующий запрос с теми же самыми IP-адресами в аргументах клиент делает уже с `If-Modified-Since`. Сервер отвечает `304 Not Modified`, если у клиента на руках уже есть ruleset, сгененированный из самой последней известной серверу ревизии CVS. Этим объясняется пикообразная форма графика 200-х ответов на сервисе hbf.yandex.net, обычно там 304.
- multi-worker дизайн. Используется `gunicorn`. В мастер-процессе работают два потока (os-thread): один с gunicorn (`gunicorn_app.py`), другой занимается периодическим обновлением правил и их индексацией (`hbf/cvs.py`). Если правила изменились, индексирующий тред посылает gunicorn-у (в свой собственный процесс) SIGHUP и тот gracefully запускает новые worker-ы через fork. Они наследуют все подготовленные индексы через CoW, никакой сериализации, shmem и прочего IPC больше не происходит. Каждый worker отправляется в независимое от других свободное правило, пока его не убьют на следующем цикле переиндексации.
- Кеширование результатов `Ruleset.render()`. Т.к. объекты `Ruleset` с одинаковым ключом должны быть одинаковыми, то и результат их render (тяжелая операция) можно закешировать. Этот кеш наполняется в каждом worker независимо, и не разделяется между ними. Дырки с/на большие сети имеют большой cache hit, т.к. применимы к запросам от многих клиентов.
- Кеширование различных immutable (по сути, но не формально immutable) через lru_cache во многих местах кода. Примеры сущностей, которые конструируются только при lru_cache miss: `ip_network`, `Rule`, `Hosts`.
- Некоторые классы написаны на Cython для увеличения производительности. Это примитивы ip address и ip network (`hbf/ipaddress2.pyx`), а также модуль `art`, строящий префиксные деревья с автоматической группировкой нод.
### Nginx
На серверах настроено два конфига /etc/nginx/sites-enabled/20-hbf.conf и /etc/nginx/sites-enabled/10-main.conf.
В 20-hbf.conf файле настроена локационная тема. В server_name указана что-то вроде man-hbf.yandex.net, а в 10-main.conf дефолт для всего остального (например main-hbf.yandex.net hbf.yandex.net man.hbf.yandex.net sas.hbf.yandex.net, ...). По идее в hbf-agent указан местный hbf-server, но может быть указано и просто hbf.yandex.net. Второй случай попадает в конфиг 10-main.conf, где по src-адресу проверяет откуда пришел запрос и делается редирект на локальный сервис. Используется [geo модуль nginx](https://nginx.org/ru/docs/http/ngx_http_geo_module.html).
```
map $geo $geo_to_upstream_ip {
    vla   "[2a02:6b8:0:3400:0:5c3:0:9]";
    man   "[2a02:6b8:0:3400:0:5c3:0:a]";
    sas   "[2a02:6b8:0:3400:0:5c3:0:b]";
    myt   "[2a02:6b8:0:3400:0:5c3:0:c]";
    iva   "[2a02:6b8:0:3400:0:5c3:0:d]";
}
...

    location / {
        if ($http_x_forwarded_for) {
            return 555 "loop detected. Host=$http_host";
        }
        proxy_pass http://$geo_to_upstream_ip:80;
        # переменная geo береться из конфига geo.conf
        proxy_set_header        Host            $geo-hbf.yandex.net;
        proxy_set_header        X-Real-IP       $remote_addr;
        proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
    }

```
Это делается для локализации трафика. Чтобы запросы из ман обрабатывались в ман и так далее. Маппинг между сетями и датацентрами прописан в /etc/nginx/conf.d/geo.conf.
```
# generated at 22-04-2022 18:28:24 by /usr/sbin/build_geoconf.py
geo $geo {
    default     sas;
    5.45.192.0/18       sas;
    5.45.194.0/23       vla;
    ...
```
Обновляется он запуском [update_geo.conf](https://a.yandex-team.ru/arc_vcs/admins/salt-media/noc/roots/units/hbf/files/usr/sbin/build_geoconf.py) через cron.
```
/etc/cron.d/update_geo.conf:3 0 * * *	root	p=$(md5sum /etc/nginx/conf.d/geo.conf); /usr/sbin/build_geoconf.py /etc/nginx/conf.d/geo.conf && ([ \"$p\" = \"$(md5sum /etc/nginx/conf.d/geo.conf)\" ] || systemctl reload nginx.service)
```
