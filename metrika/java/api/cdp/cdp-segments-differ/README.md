# CDP-SEGMENTS-DIFFER
Сервис, поддерживающий (обновляющий) список сегментов CDP для каждого идентификатора и отправляющий его в BigB.
Описание CDP можно прочитать тут - https://a.yandex-team.ru/arc_vcs/metrika/java/api/cdp/README.md.

## Описание
`cdp-segments-differ` - процессинговый сервис, поддерживающий (обновляющий) список сегментов CDP для каждого идентификатора (`UserID`) и отправляющий его в BigB.
Для понимания работы `cdp-segments-differ` важно понимать как работает очередь обработки сегментов. Её описание
можно прочитать [на WIKI тут](https://wiki.yandex-team.ru/users/lavrinovich/cdp-remarketing-in-pictures/). Также можно
ознакомится с реализацией этой очереди [в коде класса SegmentsProcessingQueueYdb](https://a.yandex-team.ru/arc_vcs/metrika/java/api/cdp/cdp-processing-common/src/java/ru/yandex/metrika/cdp/processing/ydb/SegmentsProcessingQueueYdb.java).

### Deployment
production - https://deploy.yandex-team.ru/stages/cdp-segments-differ-production

testing - https://deploy.yandex-team.ru/stages/cdp-segments-differ-test

### Что делает?
Сам по себе сервис - `BusyWorker`([интерфейс тут](https://a.yandex-team.ru/arc_vcs/metrika/java/api/cdp/cdp-processing-common/src/java/ru/yandex/metrika/cdp/processing/worker/BusyWorker.java)).
По сути в бесконечном цикле выполняет некоторую работу. Если быть более точным, то тут используется `ParallelBusyWorker` -
реализация интерфейса `BusyWorker`, позволяющая выполнять некоторый заранее определённый `Runnable` в бесконечном цикле
параллельно в несколько потоков ([код тут](https://a.yandex-team.ru/arc_vcs/metrika/java/api/cdp/cdp-processing-common/src/java/ru/yandex/metrika/cdp/processing/worker/ParallelBusyWorker.java)).
Назовём одно исполнение заданного `Runnable` "тактом" работы сервиса.

Один "такт" работы `cdp-segments-differ` выглядит следующим образом:
1. Достаём из очереди сегментов в YDB (`segments_queue/segments_queue`) очередной идентификатор сегмента (`segmnent_id`) для диффа.
У этого сегмента в очереди будет `stage = AWAITING_DIFF`. Детальная схема доставания элемента из очереди для диффа описана
[вот тут на WIKI](https://wiki.yandex-team.ru/users/lavrinovich/cdp-remarketing-in-pictures/# diffsegmenta)
2. Достаём из таблички сегментов в YDB (`segments_data/segments`) описание сегмента по его идентификатору.
3. Вычитываем из ClickHouse (из `mtcdp`) два snapshot-а сегмента старый (с `EvaluationVersion = diff_version`) и новый (с `EvaluationVersion = evaluation_version`).
4. Вычисляем добавившиеся и удалившиеся идентификаторы (`UserID`) для данного сегмента.
5. Читаем из таблицы в YDB (`segments_data/user_segments`) текущий набор сегментов для каждого изменившегося идентификатора (`UserID`).
6. Обновляем список сегментов для каждого идентификатора (`UserID`). Так как за один такт обрабатывается один сегмент, то
всё обновление заключается лишь в добавлении или удалении конкретного `segment_id` для каждого идентификатора (`UserID`).
7. Для каждого идентификатора (`UserID`) делаем "уборку мусора". Механизм описан ниже.
8. Инрементируем версию для каждого идентификатора (поле `version` в таблице `segments_data/user_segments`).
9. Сохраняем обновлённое состояние в таблицу в YDB (`segments_data/user_segments`).
10. Отправляем полное состояния набора сегментов для каждого изменившегося идентификатора (`UserID`) в BigB

#### "Уборка мусора"
При работе всей этой схемы могут возникать 2 проблемы, которые важно каким-то образом решать:
1. Сегменты могут протухать. Например могут просто отключить ретаргетинг на данный сегмент.
2. Сегментов может быть слишком много для конкретного идентификатора (`UserID`).

Уборка мусора призвана решить эти две проблемы. После обновления набора сегментов для некоторого `UserID` выполняются 2 действия:
1. Удаляются протухшие сегменты. Для этого в стейте по `UserID` хранится не просто набор `segment_id`, а набор пар
`(segment_id, evaluation_version)`. Также в очереди сегментов у каждого сегмента есть поле `minimal_valid_version`.
Если у какого-то сегмента в стейте `evaluation_version < minimal_valid_version`, то такой сегмент из стейта удаляется,
так как он "протух".
2. При достижении некоторого заранее определённого порога количества сегментов соответствующих одному `UserID` удаляются
"самые старые" сегменты из стейта. Набор сегментов в стейте упорядочен по времени попадания в стейт. В этом смысле этот
набор сегемнтов для каждого `UserID` можно считать некоторой очередью с фиксированным максимальным размером, при достижении
которого "самые старые" сегменты из очереди удаляются. Сейчас этот "максимальный размер очереди" равен 600. Подобный механизм
существует и в самом BigB, в который эти сегменты отправляются  и кажется там лимит количества сегментов равен 500. На нашей
стороне этот лимит намерено чуть выше, чтобы при "протухании" сегментов или про выпадании куки из конкретного сегмента
был запас.

#### Поточность
Так как размеры сегментов потенциально могут быть очень большие, то критически важно не вычитывать snapshot-ы сегментов целиком.
Для этого с одной стороны используется упорядоченное по `UserID` чтение из ClickHouse, а с другой стороны специальный `DiffIterator`,
который позволяет вычислять diff двух сортированных итераторов. Код этого итератора [можно найти тут](https://a.yandex-team.ru/arc_vcs/metrika/java/api/common/src/java/ru/yandex/metrika/util/collections/DiffIterator.java).
Такой подход позволяет свести потребление памяти к O(1), вне зависимости от размера snapshot-ов. Как следствие пункты 4-10 из
описания одного "такта" работы выполняются не для всего сегмента, а для кусочков diff-а фиксированного размера. Помимо прочего,
это позволяет сделать обработку такого кусочка транзакционной с точки зрения YDB.

#### Транзакционность
Так как каждый идентификатор (`UserID`) может попадать в произвольный набор сегментов, то при обработке это может приводить
к классическому data race, когда два обработчика параллельно обрабатывают два разных сегмента и пытаются обновить один и
тот же `UserID`. Чтобы избежать проблем с консистентностью данных здесь используются транзакции на уровне YDB. Таким
образом на уровне данных в YDB пункты 5-9 из описания одного "такта" работы выполняются атомарно для некоторой пачки
идентификаторов (`UserID`).

### Куда пишет данные?
Актуальное состояние набора сегментов для каждого идентификатора (`UserID`) хранится в таблице в YDB (`segments_data/user_segments`).
Обновлённые наборы сегментов для каждого идентификатора (`UserID`) пишутся в BigB c помощью топика в Logbroker (`cdp-segments-log`).

## Графики
Графики для production:
1. [Мониторинг топика `cdp-segments-log` в Logbroker](https://logbroker.yandex-team.ru/lbkx/accounts/metrika/prod/cdp/cdp-segments-log)

Для самой очереди сегментов в YDB графиков пока нет, будут добавлены после [CDP-582](https://st.yandex-team.ru/CDP-582).

