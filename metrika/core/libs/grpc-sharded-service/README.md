# Шаблон для шардированых реплицированных cache-сервисов

Клиент предоставляет единственный метод Get([Key]) -> [(ID, Value)].
Используется согласованность в конечном счете (eventually).


## Термины

Key (TKey) --- ключи, которые отправляются сервером клиенту.
При этом ключ не обязан быть простой структурой, 
на практике он может быть довольно большим и сложным, например,
у goals-server ключ несет в себе флаги хита и урлы.
Такая терминология выбрана для единообразия и сходства c key-value хранилицами.

ID (TEntryID) --- некоторое простое значение (получаемое из Key), которое уникально для ключа в рамках запроса.
Это может быть просто порядковый номер ключа в запросе, а может быть и что-то чуть более сложное.
Например, в goals-server ID = WatchID хита.

Value (TValue) --- возвращаемое значение. Для goals-server Value = списки целей.

ShardingKey (TShardingKey=ui64) --- значение (получаемое из Key), по которому определяется шард.
Для goals-server ShardingKey=CounterID, поскольку требуется последовательное сканирование в mysql,
но в других сервисах для равномерного распределения можно использовать и какой-нибудь хеш.


## Взаимодействие сервера и клиента

### Сервер

Сервер просто отвечает на grpc запросы.
Единственный доступный запрос Get(Request=[Key]) -> Response=([(ID, Value)], Shard, AllMatched).

Сервер получает набор ключей.
Для ключей выполняется запрос и возвращается для каждого ключа (GetID(Key), Value).
В ответе всегда возвращается шард (полуинтервал), который пригодится позже на клиенте.
Для каждого ключа сервер также проверяет, что ключ (GetShardingKey(Key)) лежит в его шарде.
Если ключ в шарде не лежит, то ничего не возвращается, кроме описания шарда и AllMatched=false.
Если все ключи в шарде лежат, то AllMatched=true.
Это требуется реализовывать самому.

! Get от пустого множества ключей должен обрабатываться корректно.

Сервер распологается в нескольких дц.
Каждый шард существует в единственном экземпляре в каждом дц.

### Клиент

Клиент предоставляет единственный метод Get([Key]) -> [(ID, Value)].

Клиент поддерживает внутри себя сведения о местоположении шардов.
Они обновляются при получении ответа за любой запрос Get к серверу (если запрос более "новый").
Раз в PingPeriod клиент также обновляет набор известных ему хостов (yp или конфиг) и рассылает им всем пустые запросы Get,
чтобы получить в ответе шарды.

При выполнеии запроса Get([Key]) -> [(ID, Value)] клиент просто отправит на каждый известный ему шард запрос c ключами, которые принадлежат данному шарду (т. е. отправляются запросы на все активные реплики всех шардов).
Затем клиент принимает ответы от хостов, помечая уже полученные ключи.
Как только все нужные ключи получены клиент возвращает ответ (устанавливает promise).
Из дальнейших ответов клиент будет вытаскивать только шард для обновления.

Запросы к каждому хосту имеют таймаут и ретраятся отдельно.
Весь набор запросов целиком не ретраится.

Если вдруг оказалось, что есть ключи, для которых клиент не может найти шард, то клиент ждет PingTime и повторяет поиск.
Если же шарды выбрать удалось, но по каким-то причинам после получения ответа/таймаута всех запросов остались ключи, для которых ответ не найден, то клиент возвращает ошибку.


Для клиента требуется определить структуру описания запроса вида 
```
struct TGrpcGoalsRequestDescription {
    // описаны выше
    using TKey = THitData;
    using TValue = TGoalMatched;
    using TEntryID = TWatchID;

    // классы из .proto
    using TGRpcService = TGoalsMatcherService;
    using TRequest = THitsBatch;
    using TResponse = TGoalReachesShardedBatch;

    // название запроса
    using TAsyncRequest = typename NGrpc::TSimpleRequestProcessor<typename TGRpcService::Stub, TRequest, TResponse>::TAsyncRequest;
    TAsyncRequest AsyncRequest = &TGoalsMatcherService::Stub::AsyncMatchSharded;

    // Ключ шардирования
    NShardedService::TShardingKey GetShardingKey(const TKey& key);

    // Получение ID
    TEntryID GetEntryID(const TKey& key);

    void Serialize(const TVector<TKey>& keys, TRequest& request);

    // Возвращает true, если поматчились все ключи.
    // false может вернуться, например, при отправке запроса на неправильный шард или при отсутствии шарда в данной локации.
    // Ретраить false не нужно.
    bool Deserialize(const TResponse& response, THashMap<TEntryID, TValue>& values, NShardedService::TShard& shard);
};

using TGoalsClient = NShardedService::TShardedClient<TGrpcGoalsRequestDescription>;
```

### Зачем?

Такая схема:

- Легко масштабировать ram и cpu (просто больше шардов делаем).
- Устойчива к перешардированию (рестарт серверов происходит по дц).
- Устойчива к падениям/закрытию дц/тормозам хостов, так как ответ просто будет получен с другого хоста.

## Выбор шарда для хоста (сервер)

Состояние поддерживается в таблице
```
(dc, shard, host, locked_time, last_ping)
```
Есть ограничение на уникальность пары (dc, shard).

Сервер пытается захватить шард, который долго не пинговался, при этом отдавая предпочтение  шарду, который был на этом сервере ранее.
```
SELECT shard FROM {table} WHERE dc = '{dc}' AND (host = '{host}' OR last_ping < {timeout}) ORDER BY CAST(host = '{host}' AS SIGNED INTEGER) DESC, last_ping ASC, shard ASC LIMIT 1 FOR UPDATE SKIP LOCKED
```

Затем сервер постоянно проверяет (раз в PingPeriod), что он не потерял лок и обновляет last_ping в now().
Если лок оказался по какой-то причине потерян, то abort().
Здесь несколько напрягает использование реального времени в mysql, однако к проблемам это привести не должно.
