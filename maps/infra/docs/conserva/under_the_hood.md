# Описание работы системы

## Терминология
- blob - это одна строка, содержащая часть файла не более 4 МБ. Каждый файл разбивается на blob'ы. Подробнее в докумментации yt по  [blob-table](https://yt.yandex-team.ru/docs/description/storage/blobtables)
- yt-chunk, chunk, чанк - является внутренним представлением таблиц в YT. [Документация](https://yt.yandex-team.ru/docs/description/storage/chunks).

- [job](https://yt.yandex-team.ru/docs/description/mr/jobs) - YT job'а.
- операция - YT операция - совокупность job'ов. Мы будем использовать [reduce операции](https://yt.yandex-team.ru/docs/description/mr/reduce).
- одна итерация - это один вызов reduce, во время которого мы пишем пачку файлов определенного размера из S3 в YT

## Структуры таблиц

1. Таблица [s3_meta](https://yt.yandex-team.ru/hahn/navigation?path=//home/maps/core/conserva/maps-sat-source/s3_meta) хранит в себе список файлов из S3. Точно такой же список есть и в S3, но доступ к нему очень медленный, поэтому дублируем в YT.

    * `filename`: str - имя файла
    * `data_size`: int - размер файла в байтах

2. Таблица [jobs_table](https://yt.yandex-team.ru/hahn/navigation?path=//home/maps/core/conserva/maps-sat-source/jobs_table) - промежуточная таблица, которая хранит в себе задачи по выкачиванию файлов из S3 в YT.

    * `table_id`: int(sorting key) - НЕ уникальный ключ, номер таблицы в которую должна сохраниться очередная порция данных.
    * `iteration`: int - итерация на которой нужно выполнить обработку
    * `files`: list[str] - список файлов, который нужно перекачать в таблицу `data_{table_id}` на `iteration` итерации

3. Набор табличек `data_{table_id}`, в которых хранятся файлы. [Пример](https://yt.yandex-team.ru/hahn/navigation?path=//home/maps/core/conserva/maps-sat-source/data_0).
Эта таблица является [blob-table](https://yt.yandex-team.ru/docs/description/storage/blobtables), но с модификацией - в качестве ключа сортировки используется не filename, а искусственно введенный file-index. Сортировка необходима, чтобы мы могли использовать reduce операции, которые требуют отсортированности. А использовать filename не можем, потому что файлы нам подаются на вход неупорядоченно.

    * `file_index`: int (sorting key) - монотонно возрастающий, численный ключ файла, генерить его будем сами. Гарантируется уникальность только в рамках одной таблицы и могут пересекаться в разных таблицах.
    * `blob_index`: int (sorting key) - индекс blob'a
    * `filename`: str - имя файла
    * `data`: bytes <= 4MB

## Создание бэкапа
Общая идея:
<br>Последовательно, M раз вызываем метод yt.reduce, каждый вызов запускает N джобов=кол-ву выходных таблиц. Каждый джоб обрабатывает небольшую порцию данных(гигабайты).
<br><br>Например, чтобы создать бэкап [`maps-sat-source`](https://yt.yandex-team.ru/hahn/navigation?path=//home/maps/core/conserva/maps-sat-source), весом 25 ТБ, я создал N=50 таблиц с данными, каждый job перегоняет по 20 ГБ данных. Таким образом, на каждой итерации мы перекачиваем 20*50=1000 гб данных. Соответственно, нам потребуется M = 25000 / 1000 = 25 итераций, чтобы сделать полный бэкап.
<br>Увеличение числа таблиц(N) приводит к увеличению числа одновременных job'ов, уменьшению количества итераций(M), и соответственно, уменьшает время создания бэкапа. N ограничено сверху скоростью s3 бакета, которую следует уточнять у команды MDS. [Для текущих сервисов уже посчитали.](https://st.yandex-team.ru/MDSSUPPORT-1095#602c1671a8a6104eae54dee4)

Процесс создания бэкапа состоит из трех стадий, каждой из которых соответствует табличка в предыдущем разделе
1. Перекачиваем список файлов из s3 в YT.
2. Нарезаем список файлов из таблицы `s3_meta` на задачи и сохраняем в таблицу `jobs_table`.
   <br>**Не** уникальным ключом в этой таблице является `table_id` - номер таблицы, в которую на третьем этапе нужно сохранить файлы. На каждой итерации, для каждой `table_id` будет запускаться отдельный job, таким образом в один job прилетает множество строк из `jobs_table`, у которых одинаковый `table_id`, но разный `iteration`. Обрабатывать мы будем только ту строку, в которой `iteration` соответствует текущей итерации.
   <br>При этом по возможности список файлов `files` держим в пределах 20 гигабайт.
3. M раз запускаем reduce(by table_id) задачи, каждая задача перекачивает свой список файлов в свою отдельную таблицу `data_{table_id}`.


###  Почему размер порции равен 20 ГБ?
Подобранное число, обусловленное 2 ограничениями:
- С верхней стороны не хочется делать его слишком большим, т.к. если YT решит прервать нашу задачу, то весь прогресс будет потерян
- С нижней, не хочется чтобы накладные расходы на запуск джобов вносили значимый вклад в общее время работы системы.
Для 20 гигабайт время исполнения одного job'a занимает 8-20 минут.


### Зачем нам N таблиц?
Из структур таблиц видно, что данные у нас отсортированы по file_index. Это необходимо для того, чтобы на этапе восстановления мы могли запустить reduce операцию, которая требует отсортированности ключа.
Если на этапе создания бэкапа мы будем писать из job'ов в одну таблицу, то порядок вставки будет неопределен, а значит мы не сможем сформировать отсортированный ключ.
Таким образом, каждому джобу для создания бэкапа необходимо завести отдельную таблицу.

## Восстановление файлов из бэкапа
На первом этапе сформируем таблицу s3_meta, чтобы иметь на руках этот список и повторно не сохранять в S3 файлы, которые там уже есть. Это нужно при частичной потере данных и когда бэкап нужно восстановить не полностью. Либо, если восстановление бэкапа упала по середине и нужно довосстановить его.

Восстановление данных будем производить независимо для каждой таблицы `data_{table_id}`, делая reduce(by file_index). Запустится много(сотни) параллельных джобов, каждой из которых на вход будет подаваться набор файлов, которые мы и будем перекачивать в S3.
<br>При этом для достижения наилучшей скорости, будем запускать параллельно несколько yt.reduce операций.

## Ограничения скорости
S3 имеет ограниченную пропускную способность, что растянет бэкап во времени. Чтобы контролировать скорость, у нас есть несколько механизмов:
- Механизм от команды MDS, который позволяет ограничить скорость для конкретного bucket'a
- Загрузка файлов cpu-bound и в среднем одно ядро(одна job'a) способно переварить 20 МБ/сек. Но данное число ни кем не гарантируется, т.к. выведено эмпирически.
