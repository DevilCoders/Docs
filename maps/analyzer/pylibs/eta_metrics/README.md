# Подсчёт eta-метрик

Библиотека для подсчёта и агрегации eta-метрик на ClickHouse over YT.

1. [Описание работы](#описание-работы)
    1. [Формат входных данных](#формат-входных-данных)
1. [Быстрый старт](#быстрый-старт)
1. [Модули и функции](#модули-и-функции)
    1. [Расширение статсов](#расширение-статсов)
    1. [Расчёт метрик](#расчёт-метрик)
    1. [Агрегация по регионам](#агрегация-по-регионам)
    1. [Список модулей](#список-модулей)
1. [Метрики](#метрики)
    1. [Пользовательские метрики](#пользовательские-метрики)
1. [Группировка](#группировка)
1. [Примеры](#примеры)
1. [TODO](#todo)

## Описание работы

Библиотека предназначена для расчёта метрик, которые затем пригодны для быстрой доагрегации на CHYT (в т.ч. в чартах). Также приводит статсы (см. [глоссарий](https://a.yandex-team.ru/arc/trunk/arcadia/maps/analyzer/docs/glossary.md)) к единому виду, добавляя к ним колонки с ошибками, колонки для группировки (как то: приложение, платформа, группа длин и т.п.).

### Формат входных данных

Метрики считаются на таблицах, у которых должны быть колонки с такими данными:
- эталонное время (`etalon_travel_time`)
- эталонная протяжённость поездки (`etalon_travel_length`)
- прогнозное время (`predicted_travel_time`)
- прогнозная протяжённость поездки (`predicted_travel_length`)
- время, когда была совершена поездка (`etalon_at`)
- время, когда был сделан прогноз (`predicted_at`)

При этом колонки могут называться как угодно, для указания конкретных имён в библиотеке предусмотрен тип `ColumnNames` в модуле [`columns`](columns.py), там же есть примеры. В результате они будут приведены к именам по умолчанию (выше указаны в скобках). При этом возможно не только простое переименование, но и перерасчёт (домножить / поделить и т.п.), см. например `FIXPRICE` в модуле [`columns`](columns.py).

Также _можно_ указать одну и ту же колонку для эталонного и прогнозного расстояния, если прогнозного расстояния у вас нет, но в этом случае, понятно, относительная ошибка расстояния будет бессмысленна (хотя и посчитается).

## Быстрый старт

Для простого расчёта метрик подойдёт функция `calculate_metrics`, которая вбирает в себя все возможности:
- добавляет в статсы дополнительные колонки
- агрегирует метрики по указанным ключевым колонкам
- при необходимости агрегирует и по родительским регионам

Подробное описание функции см. в документации к функции в модуле [`metrics`](metrics.py).
См. также [пример](example/main.py).
```python
rich_stats, metrics = calculate_metrics(
    ytc,
    stats,  # таблица с прогнозами и эталонными временами/длинами
    # group by - список колонок для группировки, также может включать вычислимые поля, см. соответствующий раздел
    group_by=['app', 'platform', 'region_id', eta.Exprs.LOCAL_HOUR.value],
    min_count=100,  # минимальный размер группы в метриках
    # column_names=ColumnNames(etalon_travel_time='my_etalon_column', ...)
)
```

Если нужно разделить этапы "обогащения" статсов и непосредственно расчёта метрик, то код выше по сути сводится к такому:
```python
rich_stats = enrich_stats(ytc, stats)
[rounded_stats] = round_regions(ytc, [rich_stats])  # по умолчанию до уровня города
metrics = evaluate_metrics(
    ytc, rounded_stats,
    group_by=['app', 'platform', 'region_id', eta.Exprs.LOCAL_HOUR.value],
    min_count=100,
)
```

## Модули и функции

### Расширение статсов

**ВАЖНО**: CHYT не умеет сохранять тип YSON, так что на входе должны использоваться сложные type_v3 типы.

* `enrich_stats` — обогатить таблицы с колонками прогнозных и эталонных длин/времён колонками с ошибками, а также колонками для группировок: платформа/приложение/группа длин и пр.; по сути вызывает последовательно `calculate_errors` и `extend_stats`
* `calculate_errors` — добавить к таблице с колонками прогнозных и эталонных длин/времён колонки с ошибками (средняя, логарифмическая, знаковая и т.п.). Также, что важно для функции `extend_stats`, переименовывает колонки, приводя их к принятому в библиотеке виду
* `extend_stats` — приводит колонки с временем прогноза и эталона к YT-типу `Timestamp` и дополняет таблицу с ошибками колонками для группировок:
    * `on_route` — трек на маршруте? Считается на основе `route_coverage`/`route_confidence`. Если исходная колонка опциональная, то результат также опционален
    * `app`/`platform` — приложение/платформа, на основе `clid`. Неизвестные будут иметь значение `unknown`
    * `length_group` — группа длин на основе `etalon_travel_length`
    * `timezone` — имя таймзоны на основе `region_id`. Опционально, так как не все регионы имеют таймзону (например, регион Земля — на нижнем уровне редко, но встречается)
    * `local_etalon_at` — локальное время старта эталона на основе `region_id`, т.е. смещено относительно `etalon_at` на основе таймзоны

    **NOTE**: Если каких-то колонок в исходной таблице нет, то на выходе не будет зависимых колонок. Имена колонок, используемых в функции, не настраиваются, потому вызывать её надо _после_ `calculate_errors` (либо переименовать самому).
### Расчёт метрик
* `calculate_metrics` — всё-в-одном функция для расчёта метрик; умеет агрегировать по регионам, возвращает статсы, дополненные полями для группировки и ошибками, а также сами метрики; по сути сводится к:
  - обогащению статсов (`enrich_stats`)
  - округлению регионов, если требуется (`round_regions`)
  - расчёту самих метрик (см. ниже - `evaluate_metrics`)
* `evaluate_metrics` — аналогична функции выше, но на вход принимает уже обогащённые статсы (результат функции `enrich_stats`), а далее агрегирует, в т.ч. по регионам<br>
    **NOTE**: чаще всего имеет смысл предварительно округлить регионы при помощи `round_regions` до уровня города, например
* `aggregate_stats` — сагрегировать ошибки по указанным колонкам и посчитать средние ошибки, а также квантили (сохраняются в виде состояния с возможностью последующей доагрегации)
* `aggregate_metrics` — доагрегировать метрики по более общим колонкам, при этом средние ошибки пересчитываются с учётом веса (кол-ва), а состояния квантилей объединяются, т.о. сохраняется возможность получить квантили от более крупных групп.

### Агрегация по регионам
Также есть функции для работы с регионами:
* `extend_regions` — дублировать статсы/метрики для родительских регионов. Это позволяет реализовать агрегацию метрик по родительским регионам: сначала продублировать метрики записями для родительских регионов, а затем выполнить `aggregate_metrics` по тому же набору ключей.
* `round_regions` — округлить регионы вверх до определённого уровня; удобно, если хочется ограничить подробность метрик уровнем города, например
* `set_regions_names` — добавить колонки `region_name` и `region_en_name` с названиями регионов; это имеет смысл делать после всех агрегаций. Также возможно сохранить имеющиеся имена, если они уже есть в статсах/метриках, для этого можно передать [пользовательские метрики](#пользовательские-метрики) `REGION_NAME` & `REGION_EN_NAME`, но ответственность за то, что группировка идёт по `region_id`, лежит на вызывающей стороне.

### Список модулей
* [`columns`](columns.py) — имена колонок в статсах для переименования, используется в `calculate_errors`
* [`custom`](custom.py) — пользовательские метрики, описывают выражение для вычисления ошибки в `calculate_errors` и выражения для последующих агрегаций; пример использования (`JAMS_COVERAGE`) можно найти в модуле [`presets`](presets.py)
* [`group_by`](group_by.py) — вычислимые колонки для группировки
* [`metrics`](metrics.py) — расчёт и агрегация метрик
* [`regions`](regions.py) — округление регионов и добавление родительских
* [`presets`](presets.py) — пресеты для уже используемых метрик (онлайн/оффлайн/фикспрайс)
* [`schema`](schema.py) — описания некоторых колонок ошибок и группировки
* [`stats`](stats.py) — расширение статсов колонками для группировки

## Метрики

Считаются такие метрики:
* `etalon_travel_time` — общее эталонное время (с)
* `etalon_travel_length` — общая эталонная протяжённость (м)
* `predicted_travel_time` — общее прогнозное время (с)
* `predicted_travel_length` — общее прогнозное расстояние (с)
* `predicted_speed` — средний прогноз скорости (м/с)
* `etalon_speed` — средняя фактическая скорость (м/с)
* `logarithm_time_error` — средняя логарифмическая ошибка прогноза времени: `log(predicted/etalon)`
* `relative_time_error` — средняя относительная ошибка прогноза времени: `max(predicted, etalon)/min(predicted, etalon)`; при агрегации ограничивается сверху (по умолчанию 10.0)
* `relative_distance_error` — средняя относительная ошибка прогноза расстояния, при агрегации ограничивается сверху
* `signed_time_error` — средняя знаковая ошибка времени (с)
* `absolute_time_error` — средняя абсолютная ошибка времени: `abs(predicted - etalon)` (с)
* `relative_speed_error` — средняя относительная ошибка скорости; при агрегации ограничивается сверху

Помимо этого сохраняется состояние для расчёта квантилей относительной ошибки времени в колонку `relative_time_error_q`. В терминах ClickhHouse колонка имеет тип `AggregateFunction(quantileTDigest, Double)`, однако на YT она хранится просто как строка, и необходимо колонку скастить к нужному типу.

Получить квантиль 0.1
```sql
select finalizeAggregation(cast(relative_time_error_q, 'AggregateFunction(quantileTDigest(0.1), Double)')) as rel_01 from `//table`;
```

Доагрегировать по группе и получить квантиль 0.1
```sql
select quantileTDigestMerge(0.1)(cast(relative_time_error_q, 'AggregateFunction(quantileTDigest, Double)')) as rel_01
from `//table` group by clid;
```

### Пользовательские метрики

Есть возможность описать пользовательские метрики, которые затем будут вычисляться в ошибках и агрегациях — `Metric`. Исходная ошибка вычисляется при помощи вычислимой колонки, агрегация проводится при помощи указанной агрегации (см. класс `Aggregation`, там есть `avg`, `avgBy`, `any`)

В `presets.CustomMetrics` определены несколько:

```python
JAMS_COVERAGE = Metric(
    expr=ColumnExpr.make_from('jams_covered / etalon_travel_length as jams_coverage'),
    aggr=Aggregation.avgBy('etalon_travel_length'),
)
REGION_NAME = Metric(expr=ColumnExpr.make_from('region_name'), aggr=Aggregation.any())
```

* `JAMS_COVERAGE` — покрытие пробками; также входит в пресет оффлайн-качества
* `REGION_NAME` & `REGION_EN_NAME` — имена регионов; по сути просто выбирает любое имя в группе, т.о. результат будет иметь смысл только при агрегации по `region_id`.

Также можно использовать `custom.preserve(name)`, чтобы сохранить какую-либо произвольную колонку из статсов при агрегациях. При этом из группы будет выбрано произвольное значение. По сути это тоже кастомная метрика с агрегацией `any`.

## Группировка

Помимо группировки по колонкам можно проводить группировку по вычислимым полям. В коде они представлены типом `ColumnExpr` и обозначают выражение вида `<expr> as <name>`. Например, есть такое вычислимое поле для дня недели:

```python
LOCAL_WEEKDAY = ColumnExpr(
    expr="toDayOfWeek(toDate(local_etalon_at / 1e6, 'UTC'))", name='local_weekday',
)
```

Его можно передавать в составе `group_by` в функции `calculate_metrics`/`evaluate_metrics`/`aggregate_stats`. В функцию `aggregate_metrics` передавать тоже можно, но она игнорирует выражение, так как ожидается, что колонка уже добавлена в метрики на шаге расчёта метрик. Это позволяет вызвать последовательно `aggregate_stats`, а затем `aggregate_metrics` с одним и тем же набором `group_by`.

В модуле [`group_by`](group_by.py) определены такие вычислимые поля:
* `LOCAL_DAY`, `LOCAL_WEEKDAY`, `LOCAL_HOUR` — локальные дата, день недели и час
* `DAY`, `WEEKDAY`, `HOUR` — дата, день недели, час в UTC

и такие наборы для группировок:
* `COMMON` — общий: приложение, платформа, группа длин и регион
* `ONLINE` — для `online-eta`: `on_route` и `vehicle_type`
* `FIXPRICE` — как `ONLINE`, но без типа ТС, так как там только такси
* `OFFLINE_ROUTES`, `OFFLINE_TRACKS` — с добавлением типа прогноза для `offline-eta` на основе маршрутов и треков соот-но

## Примеры

См. также [пример](example).

```python
import maps.analyzer.pylibs.envkit as envkit
import maps.analyzer.pylibs.eta_metrics as eta


# Grouping columns: online eta and local hour
GROUP_BY = eta.Groups.COMMON.value + eta.Groups.ONLINE.value + [
    eta.Exprs.LOCAL_HOUR.value
]


def calc_metrics_quickly(table: str):
    with envkit.get_context() as ytc:
        # calculate metrics and enriched stats, aggregated by region
        stats, metrics = eta.calculate_metrics(
            ytc, stats,
            group_by=GROUP_BY,
            min_count=1000,
            column_names=eta.TRACKS,
        )

def calc_metrics_manually(table: str):
    with envkit.get_context() as ytc:
        # Enrich `table` with various errors (relative, log, ...)
        errs = eta.calculate_errors(ytc, table)
        # Extend errors with columns to group by
        [errs] = eta.extend_stats(ytc, [errs])

        # Calculate metrics, grouping by `group_by` columns
        # It will also save quantile states to allow extra aggregation
        metrics = eta.aggregate_stats(ytc, errs, group_by=GROUP_BY)
        # Add parent regions to metrics and reaggregate to get metrics by all regions
        [metrics] = eta.extend_regions(ytc, [metrics])
        metrics = eta.aggregate_metrics(ytc, metrics, group_by=GROUP_BY)

        # Reaggregate metrics by more common columns, it will merge groups taking care of quantiles too
        by_region = eta.aggregate_metrics(ytc, metrics, group_by=['region_id'])
```


## TODO

Функция `extend_stats` принимает список колонок, которые обязательны на выходе, и валидирует, что они на выходе будут. По идее туда можно передавать и колонку, которую сам `extend_stats` не добавляет. Было бы удобно туда передавать сразу `group_by`, но тут есть подвох с вычислимыми полями, так как колонки `local_hour` например в статсах не будет, и на деле тут нужно требовать наличия `local_etalon_at`. Если добавить в вычислимые поля инфу об исходных колонках, то можно будет сделать так, что в `extend_stats` можно будет передать просто `group_by`, а он убедится, что в статсах есть всё необходимое, но это выглядит как переусложнение.
