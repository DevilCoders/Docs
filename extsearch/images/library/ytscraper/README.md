# ytscraper

Библиотека - клиент скрепера, скачивающий картиночный серп с различными параметрами. План скачки забирается из таблице на YT (запрос забирается из колонки query, домен из колонки tld, регион берётся из колонки region, а при его отсутствии вычисляется по значению tld).

На текущий момент есть 2 реализации:

1. SerpsDownloader - работает через локальную выкачку данных со скрепера. Результат затем сохраняется в выходные таблицы поблочно. На каждую такую выходную таблицу зовется в режиме fire and forget дефрагментация.
2. SerpsDownloaderYT - данные обрабатываются прямо на YT. Клиент берет данные прямо из служебных таблиц scraper на hahn (или banach, в качестве fallback location). Если кластер пользователя отличается от кластера scraper, то временная таблица с тикетом копируется на конечный кластер с помощью transfer_manager и там обрабатывается.


Кастомизация логики построена на конфигурировании и колбеках.

Пользовательские коллбеки:

```

def on_get_query_data(self, plan_row) # на вход приходит строка таблицы с планом закачки. Необходимо вернуть (query, tld, region). По умолчанию ищет в столбцах таблицы "query", "tld", "region"

def on_get_queries_batches(self) # здесь подготавливаются батчи (имеет смысл переопределять только если формат on_get_query_data не подходит)

def on_create_request_data(self, batch) # здесь батч при необходимости можно изменить перед запросом в скрапер (имеет смысл переопределять только если переопределен on_get_queries_batches)

def on_batch_failed(self, ticket, status)  # вызывается, если тикет скрепера закрылся с ошибкой. По умолчанию бросает исключение и обработка тикета завершается

def on_parse_serp(self, serp) # здесь парсится скаченный серп

def on_parse_exception(self, serp, exception) # вызывается, если произошло исключение при парсинге серпа. Реализация по умолчанию просто пропускает такой серп.

def on_after_batch_written(self, serps_table, ticket) # вызывается после скачивания очередного батча

def on_write_table(self, parsed_serps) # генератор результатов в выходную таблицу, на вход получает список распарсенных серпов в том формате, который возвращает on_parse_serp()

def on_complete_download(self) # зовется по окончании всего плана
```

Для истории: как фаршируется выдача скрепера можно посмотреть вот [здесь](https://git.qe-infra.yandex-team.ru/projects/SEARCH_INFRA/repos/serp-scraper/browse/server/parsers/src/main/java/ru/yandex/qe/scraper/parsers/concrete/yandex/json/images/YandexJsonImageSearchParser.java)
