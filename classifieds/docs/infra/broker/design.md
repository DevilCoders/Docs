# Брокер брокеров

Система для доставки событий до стораджей (yt, hive, ch) и других систем (kafka, callbacks?).

Основная цель - более простая альтернатива ручной организации поставки в yt/hive.
В идеале должна позволять сделать себе новую поставку без тикетов и ожидания команды, ответственной за сервис.

То есть клиент
1. Описывает схему сообщения в протобаф [(рекомендации к схеме)](schema_guide.md)
2. ???
3. Получает таблички в ыте и хайве

Где (2) это либо очень простой конфиг, либо вызов api.

## Требования

* Типизация. Все сообщения схематизированы в протобафе.
Позволяем эволюцию: добавлять поля, возможно удалять? Отлавливаем несовместимые изменения, возможно.
Подключаем валидатор, при наличие. Схема не обязательно плоская. 
Разные типы событий пишутся разными потоками, в разные таблицы. 
Даня предлагает разбить фронтлог на отдельные события-схемы-таблицы. Но на уровне апи это не форсим.
* Гарантии работоспособности при выключении одного из стораджей (lb и s3). Скорее всего будут проблемы с обеспечением 100% гарантии доставки для одиночных сообщений. Так что будут батчи в памяти и флаш при завершении работы.
* Полностью rt-конфигурация. Схема из sraas, конфиги в каком-нибудь сторадже.
* Поставка в hive и yt. По желанию, настраивается.
* Возможность получения нового потока (топика в кафке?) путем фильтрации другого потока или слияния других потоков.
* Дедупликация. На уровне стораджа делаем поддержку дедупликации. Вероятно, небесплатно - дополнительная задержка, мб ограничения по старости события.

## Крупноблочные компоненты системы

* Брокер. API для отправки событий. Принимает события, валидирует по схеме и валидатору (при наличие аннотаций валидации). 
Надежно сохраняет их в персистентный источник, с возможностью переживать его недоступность. 
Конечный результат распихивает по нужным топикам.
* Фильтратор. Читает топик, фильтрует, пишет в другой топик.
* Трансмиттер. Раскладывает топик по хайву или yt. Автоматически создает и обновляет схему.
* Контроллер. Содержит конфигурации всех потоков, что куда пишется, как фильтруется, куда доставляется. Плюс делает ретроспекцию топиков на тему, сколько там групп и мб данных. 
* Конфигуратор?. Отдельный сервис для быстрой доставки конфигурации и применение их без рестарта сервисов.
* Коллбачер. Транслирует события из топика в потребителя колбэками (если кому-то будет нужен)

Клиенты шлют в брокер. Брокер распихивает по топикам. Эти топики потом дополнительно обрабатываются фильтратором, чтобы получить нужные потоки событий. Трансмиттеры раскладывают это по базкам. Настройки всего централизованно хранятся в конфигураторе, над которым еще и админка сделана (нет, конечно, но мало ли).

## Имплементация

Все сообщения провязываем версией schema-registry и тянем схему из sraas, без исключений.

### Гарантия доставки

Имеем два варианта:

1. Брокер всегда пишет в надежный сторадж и оттуда уже разгребает в нужные топики.
2. Брокер сразу пишет в нужные топики, а при проблемах фолбэчится на дополнительный сторадж.

В качестве стораджа можно использовать lb. Как минимум потому что у него per-dc инсталляция и всегда можно попробовать сфолбэчится на другой дц.
В (2) нужно будет как-то обрабатывать ситуацию, когда запись в топики идет, но не успевает с потоком. Хотя в (1) та же самая ситуация может быть. Простой вариант - мы посылаем клиента (429 или backpressure стрима). Сложнее - подключаем запасной сторадж, куда сливаем то, что не успеваем. Идем по пути (2)

При штатной остановке брокера судорожно пытаемся распихать неотправленное по стораджам.

При внезапной остановке (оом), наверное, теряем события. Фактически наш брокер это кафка с ack=0, типизацией и фолбэком в другой дц при недоступности.

В фильтраторах и трансмиттерах используем транзакции (в случаях кафки как минимум).

Отправляем клиенту ack-и. Если клиент хочет быть уверен, что данные не потеряются, то он должен их дожидаться.

### Конфигурирование

Стремимся к системе, которую не надо будет передеплоивать из-за изменения каких-то настроек, появления новых поставок, изменения схемы.
При этом нужно как-то обезопасить себя на случай кривых конфигураций.

Имеем настройки

* Потоки. Это топики в кафке/лб и тип сообщений в них. Схема должна быть в sraas
* Перекладывания с фильтрацией. Ссылается на два потока с опциональным фильтром. Нужно еще придумать, как нам описывать эти фильтры. 
* Подключить какой-нибудь интерпретируемый язык? Или простые фильтры вида field in ('v1', 'v2')
* Поставка в yt/hive. Настройка срока хранения. Для ытя еще партиционирование сегодняшних событий, возможно.

Думаем сделать это так. Имеем конфиги в репозитории, с их схемой (для валидации). После принятия пр-а с изменениями, они с версией отправляются в сервис, который будет заниматься раздачей их другим сервисам через апи. Наверное, с версионностью, чтобы при случае можно было быстро откатиться на прошлую версию.

### Поставка в hive/yt

В hive повторяем текущий трансмиттер.
Единственное, добавляем динамичность - новые поставки должны заводиться без рестарта, то же про изменения схемы.
Тут нужно будет научиться распределять топики разной толщины по кластеру (сейчас это просто разные инстансы, с разными параметрами контейнера).

Наверное, добавляем exactly-once относительно кафки, как сделано в новом холокроне.

YT делаем по копирке. Единственное, логфеллер использует N-минутное партиционирование для недавних данных. Надо понять, зачем это делается (а не просто дописывается в текущий день), возможно, нужно будет делать так же.

Делаем дедупликацую на уровне записи в yt и мб хайв

### Фильтрация/перенаправление

Фактически это микро-джобы, которые читают из топика, фильтруют (или нет) и пишут в другой топик.

Непонятно, как делать фильтрацию, чтобы хватило всем. Как минимум просто field in (values). Как максимум - движок выражений.

Нужно ли нам думать о преобразовании.
С одной стороны иногда возникает ситуация, когда то, что находится в кафке нужно разложить по хайву как-то по-особенному. 
С другой, не уверен, что здесь этому место. Хочешь поставку в нужном формате - шлешь сразу в нужном формате.