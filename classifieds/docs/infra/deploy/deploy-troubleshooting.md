# FAQ по деплою и траблшутинг сервисов в shiva

В этой доке мы собрали самые часто встречающиеся вопросы по деплою и работе сервисов, живущих в shiva. Мы создали отдельный раздел про траблшутинг, чтобы вы могли провести первичную диагностику проблемы работы сервиса: возможно, обнаруженную проблему вы сможете решить **самостоятельно**.

## FAQ

### Вопросы про сетевое взаимодействие

#### Есть локальный компьютер, дев-виртуалка, тестинг и прод. Откуда куда можно ходить?

С локального компьютера и dev-виртуалки вы можете отправить запросы только в тестинг - напрямую в сервис или в балансер. Доступа в прод нет, поэтому ходить в балансер в проде в этом случае нельзя, но сервисы в проде могут ходить друг в друга.

Альтернативный способ ходить в прод с локального компьютера - использование h2p. Документацию можно найти [тут](../tools/h2p/quick-start.md).

#### Моему сервису нужно ходить в сервис Яндекса за пределами Вертикалей, что делать? Сервису Яндекса за пределами Вертикалей нужно ходить в мой сервис, что делать?

Необходимо заказать сетевую дырку в [puncher](https://puncher.yandex-team.ru/) между нашим сетевым макросом и внешним балансером или макросом. Наш сетевой макрос в dev+test `_VERTISDEV_`, в проде `_VERTISPROD_`. Разработческие компьютеры в данные макросы не входят и для них необходимо заказывать персональные дырки (из офиса и/или для VPN).

#### Мои запросы таймаутятся при походе во внешние ресурсы, что делать?

Необходимо использовать проксирование в соответствии с [документацией](../infrastructure/external-proxy.md).

#### Мой сервис переезжает из кондуктора/jenkins в shiva. Что делать с балансерами?

Первый вариант - переключать постепенно клиентов сервиса на балансер, сгенерированный shiva. Для бесшовного переезда можно использовать [дополнительные поля](../service-map.md#old_address_test) в карте сервиса - их необходимо использовать только как временное решение. Дополнительно можно проконсультироваться в чатах verticals-infra-deploy и verticals-duty.

### Деплой

#### Какие инструменты можно использовать для CI/CD?

На данный момент для CI можно использовать тимсити-агентов и гитхаб-раннеры. Для интеграции с шивой можно использовать несколько инструментов:

1. [GRPC-API](../deploy/integration/api.md)
2. [Docker-autorelease](../deploy/ci/docker-autorelease.md)

В рамках тикета [VOID-1917](https://st.yandex-team.ru/VOID-1917) будет проведена работа по созданию единого пайплайна автоматизации CI+CD.

#### Как можно следить за событиями моего сервиса (рестарты, проблемы с запуском, падения сервиса)?

В [телеграм-боте shiva](../deploy/integration/telegram-bot.md) доступна опция подписки на все события сервиса или только на его падения.

#### Наблюдаю проблему с доставкой секретов. Как провести диагностику?

На первом шаге - проверьте [синтаксис](templates.md) для доставки секретов и попробуйте повторно делегировать секрет. Если не помогло - надо писать в чат verticals-infra-deploy.

#### Можно ли переопределять базовые переменные окружения

[Базовые переменные окружения](../service-preparation/default-env.md) с префиксом `_DEPLOY` зарезервированы под систему деплоя. Мы крайне не рекомендуем переопределять их самостоятельно, при коммите таких переменных в манифест деплоя вы получите предупреждение при проверке конфига.

#### Какие есть нюансы при сборке докер-образа с моим приложением?

Все образы мы храним в нашем внутреннем registry - `registry.yandex.net/vertis/<имя_образа>:<тег>`

Базовые образы можно найти в `registry.yandex.net/vertis-base/ubuntu:<trusty/xenial/bionic>`

При создании докер-образа из Dockerfile настоятельно рекомендуем в строке с директивой `FROM` использовать один из образов, уже находящихся в нашем registry, а внешний образ - переложить перед использованием в наш registry. Завязываться на внешние registry (например, docker hub) мы настоятельно не рекомендуем - лимиты на `pull` образов уже введены, и образ может стать недоступным.

#### Участвует ли тестинг или дев в учениях?

Нет, учения только для прода. Но во время общеяндексовых работ - участвует и дев, и тестинг.

#### Будет ли работать деплой в прод во время учений?

Да, в открытом датацентре, если ваш сервис живет в Shiva. Если он катится через Jenkins или Conductor, деплой работать не будет, и мы рекомендуем вам переезжать в Shiva. Если закрывается VLA, деплоиться можно в SAS и наоборот. При закрытии ДЦ MAN или MYT - деплой продолжает работать и во VLA, и в SAS для всех способов деплоя.

## Траблшутинг деплоя

В этом разделе находится чек-лист для самостоятельного траблшутинга проблем при деплое - возможно, проблему вы сможете диагностировать и решить самостоятельно.

1. Проверяем карточку деплоя и описание ошибки. Shiva пробрасывает человеко-понятное описание ошибки в карточку деплоя.

    Пример карточки деплоя с ошибкой:
    ![deploy-card](images/deploy-card.png)

   1.1. Если видим **unhealthy** - необходимо проверить наличие [health check](../service-preparation/service-requirements.md#healthcheck). Если его нет - прикрутить.

   1.2. Если хелсчек на месте, необходимо проверить, падали ли хелсчеки на инстансах приложения, пользуясь [этим графиком](https://grafana.vertis.yandex-team.ru/d/000000228/nomad-services-statistics?orgId=1&refresh=30s&viewPanel=27&var-datasource=Prometheus-testing&var-job=autoru-api&var-task=autoru-api-task&var-alloc_id=All&var-group=All&var-dc=All&var-host=All&var-average_interval=1m&from=now-30m&to=now). Если хелсчек падает на одном и том же хосте при повторении попытки деплоя - необходимо обратиться к дежурному админу или в чат `verticals-infra-deploy`, скорее всего это инфраструктурная проблема. Если проблема воспроизводится на разных хостах - проблема на стороне сервиса.

2. Проверить логи приложения. Под катом **Неудачные allocations** есть ссылки на логи каждого из проблемных инстансов приложения.

3. Если вы видите странное распределение нагрузки между датацентрами (трафик идет только в один из них) - проверьте, не потерялись ли ваши аллокейшены с помощью [графика потерянных аллокейшенов](https://grafana.vertis.yandex-team.ru/d/c7t4AvW7k/nomad-jobs-and-allocations-statistics?orgId=1&viewPanel=94&var-datasource=Prometheus-testing&var-dc=All&var-host=All&var-job=All&var-periodic_job=All&var-group=All&var-task=All&var-topN=50). Единица около имени сервиса означает, что аллокейшены в указанном датацентре пропали. Чтобы их вернуть - достаточно рестарта сервиса. В случае, если все инстансы на месте - обратитесь в чат verticals-duty.

**Если эти методы вам не помогли - нужно обратиться в чат verticals-infra-deploy.**

## Полезные графики

1. Shiva автоматически генерирует дашборды для сервисов. Про это можно прочитать [тут](../auto.md#grafana-dashboard).

2. Дашборд [Nomad jobs and allocations statistics](https://grafana.vertis.yandex-team.ru/d/c7t4AvW7k/nomad-jobs-and-allocations-statistics?orgId=1&var-datasource=Prometheus-testing&var-dc=All&var-host=All&var-job=All&var-periodic_job=All&var-group=All&var-task=All&var-topN=50) содержит много графиков с посервисными фильтрами.

3. Дашборд [Nomad services statistics](https://grafana.vertis.yandex-team.ru/d/000000228/nomad-services-statistics?orgId=1&refresh=30s) содержит информацию о потреблении ресурсов сервисами и информацию о рестартах сервисов.
