# Best practices для Yandex.Cloud

Эта дока посвящена тому, как правильно и удобно пользоваться Яндекс.Облаком, обновлять compute-хосты и инстанс-группы.

## Как пользоваться репозиторием terraform

Все изменения для поселения и апгрейда кластеров в облаке мы делаем в репозитории [terraform](https://bb.yandex-team.ru/projects/YANDEX-CLASSIFIEDS/repos/terraform/browse), управляем кластерами с помощью одноименной утилиты.

Структура репозитория следующая:
* `terraform/modules` - здесь живут шаблоны для поселения кластеров - файлы, в которые подставляются переменные из модулей для кластера
    * `yandex_compute_instance` - шаблон для compute-нод. Нужен в большинстве случаев. Из особенностей - группы размещения мы используем только для консул-серверов.
    * `yandex_compute_instance_group_fixed_scale` - шаблон для инстанс-групп. Инстанс-группы нужны вам, если вы хотите, чтобы облако гарантировало число живых хостов в кластере и само их лечило переналивкой или рестартами. Это шаблон инстанс-группы с **фиксированным** количеством хостов.
    * `yandex_compute_instance_group_auto_scale` -  шаблон инстанс-групп с автоскейлингом. Нужен, когда хочется сдувать и раздувать кластер по определенным метрикам. На текущий момент захардкожено использование двух метрик, но вместо одной можно вставить заглушку.
* `terraform/compute` - здесь живут конфиги для кластеров, в которые мы прописываем переменные для подстановки в шаблоны. В целом, IDE Goland и сам terraform всегда подскажет, каких переменных не хватает, а какие лишние. Хорошая практика - отдельный "модуль" внутри конфига для каждого датацентра.
* `terraform/cloud-init-templates` - здесь живут шаблоны cloud-init конфигов, из которых приезжают разные вещи и запускаются скрипты при подъеме тачки.
* `terraform/utils` - разные полезные и нужные скрипты.

**Всегда** нужно помнить о важной вещи: изменения, которые вносятся внутри `terraform/compute` изолированные и не распространяются на другие кластера в облаке. **НО** внесение изменений в `terraform/modules`, `terraform/variables` или `terraform/cloud-init-templates` могут быть непредсказуемыми и деструктивными, и скорее всего, потребуют изменения всех конфигов в `terraform/compute` и спровоцируют редеплой уже работающих кластеров, поэтому делать изменения в них нужно с осторожностью.

## Переменные в конфигах кластера

В конфигах кластеров мы вместо большого количества id'шников (сервисный аккаунт, сеть, подсеть, диски, фолдеры и т.д.) решили ссылаться на переменные из файла `terraform/variables/variables.tf`. Id в пределах тестинга и прода различаются, как в некоторых случаях и для зон. Инструкция по использованию общих переменных лежит [тут](terraform.md#dobavlyaem-novuyu-gruppu-hostov)

Сервисный аккаунт - робот облака с определенными правами, от имени которого:
1. Выполняются операции по подъему хостов;
2. Выполняется поход за первичным секретом.

Также у нас есть сервисный аккаунт для инстанс-групп, который используется в соответствующих конфигах инстанс-групп. Он осуществляет управление операциями в инстанс-группах.

## Безопасная работа с terraform

**Главное правило** - всегда нужно выполнять `terraform plan` и проверять, правильный ли план изменений предлагает терраформ.

Редеплой - это полная потеря хостов в кластере с рестартом или пересозданием. Он требуется часто и к нему нужно быть готовым.
Как правило, в большинстве случаев изменения в конфиге могут быть применены in-place без редеплоя, но если уверенности в безопасности операции нет - лучше протестировать свои изменения на `compute/terraform_dev`.

Чтобы знать наверняка, к чему приведут применяемые изменения, лучше подглядывать в доку от самого Облака - [Последствия изменений виртуальных машин](https://cloud.yandex.ru/docs/compute/concepts/instance-groups/deploy/instance)

## Обновление compute-кластеров

Общий пайплайн работы с compute следующий:

1. Проверить, приводят ли изменения к редеплою кластера. Самый простой способ - применить изменения на `terraform_dev` и посмотреть, что будет. Если хосты рестартятся или пересоздаются - нужно обратиться к следующим шагам. Если этого не происходит, и результат операции успешный (целевые параметры изменились), можно смело применять изменения на нужном кластере.
2. Если происходит редеплой - нужно придумать стратегию подготовки к безопасному выключению хостов, например:
    * для хостов с номад-клиентами это дрейн тачки;
    * для номад-сервера - перебрасывание лидера рестартом, если хост является лидером;
    * для консул-сервера - leave для лидера, для остальных - хост можно просто выключить.
3. Понять, сколько хостов можно можно потерять. В случае номад/консул серверов - это только один хост, для докера - до ~пяти одновременно.
4. Прописываем целевые изменения в конфиге и в поле `ignore_changes` добавляем дополнительно ту секцию конфига, которую собираемся изменить, делаем `terraform plan` и ожидаем получить ответ "No changes. Infrastructure is up to date." Если терраформ предлагает сделать изменения, значить что-то пошло не так.

    **Пример**

    В конфиге некоторого compute-кластера хотим сделать изменение поля `node_memory = 16` и выделять хостам 32 Gb RAM. Изменяем соответствующее поле: `node_memory = 32`.
    Затем необходимо проверить актуальное состояние файла-шаблона `terraform/modules/yandex_compute_instance`
    ```
    resource "yandex_compute_instance" "vm" {
      lifecycle {
        ignore_changes = [network_interface, boot_disk, metadata]
      }
        <...>
      resources {
        cores  = var.node_cores
        memory = var.node_memory
        <...>
      }
    ```
    В поле `ignore_changes` не прописан блок `resources`, а значит изменения, которые будут применены, не будут проигнорированы для запущенных машин. Чтобы ничего не разломать, добавляем эту секцию в поле: `ignore_changes = [network_interface, boot_disk, metadata, resourcs]`, чтобы изменения применялись только к новым хостам.

5. Если в облачной документации из блока про безопасную работу с терраформ написано, что для применения изменений достаточно рестарта хоста - делаем подготавливаем хост и ребутаем. Если необходимо пересоздание - подготавливаем хост к выключению, выключаем, удаляем его через UI облака. После этого возвращаемся в terraform и выполняем `terraform plan`, убеждаемся, что будет создан один хост, и делаем `terraform apply`. Это нужно для того, чтобы убедиться в том, что изменения будут применены только к создаваемым хостам.
6. Пункты 4-5 повторяем до полного редеплоя кластера.
7. Убираем из ignore_changes то, что дописали в пункте 4 и проверяем с помощью `terraform plan`, что инфраструктура соответствует конфигу и больше изменений нет.

Когда необходимость изменений не срочная, целесообразно их собрать пачкой и выполнить редеплой.

## Обновление инстанс-групп

Обновление инстанс-группы докер-хостов в тестинге проводится без проблем и дополнительных манипуляций не требует, т.к. мы используем хелсчеки для того, чтобы облако понимало, когда определенный хост начал полноценно работать и вся инфраструктура на нем настроена правильно.

Обновление других инстанс-групп более сложная задача, потому что опции `ignore_changes` нет, и простое применение изменений может привести к тому, что потеряем весь кластер.
На этот случай есть такой лайфхаки по обновлению:

1. Переключить инстанс-группу с автоскейлинга на фиксированное масштабирование, чтобы ничего не разнесло. Для этого в `source` изменяем файл, комментим блок переменных для автоскейлинга и прописываем переменные для фиксированного скейлинга, `terraform plan` и применяем изменения.

    <details>
        <summary>Пример</summary>

        Конфиг инстанс-группы тимсити-агентов во vla до переключения:

             module "vertis_vtest_tc_cloud_vla_ig" {
             source = "../../modules/yandex_compute_instance_group_auto_scale"
             <...>
           # autoscaling
             initial_size_ig  = 38
             max_size_ig      = 46
             min_zone_size_ig = 26

             stabilization_duration_ig = 1800
             warmup_duration_ig        = 600
             measurement_duration_ig   = 600

             first_metric_name   = "<metric1_name>"
             first_metric_target = 87

             second_metric_name   = "<metric1_name>"
             second_metric_target = 87
           }

        Конфиг после изменений:

           module "vertis_vtest_tc_cloud_vla_ig" {
             source = "../../modules/yandex_compute_instance_group_fixed_scale"
             <...>

             compute_count = 15

           # autoscaling
             #initial_size_ig  = 38
             #max_size_ig      = 46
             #min_zone_size_ig = 26

             #stabilization_duration_ig = 1800
             #warmup_duration_ig        = 600
             #measurement_duration_ig   = 600

             #first_metric_name   = "<metric1_name>"
             #first_metric_target = 87

             #second_metric_name   = "<metric1_name>"
             #second_metric_target = 87
           }
    </details>
2. В терраформе выбираем [деликатную стратегию](https://cloud.yandex.ru/docs/compute/concepts/instance-groups/policies/deploy-policy#strategy) остановки виртуальных машин `strategy: OPPORTUNISTIC` и применяем изменения. Этот шаг нужен для того, чтобы самим выбирать, какую тачку в текущий момент редеплоить.

    <details>
        <summary>Пример</summary>

           module "vertis_vtest_tc_cloud_vla_ig" {
             source = "../../modules/yandex_compute_instance_group_fixed_scale"
             <...>
             deploy_policy_strategy = "OPPORTUNISTIC"
           }
    </details>
3. Меняем конфиг как нам нужно, смотрим изменения с помощью `terraform plan`, проверяем правильность изменений и применяем их с помощью `terraform apply`.
4. Теперь можно редеплоить группу хостов: подготавливаем хосты к выключению (как при обновлении compute, пункты 2-3), и ребутаем или выключаем их и удаляем через UI (аналогично пункту 5 про compute). Инстанс-группа сама включит или перенальет эти хосты.
5. Повторяем до полного редеплоя.
6. Переключаем стратегию остановки на принудительную.

    <details>
        <summary>Пример</summary>

           module "vertis_vtest_tc_cloud_vla_ig" {
             source = "../../modules/yandex_compute_instance_group_fixed_scale"
             <...>
             deploy_policy_strategy = "PROACTIVE"
           }
    </details>
7. Включаем автоскейлинг.

    <details>
        <summary>Пример</summary>

        До:
            module "vertis_vtest_tc_cloud_vla_ig" {
             source = "../../modules/yandex_compute_instance_group_fixed_scale"
             <...>
             compute_count = 15
             <...>
           }

        Конфиг после изменений:
           module "vertis_vtest_tc_cloud_vla_ig" {
             source = "../../modules/yandex_compute_instance_group_auto_scale"
             <...>
           # autoscaling
             initial_size_ig  = 38
             max_size_ig      = 46
             min_zone_size_ig = 26

             stabilization_duration_ig = 1800
             warmup_duration_ig        = 600
             measurement_duration_ig   = 600

             first_metric_name   = "<metric1_name>"
             first_metric_target = 87

             second_metric_name   = "<metric1_name>"
             second_metric_target = 87
           }
    </details>

**Пример задачи и решения**
Нужно в инстанс-группе тимсити-агентов с автомасштабированием поменять диски.

Переключаем группу на фиксированное масштабирование, изменяем стратегию на деликатную и применяем ее.
Изменяем размер диска в конфиге, делаем `terraform plan` и `terraform apply`.

Мы можем одновременно потерять 5 тачек в ДЦ - дрейним их и удаляем в UI, чтобы создались новые тачки с новыми дисками. Облако само их перенальет.

Включаем принудительную стратегию остановки, возвращаем автоскейлинг.


## Переключение автомасштабирование - фиксированное масшстабирование

При переходе с фиксированного масштабирования на автоматическое достаточно переключить конфиг и применить изменения, но `min_zone_size` должен быть равен или немного меньше `initial_size` инстанс-группы. Облако попытается сдуть ваш кластер в ноль, если этого не сделать. Когда метрика autoscaler_decision в мониторинге кластера в Yandex.Cloud стабилизируется, можно постепенно снижать `min_zone_size` до нужного уровня.

При нескольких переключениях фикс-авто машстабирования нужно не забывать проверять переменную `initial_size`.

При переключении с автомасштабирования на фиксированное количество хостов подводных камней нет.
