# Teamcity
Прод: [https://t.vertis.yandex-team.ru/](https://t.vertis.yandex-team.ru/)
Тестинг: [https://t.test.vertis.yandex-team.ru/](https://t.test.vertis.yandex-team.ru/)

## Деплой
Сервис живёт в lxc.
Прод: `%vertis_vprod_tc_srv`
Тестинг: `%vertis_vtest_tc_srv`
Пакет: `yandex-vertis-teamcity-server`, представляет собой перепакованный официальный тарболл с некоторыми нашими дополнениями: monrun-конфиг, systemd-юнит, мб что-то ещё.
[Сборка пакета](https://t.vertis.yandex-team.ru/project.html?projectId=VertisAdmin_YandexVertisTeamcityServer).
Скачивание тарболла и упаковывание в пакет происходит во время выполнения сборки, поэтому запускать её нужно обязательно указав версию Teamcity (чтобы скачать тарболл нужной версии). Так же нужно указать нашу ревизию, например `vertis0`.
Для выкладки пакета и наших конфигов на хосты используются соответствующие группам плейбуки. На прод катятся роли: `teamcity-server`, `teamcity-filesync` и `teamcity-backup`. В тестинг же катим только `vertis-teamcity`.

## Конфигурация
Основная часть настроек выполнена в морде Teamcity, и хранится, соответственно, в базе mdb/mysql, либо в конфигах сборок на файловой системе в реплицируемой рабочей директории Teamcity. Поэтому очень желательно всё это не продолбать ^^ (см. [Бэкапы](#bekapy)).

## Особенности
Архитектурно данные Teamcity хранятся в 3 сущностях:
  - в СУБД (у нас это mdb/mysql) храним общие настройки и некоторую мета-информацию о сборках;
  - на файловой системе в рабочей директории лежат конфиги, логи сборок, плагины и что-то ещё;
  - артефакты сборок выкладываются в S3. Настроено это [здесь](https://t.vertis.yandex-team.ru/admin/editProject.html?projectId=_Root&tab=artifactsStorage).

## Балансировка и отказоустойчивость
Полноценного HA у Teamcity ещё не изобрели. В каждый момент времени должен выполняться только один инстанс TC. Попытка запустить более одного инстанса над общей СУБД и/или реплицируемой рабочей директорией может привести к split brain. Поэтому сервис работает в режиме ручного фейловера: сервис выполняется только на одном из серверов (как правило, во Владимире).
Чтобы минимизировать вероятность запуска сервиса сразу на обоих серверах, у TC отключен автозапуск при загрузке ОС. Поэтому если у нас ребутнулся сервер с активным инстансом TC, запустить его после заргузки сервера или на сервере в другом ДЦ нужно вручную.
Есть мониторинг того, что выполняется ровно 1 инстанс: не менее (что означало бы недоступность сервиса), и не более (что означало бы потенциальный split-brain).
Входная точка в Teamcity от людей и от билд-агентов - это nginx-балансер, кластера `%vertis_vprod_lb_int_nginx` и `%vertis_vtest_lb_int_nginx`  в проде и в тестинге. Апстрим для прода - noc-slb `tc-srv-int.noc-slb.prod.vertis.yandex.net`, растянутый над TC-серверами.

## Синхонизация рабочей директоии
Рабочая директория Teamcity должна быть реплицирована между серверами на случай переключения сервиса в другой ДЦ. Сейчас используется периодическая синхронизация через rsync, с локами в zk-flock. Лог синхронизации пишется в `/var/log/teamcity-filesync/teamcity-filesync.log`.
Раз в 5-10 минут по крону синхронизируется содежимое директории `/opt/TeamCity/BuildServer`, с некоторыми исключениями, описанными в скрипте синхронизации - это кэши, бэкапы и что-то ещё.
У синхронизации есть monrun-чек, который проверяет, что последняя периодическая синхронизация была выполнена не слишком давно (не более 60 минут назад), что процесс занял не слишком много времени (не более 15 минут), и что в процессе не возникло ошибок.
Перед запуском rsync скрипт синхронизации выполняет несколько проверок: смотрим, что размер стораджа изменился не слишком сильно (текущее значение проверки - 10%), кроме того не синхронизируем в случае, если локально teamcity-server не запущен (не отвечает его api). То есть синхронизируем только с активной ноды.
Если нужно вручную разово выполнить синхронизацию, запускаем на исходном сервере команду `tc-filesync.sh -v`и дожидаемся её завершения. Процесс занимает несколько минут, следить можно в логе. Если размер стораджа изменился выше порога, синк не выполняется, зажигается мониторинг. Можно принудительно выполнить синхронизацию через ключ `-f`:
`tc-filesync.sh -v -f`.
Если нужно принудительно синхронизировать сторадж в случае, если teamcity-server локально не запущен, то опять же ключ `-f` поможет в этом.
Выкатывается синхронизация ролью `teamcity-filesync`.

## Переключение в другой ДЦ
Основной способ переключения - через `drills-helper`, который, в свою очередь, выполняет скрипт `/usr/bin/switch-master.sh`.
Если что-то идёт не так, то переключение можно выполнить вручную. Для этого:

  - останавливаем сервис на старом сервере:
  `service teamcity-server stop`
  - выполняем финальную синхронизацию рабочей директории:
  `tc-filesync.sh -v -f`
  Если скрипт падает с сообщением "According status file and rsync process running sync is in progress. Exiting.", смотрим его лог. Если в данный момент действительно выполняется периодическая синхронизация, дожидаемся её завершения, после чего снова пробуем выполнить финальную синхронизацию.
  - запускаем сервис на новом сервере:
  `service teamcity-server start`

## Мониторинг
Замониторили всё что могли. У Teamcity есть monrun-чек, однако он не прокинут в Juggler, так как на одном из хостов сервис всегда остановлен. Есть метрики в прометеус, дашборды смотреть тут:
  - [teamcity](https://grafana.vertis.yandex-team.ru/d/_BCntTLmk/teamcity)
  - [teamcity-raw](https://grafana.vertis.yandex-team.ru/d/UE9VUpPZz/teamcity-raw)

Графички mdb/mysql:
  - [solomon](https://solomon.yandex-team.ru/?cluster=mdb_mdbl1tlqoi585p4hf2qg&project=internal-mdb&service=mdb&dashboard=internal-mdb-cluster-mysql)
  - [yc](https://yc.yandex-team.ru/folders/foof1m2t24sjktbjo7ej/managed-mysql/cluster/mdbl1tlqoi585p4hf2qg?section=monitoring)

## Бэкапы
Бэкапы делаются средствами самого Teamcity, складываются нашим скриптом в S3, являют собой .zip-архивы, и включают в себя:
  - содержимое БД
  - конфиги сборок
  - логи сборок
  - настройки Teamcity.

Проще говоря, бэкап содержит вообще всё кроме артефактов сборок, которые и так уже в S3.
Запускается бэкап раз в сутки в 2 часа ночи. Лог-файл: `/var/log/teamcity-backup/teamcity-backup.log`
Есть мониторинг, который зажигается, если:
  - последний бэкап был слишком давно (более 36 часов назад)
  - процесс бэкапа занял слишком много времени (более 9 часов)
  - в процессе возникли ошибки

Мониторинг сделан warn'ом ночью и в выходные.
Бэкапы складываются в S3 в бакет `vertis-backups`, в директории `teamcity` и `teamcity-monthly` - ежедневные и едемесячные бэкапы, соответственно. Ежедневные бэкапы ротируются этим же скриптом, оставляя только 7 последних бэкапов, удаляя остальные. Ежемесячные бэкапы не ротируются.
Скрипт содержит небольшой хелп по использованию:
`$ tc-backup.sh`
Сейчас процесс выполнения бэкапа занимает около 4 часов, так что если зачем-то запускаем вручную, то лучше это делать в tmux'е.
Кроме бэкапов, ведётся так же история изменения билд-конфигураций, смотреть можно в [этом репо](https://bb.yandex-team.ru/projects/YANDEX-CLASSIFIEDS/repos/vertical-teamcity-backup/browse).

### Восстановление из бэкапа
Для восстановления бэкапа пользуемся тулзой maintainDB.sh из поставки Teamcity. Документация [тут](https://confluence.jetbrains.com/display/TCD18/Restoring+TeamCity+Data+from+Backup).
В общем случае процесс восстановления будет таким:
  - останавливаем Teamcity
  - смотрим какие есть бэкапы в S3 командой `tc-backup.sh -l`
  - скачиваем файл бэкапа из S3 в текущую директорию: `tc-backup.sh -d <filename>`
  - и далее восстанавливаемся утилитой `maintainDB.sh` из поставки Teamcity примерно так:

`sudo -u vertis-tc JAVA_HOME=/usr/lib/jvm/java-8-oracle/jre TEAMCITY_MAINTAINDB_MEM_OPTS=-Xmx4g /opt/TeamCity/bin/maintainDB.sh restore -A /opt/TeamCity/BuildServer -F <filename> --continue`

Ключ `--continue`позволяет восстановить только отсутствующие куски данных. Думаю, что потенциально именно этот кейс может пригодиться, если продолбалось только часть чего-нибудь, например только содержимое БД.
Если же хотим сделать полное восстановление, то нужно:
  - дропнуть все таблицы из mdb/mysql
  - удалить содержимое рабочей директории `/opt/TeamCity/BuildServer`  кроме конфигов `database.properties`, `ldap-config.properties `
  - сделать восстановление из бэкапа без ключа `--continue`.

Нужно понимать, что полное восстановление займёт значительно больше времени, нежели частичное - несколько часов, задача примерно на целый день.

## Лицензии
Сейчас лицензии закуплены через Яндекс. Мы располагаем отдельными лицензиями на билд-агентов (по одной на агента), и enterprise-лиценизей на безлимит по количеству сборок. Все они [добавлены в TC](https://t.vertis.yandex-team.ru/admin/admin.html?item=license).
Ключи лицензий продублированы в [кредах](#kredenshely).

## Креденшелы
[mysql production](https://yav.yandex-team.ru/secret/sec-01cwbh30415ff9xng09q21v6yr)
[mysql testing](https://yav.yandex-team.ru/secret/sec-01d0vw8gb8g59et3yv59s4d90h)
[Пароль админа](https://yav.yandex-team.ru/secret/sec-01cwbh4j8qb788y6st947bedv8)
[Лицензионные ключи](https://yav.yandex-team.ru/secret/sec-01d0vwfsayz86anq0csv5f7pjh)
