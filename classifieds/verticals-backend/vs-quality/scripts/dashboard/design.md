# Обзорный Дашборд
## Используемые Подходы
### Дашборды отдельных систем
Пожалуй, самый распространенный подход, при котором для каждой новой системы делается отдельный дашборд с полным набором метрик, который позволяет получить информацию о состоянии и динамике всех базовых индикаторов функционирования сервиса.
Плюсы:
* позволяет собрать вместе исчерпывающий объем информации для решения рядовых проблем

Минусы:
* с ростом и усложнением сервиса растет и количество индикаторов, что усложняет чтение и интерпретацию данных
  * теряется локальность, связанные метрики зачастую просто невозможно положить рядом (для наглядности)
  * усложняется процесс поиска конкретного индикатора
  * на определенном этапе начинаются сложности в интерпретации отдельных индикаторов у людей, не занимающихся непосредственно разработкой конкретного сервиса
* так как вся информация изолирована на отдельных дашбордах, возникает еще рад сложностей
  * тяжело обнаруживать аномалии работы, не приводящие к серьезным проблемам и алертам, как следствие. Возникает прежде всего потому, что с ростом числа сервисов плавно теряется смысл отслеживать детально работу каждого из них, если не наблюдается явных проблем.
  * сложно сопоставлять проблемы разных сервисов. В частности, то, что некоторой сервис был затронут общей проблем можно не узнать вовсе, либо узнать со значительным запозданием (из-за особенностей работы сервиса, например) 
  * нет возможности оперативно выяснить, все ли сервисы на текущий момент находятся в работоспособном состоянии или нет

### Обзорная информация на базе Heatmap
Довольно распространенный подход, при котором состояние отдельных индикаторов отображается разным цветовом, в зависимости от состояния. Собрав вместе на одной странице индикаторы по разным системам, можно получить интересующую нас сводную информацию. У подобного подхода есть свои сильные и слабые стороны
Плюсы:
* можно собрать вместе довольно большое количество индикаторов
* получается наглядное представление масштаба имеющихся проблем
* хорошо подходит для определения самого факта наличия проблем

Минусы:
* не видна динамика изменения состояния. Если проблема кратковременная, то она просто исчезает с дашборда
* недостаток средств структурирования представления. Heatmap как средство отображение предполагает в некотором смысле одноранговость индикаторов, что накалывает свои ограничения на имеющиеся средства отображения
* из-за предыдущих двух проблем сложно восстанавливать взаимосвязи имеющихся проблем
* для получения любой дополнительной информации необходимо переходить на профильный дашборд и искать нужные индикаторы
* если проблема затрагивает несколько систем, сбор информации значительно затягивается

## Зачем нужен новый вариант
Рассмотрим типовую последовательность действий в случае возникновения проблем с сервисом:
1. некоторым способом получаем информацию о факте наличия проблемы (получаем алерт, например)
2. проверяем индикаторы связанные непосредственно с проблемой (если проблема, например, с лагом, то смотрим динамику изменения лага и скорость обработки)
3. если индикаторы в норме, то начинается поиск сервиса, который повлиял на работу текущего (обычно анализ логов, либо проверка индикаторов работы связанных сервисов)
4. выясняются причины аномалий уже в конкретном сервисе

Каждый из пунктов требует времени, но что хуже, каждый из них требует определенных знаний, чтобы найти релевантную информацию. Усугубляет ситуацию еще и то, что каждый сервис может иметь собственные особенности, что становится особенно критичным с ростом количества сервисов. При этом, как можно было заметить, первые три пункта относятся к локализации проблемы, и только последний к поиску непосредственных причин. На практике это означает, что потребность в детальной информации появляется только в рамках последнего пункта, до него же нам по большей части нужны лишь некоторые базовые индикаторы.

Основное, чего хотелось бы добиться, это схлопнуть и упростить действия при возникновении проблем на столько, на сколько это возможно (особенно актуально это становится в случае критических ситуаций). Однако, текущие же подходы призваны по большей части решать частные задачи и плохо масштабируются и обобщаются. В частности:
* дашборды отдельных сервисов ориентированы по большей части на пункт (4), т.е. поиск конкретных причин. Пункт (2) в случае большого сервиса может оказаться уже не тривиальным, что в свою очередь значительно усложняет пункт (3).
* обзорный дашборд на базе heatmap ориентирован на пункт (3), причем очень ограниченно, из-за описанных выше причин.

## Основные идеи построения
* чтобы упростить пункт (1), т.е. обнаружение проблем, предлагается снабдить дашборд цветовой индикацией в случае аномально поведения
* чтобы упростить пункт (2), т.е. первичную проверку, предлагается вынести на обзорный дашборд не индикаторы наличия проблем, а скорее базовые метрики, которые наиболее полно описывают работу конкретного сервиса (3-4 метрики, по своей сути близкие к SLI). Понятно, что все метрики и краевые случаи вынести не удастся, поэтому предполагается некоторое ранжирование, а также опора на штатные каналы нотификации о проблемах.
* чтобы сделать пункт (2) доступным для людей не являющихся непосредственными разработчиками конкретных сервисов, предлагается полагаться прежде всего на интуитивно понятные наборы метрик
* чтобы упростить пункт (3), т.е. поиск связанных сервисов и первопричин проблемы, предлагается собрать вместе на одном дашборде базовую информацию по максимальному количеству сервисов, способных повлиять на работу друг друга. Понятно, что абсолютно все тут вместить не выйдет, поэтому предполагается некоторое ранжирование исходя из прошлого опыта. Также предполагается делать корректировки при возникновении новых проблем. Работа сервисов не способных сильно повлиять на нагрузку у окружающих может попрежнему контролироваться используемой системой алертов.
* чтобы сделать пункт (3) доступным для людей не являющихся непосредственными разработчиками конкретных сервисов, предлагается явным образом группировать индикаторы сервисов в наглядные группы
* чтобы упростить пункт (4), т.е. выявление конкретных причин проблем, предлагается снабдить каждый блок индикаторов (соответствующий конкретному сервису) списком ссылок на все ресурсы, которые могут помочь разработчику (и не только) в поиске, даже если он не осведомлен о всех доступных источниках информации по функционированию сервиса.

### Особенности формирования блоков метрик для сервиса
Особых требований тут нет, но чем более консистентно будет представление, тем легче будет воспринимать данные. В связи с этим предлагается следующая структура метрик в блоке:
```
+-----------+---+
|   [1]     |[2]|
+-------+---+---+
|  [3]  |  [4]  |
+-------+-------+
```
При этом, нижние метрики предлагается использовать по возможности для описания интенсивности / качества работы самого сервиса (блоки [3] и [4]), а верхний блок [1] для описания нагрузки, которую сервис создает на другие сервисы. Блок [4] вспомогательный и может опускаться, расширяя тем самым блок [1].

Блоки разных сервисов, логически связанных друг с другом, предполагается размещать на дашборде максимально близко друг к другу.

Подобная структура блоков диктуется прежде всего ограничениями, которые накладывает Grafana. Сложность заключается в том, что в ней используется тайловая система, т.е. по горизонтали экран делится на 24 столбца и просто невозможно создать блоки меньшего размера чем блок [2]. Если будет найдена или написана альтернативная система, обладающая большей гибкостью, то структура блоков может быть пересмотрена, но на базовые идеи построения это никак не влияет.

### Особенности используемых метрик
Ограничений на используемые метрики особо нет, но чем более однотипными они будут, тем легче будет читать информацию в целом. В связи с этим предлагается следующий набор базовых конфигураций:
```
+-----------+---+
|   Rate    | N |
+-------+---+---+
|Latency| Errors|
+-------+-------+
```
* для api
  * количество входящих запросов (блок [1])
  * количество работающих инстансев приложения (блок [2])
  * среднее время ответа (блок [3])
  * количество ошибок (блок [4])

```
+-----------+---+
|  Sends    | N |
+-------+---+---+
| Rate  | Queue |
+-------+-------+
```
* для приложения, обрабатывающего очередь
  * метрика, показывающая уровень воздействия на другие сервисы, т.е. количество запросов к другим сервисам, количество отправленных сообщений / событий и т.п.. Что важнее, решается в конкретной ситуации (блок [1])
  * количество работающих инстансев приложения (блок [2])
  * скорость обработки очереди (блок [3])
  * размер входящей очереди (блок [4])

В идеале, метрики должны отвечать на три вопроса
* какова общая внешняя нагрузка на сервис (rps для api и очередь для processing приложений)
* как он с ней справляется (latency / errors для api и rate для processing приложений)
* какую нагрузку при этом он сознает на других (для api может неявно следовать из rps)

Естественно, если в рамках конкретного сервиса имеются более приоритетные индикаторы, то использоваться должны именно они, но с понятным указанием названия метрики в заголовке.

### Цветовая индикация
Первостепенная задача цветовой индикации в подобном варианте обзорного дашборда немного меняется, т.к. мы уже имеем не столько индикаторы проблем, сколько картину потоков данных и запросов к сервисам. В этом случае цвета показывают скорее степень отклонения работы от нормального уровня, чем наличие проблемы. В частных случаях эти вещи могут совпадать, но далеко не всегда.

Для унификации предлагается интуитивно понятная трехцветная индикация:
* зеленый - конкретный индикатор имеет значение, соответствующее нормальному функционированию.
* оранжевый - имеются существенные отклонения в работе, которые, однако, сами по себе не должны оказывать существенного воздействия. В случае очередей, например, это ощутимое увеличение размера очереди, которое, при этом, должно обработаться в разумные сроки, порядка 10-15 минут (условно, зависит от требований к сервису).
* красный - имеются очень большие отклонения, которых обычно не наблюдаются. Сам по себе красный цвет не означает, что что-то сломано, но определенно показывает, что в процессе функционирования что-то существенно изменилось и это требует внимания.

Естественно, интерпретация цвета сильно зависит от метрики, которой он соответствует. В частности, если красный цвет показывает уровень ошибок от api, то это сильно отличается от красного цвета в уровне rps того же самого сервиса. Главное же, что деструктивный потенциал и того и другого довольно высокий.

### Дополнительные возможности
Дополнительной удобной возможностью в контексте Grafana являются ссылки, которые можно размещать на отдельных графиках. Эту возможность предлагается использовать прежде всего для того, чтобы обеспечить связность имеющихся дашбордов, т.к. сейчас бывает не очевидно, где и что стоит искать.

## Типичные варианты использования
* количество инстансов всех сервисов уменьшилось вдвое - проводятся учения, так что попутно возможны незначительный рост очередей и замедление обработки
* время ответа api сервиса существенно выросло, при этом видно замедление ответов от базы данных - необходимо разобраться с состоянием БД и нагрузкой на нее
* очереди processing сервисов растут, при этом скорость обработки не растет, а время ответа от Kafka сильно выросло - необходимо разбираться с состоянием kafka, возможно часть инстансев выпали / вернулись и идет репликация
* наблюдается повышенный фон ошибок api сервиса - необходимо перейти по ссылкам на логи / sentry, чтобы посмотреть, что именно это за ошибки
* растет очередь конкретного processing сервиса, но скорость обработки при этом нулевая / очень низкая - скорее всего имеется ошибка в процессе обработки некоторых сообщений, которые блокируют обработку. Необходимо перейти по ссылке в логи / sentry, чтобы понять, что именно это за ошибка
* наблюдается высокий rpc у api сервиса - для начала стоит перейти по ссылке на основной дашборд, чтобы понять, что именно за запросы сейчас идут в большом количестве и откуда, потенциально, они могут исходить. Если на обзорном дашборде имеются сервисы, которые потенциально могут создавать нагрузку для данного api сервиса, то стоит обратить внимание, на сколько интенсивно они работают на данный момент.
* время ответа всех api сервисов, включая совершенно несвязанных, сильно выросло - необходимо проверить работу сетевой инфраструктуры включая балансировку.
