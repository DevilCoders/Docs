# Простой вариант перекладывания из kafka в clickhouse 
## О числах
По оценке у нас сотни тысяч событий в день, т.е десятки в секунду.<br />
Думаю что ydb с любым шардированием справиться с такой нагрузкой.<br />
Кликхаус любит большие батчи (10.000 - 100.000), записывать в него стоит не чаще чем раз в секунду.<br />
Выглядит что эти числа вполне ложатся на обработку данных одним потоком.
## Наивный вариант, что если всё поместится в один поток?
![naive](naive_v2.png)

Это отказоустойчивая схема. Все наши процессы будут запущены в тасках. Т.е если таск упадёт или если закроют датацентр,
взамен старого таска поднимется другой таск и продолжит его работу.

## Дизайн с учётом таблиц

```
таблица для дедупликации
deduplication
| uniqueId | created_at | proto | schema_version |
```
```
таблица new_events, с новыми событиями, которые ещё не отправили в clickhouse
| uniqueId | proto | schema_version |
```
```
таблица для отправки батчей и ретраев в случае сетевых ошибок
batches
| batch | batchId | status|
```
### Запись новых событий
Таблица `deduplication` и таблица с `new_events` - разные таблицы, т.к они отвечают
за разное и в будущем к ним могут быть разные требования по нагрузке.<br />
Запись в `deduplication` и `new_events`.<br />
Начнём с такого алгоритма:<br/>
1. Читаем событие из кафки
2. Генерим `uniqueId`, генерим по proto и возможно учитываем schema.version.
3. Селектим по нему из deduplication, если событие уже есть, на этом останавливаемся. Значит оно уже отправлено в кликхаус.
4. Делаем insert в транзакции в `deduplication` и `new_events`
Если insert начнёт тормозить, можно будет отойти от транзакционной записи.
   Начать писать отдельно в таблицу `deduplication`, затем отдельно в `new_events` и если запись в `new_events` сфейлится,
   позже таском допушивать записи из `deduplication` в `new_events`.
   
Таблицу `deduplication` можно сразу шардировать с нормальным распеределением. YDB само решает на сколько шардов и как разбивать базу.
Для этого первый элемент ключа должен быть числом. Можно добавить поле shardId и считать его как хэш от uniqueId.
Все вопросы к эволюции схемы, как и раньше останутся на uniqueId.
   
### Формирование батчей<br />
Кликхаус дедуплицирует батч, если два батча одного размера, содержат одинаковые строки и эти строки в том же порядке.
`batches.batch` это некое представление батча, по которому легко восстановить запрос в кафку. Возможно сюда можно записать сам запрос в кафку.
Главное свойство - преобразование `batch` в запрос на запись в кафку должно быть однозначным и не должно меняться.<br/>
Алгоритм генерации батча:
1. Селект из `new_events`. Из YDB можно вытащить 1.000 записей за один запрос, можно их же инсёртить в clickhouse.
2. Генерим batchId(можно uuid), генерим `batches.batch`
3. Записываем batch в таблицу `batches` и в транзакции удаляем элементы из таблицы `new_events`.
Напоминаю, что здесь работает таск, т.е строго один процесс.
Минорный вариант - можно не удалять из `new_events`, а сделать поле `batch_id` и проставлять id батча.
   Если понадобится разбираться в каком-то баге, возможно так будет проще.   

### Пуш батчей в кликхаус<br />
В кликхаусе нет транзакций, но есть гарантия, что запись батча произойдёт 1 раз.
Чтобы кликхаус узнал батч - он должен быть ровно того же размера с теми же строками в том же порядке.<br/>
Кейс когда нужен ретрай записи - Запись в кликхаус может быть успешной, но ответ об успехе сфейлится и не прийти по сети.<br/>
Статусов NEW, IN_CLICKHOUSE достаточно. Для удобства можно добавить и другие (RETRY/IN_PROCESS).
1. Генератор батчей создаёт батч со статусом NEW
2. Таск отправляющий батчи берёт батч со статусом NEW
3. Отправляет в кликхаус
4. Если получил ОК - записывает статус IN_CLICKHOUSE (альтернатива - не используем статусы, удаляем из таблицы batches)
   Если не ок и это сетевая ошибка - можно и сразу поретраить (но не обязательно).
5. Сфейлились мы или нет на любом из этапов, в следующий раз начинаем с начала

### Таск удаления по TTL из `deduplication`<br/>
`deduplication.created_at` - Время создания. Для того чтобы не хранить сообщения вечно. Нужно задать адекватное окно, чтобы в этом окне события были уникальны.
Думаю что пары месяцев тут достаточно. Сколько нужно хранить - вопрос. ttl будет задан в таске.
Начать можно и без этой таски, посмотреть в будущем, нужно ли нам дропать старые данные.
`created_at` начнём записывать сразу, т.к иначе в будущем, даже если мы захотим мы не сможем отличить старые данные от новых.<br />

### Эволюция proto-схемы и схемы таблицы clickhouse
![Эволюция схемы](schema_and_evolution.md)

### Правила дедупликации. Куда поместить, у тех кто генерит события, у нас?
Мы посмотрели конкретно на event-ы, которые будем писать. Поняли что ключ для дедупликации простой - project, event_type, product_id.
И система у на самом деле у нас сильно связана.
Producer-ы должны заполнять сообщения одинаково, чтобы можно было использовать одинаковые запросы
в кликхаус в конце пайплайна. Значит и в середине пайплайна можно использовать одинаковые правила
для всех.

## Альтернативные идеи, и почему их не выбрал:
### Другие разбиения на таблицы
```
таблица для дедупликации, удаления по ttl и для формирования батчей
events
| uniqueId | created_at | proto | schema_version | batchId |
```
```
таблица для отправки батчей и ретраев в случае сетевых ошибок
batches
| batch | batchId | status|
```
- Таблица для дедупликации и таблица с новыми записями, два в одном.
  Почему нет - от таблицу с новыми записями может понадобится шардировать отдельно от таблицы дедупликации.
  К ним разные требования, если смотреть на то для чего они нужны - они нужны для разного.
  
- Таблица в которой сохраняем батч не нужна.
  Мы можем проставлять id и селектить с order by и batchId = xxx.
  Почему нет - код даже самых простых правил может поменяться. Это легко может привести к багу.<br />
  Батч с данными для отправки в кликхаус лучше держать иммутабельным и не собирать его заново.

### Генерация id
- id батча можно сразу писать, во время записи события в базу
  Почему нет:
  Если мы будем сразу записывать id батча, то между записью и таском, который генерит батчи
  будут race-conditions. Их можно решать с помощью синхронизации между ними, но схема станет сложной.<br />
  Как можно было бы без race condition и синхронизации?
  Пишем в момент записи, когда записали достаточно, в ещё одну таблицу про батчи пишем, что появился новый батч.
  Почему нет? Нормально, но плюсов не вижу. И схема сложнее.
- Записывать порядковый номер элемента при записи события в базу
  Если бы мы каждый раз генерили батч для повторной отправки, это имело бы смысл.<br/>
  а) порядковый id элемента можно писать как сквозной возрастающий long<br/>
  По кол-ву данных long превосходит наши данные на порядки, его хватит навсегда.
  Даже если у нас станет гораздо больше данных, главное чтобы в одном батче не было одинаковых id.
  Можно начинать отсчёт с начала, если дошли до конца.<br/>
  б) для порядкового id элемента брать оффсет из кафки<br/>
  Нужно брать оффсет + partitionId. Не нравится то что мы смешиваем области ответственности.
  
- Думал можно ли не хранить сами элементы для их дедупликации?<br />
  Хранить только уникальные id.
  Чем может быть плохо - если мы захотим добавить больше шардов в базу или изменить правила генерации id, мы не сможем их перегенерить.
  Или использовать математику, чтобы по batchId решать - входил ли в него id или нет.</br>
  Мысль не довёл до конца - сложно, под вопросом возможно или нет. Могут быть причины хранить полные данные.
- Альтернативы того где должны быть правила дедупликации
  Где должны быть правила дедупликации?<br/>
  - Генерим уникальные id на стороне producer-ов.<br/>
    Один проблемный producer может испортить данные всем.
  - У нас.
    Сложность в том, что изменения схемы потенциально ведут к изменению кода, который генерит уникальный id.
    А этого хотелось бы избежать.
  - Заложить в схему<br/>
    С помощью аннотаций указывать какие поля участвую в ключе, а какие нет.
    Может быть удобно, т.к правила приезжают вместе с новой схемой.<br/>
    Как это реализовать учитывая что у схемы может быть несколько версий - не продумал.
  - Гибридно. Часть ключа определяем мы, уникальность в этом множестве определяется ключом от producer-a<br/>
    Почему этот вариант имеет смысл быть - мы проверяем уникальность по сервису, типу события. <br />
    Это простые правила, которые не должны меняться со временем.<br/>
    Остальное - на стороне сервиса.<br/>
    Чем плохо - лог общий и вероятно правила для него должны быть общими и producer-ам придётся у себя их повторять.

