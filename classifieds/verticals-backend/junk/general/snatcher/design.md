# SNATCHER - сервис переноса профилей
Под профилем подразумевается список объявлений пользователя.

## Решаемые задачи
- Хранение информации о публичных профилях пользователей сторонних классифайдов
- Перенос объявлений пользователей сторонних классифайдов на Яндекс.Объявления с согласия пользователей.

### Хранение профилей
Профили периодически вычитываются из таблицы в YT и сохраняются в базу сервиса.
Храним следующую информацию:
 - Ссылка на профиль - для передачи в Scraping hub с целью парсинга (вместе с хешом профиля берётся из YT)
 - Хэш профиля - для идентификации профиля. На данный момент предполагаем, что юзер знает только свой hash и не угадывает другой (но это не точно!)
 - Статус обработки профиля (описаны далее)
 - Id пользователя на Яндекс.Объявлениях, после залогина пользователя. Храним, чтобы взаимодействовать с сервисом фидов, который знает только про паспортных пользователей, а так же чтобы валидировать, что один юзер не попробует перенести два разных профиля объявлений.

### Общая логика обработки профиля
1. Коллцентр уговаривает пользователя перенести свои офферы извне (авито и т.д.) в Я.Объявления.```
2. После согласования пользователю приходит СМС со ссылкой на лендинг, в которой зашит хеш пользователя (тот самый, который мы берём из YT)
3. При открытии лендинга фронт редиректит пользователя в паспорт для авторизации.
4. После авторизации, пользователь редиректится обратно на наш лендинг. Используя хеш юзера, зашитый в ссылке, и uid, фронт делает запрос на бекенд для проверки, что указанный профиль ранее не переносился, а так же, что юзер не имеет других перенесенных фидов.
5. Если юзер уже имеет перенесенный профиль или профиль не найден в базе - отдаем ошибку. Если для данного юзера этот профиль уже обрабатывается или обработан - сообщаем об этом. Иначе на лендинге есть кнопка переноса объявлений.
6. Юзер нажимает на лендинге кнопку инициации переноса профиля
7. Фронт отправляет нам запрос о старте переноса профиля с хэшом пользователя и его идентификатором на сервисе объявлений.
8. Сохраняем идентификатор пользователя в профиль в базе, если у нас уже нет профиля с таким идентификатором, иначе возвращаем ошибку и профиль не парсим.
9. Отправляем в Scraping hub таску на парсинг профиля юзера
10. После отправки периодически поллим облако, в которое должен прийти результат
11. Получаем json-результат и конвертируем его в фид формата соответствующего классифайда. (Иногда, если произошла ошибка на стороне Scraping hub, то json не придёт никогда. В таких случаях мы сами через некоторое время должны решить, что произошла ошибка и зафейлить загрузку и, соответстенно, пропустить шаги 12-13)
12. Отдаем фид на обработку в сервис фидов.
13. После обработки сервис фидов приходит в нашу ручку.
14. Нотифицируем пользователя про результат обработки.

Статусы обработки профиля
- `new` - профиль ни разу не был успешно обработан и не обрабатывается в данный момент
- `waits_parsed` - ожидается результат парсинга профиля от scraping hub
- `sent_to_feeds` - для профиля создан фид и отправлен в сервис фидов на обработку
- `processed` - профиль был обработан, объявления из него размещены на Яндекс.Объявлениях

Более подробно различные шаги описаны в соответствующих компонентах.
Пока что планируется одноразовый перенос профиля, но в будущем можем захотеть периодически обновлять перенесенный профиль.

## Компоненты

### API
Grpc API для управления процессом переноса профиля

#### Получение статуса профиля по hash-у юзера и его SellerId
Возвращаем статус профиля, чтобы фронт мог провалидировать юзера, стучащегося в лендинг, или сообщить информацию об обработке профиля
1. Идем в таблицу `profiles`, пытаемся найти профиль пользователя по хэшу, либо по sellerId.
2. Проверяем, что существует профиль, за которым не закреплен юзер, или закреплен данный юзер.
3. Проверяем, что за данным sellerId не закреплен другой профиль
4. В ответе отдаем модель с одним из следующих результатов
    NEW - если найдет профиль, который до этого не обрабатывался (в статусе `new`), и не имеет владельца, либо закреплен за данным sellerId
    NOT_FOUND - если профиль не найден
    FAILED - если профиль закреплен за другим sellerId, или если за данным sellerId закреплены другие профили
    PROCESSING - если профиль закреплен за данным sellerId и обрабатывается в данный момент
    PROCESSED - если профиль закреплен за данным sellerId и успешно перенесен


#### Инициация начала обработки профиля по hash-у юзера и его sellerId
Посылаем задачу на парсинг профиля в Scraping hub для юзера с данным hash-ом
1. Получаем в таблице `profiles` профиль юзера по hash-у, убеждаемся, что он ещё не обрабатывался.
2. Также проверяем, что в базе нет профиля с пришедшим sellerId, иначе возвращаем ошибку
3. Кладем в s3 для Scraping hub csv со ссылкой на профиль юзера
4. Сохраняем в таблице `parsed` информацию про парсинг профиля пользователя (время начала и хэш), а в таблицу `profiles` сохраняем sellerId пользователя и меняем статус профиля на `waits_parsed`

#### Получение результата парсинга профиля от фидов
Получаем результат обработки Parsed-фида от фидов. Запрос содержит в себе sellerId и статус обработки.
1. Если обработка завершена успешно, меняем в таблице `profiles` статус профиля на `processed`, иначе меняем статус на `new`
2. Нотифицируем пользователя про результат обработки // TODO Как?

### Scheduler
Компонент, отвечающий за периодические процессы.

#### Сохранение профилей из yt в базу
Периодический процесс, который пополняет базу профилями из выгрузки в yt.
Данные в таблицу заносит [Андрей Попов](https://staff.yandex-team.ru/popov-av). В случае чего стоит идти к нему.
Предполагаем, что в таблице каждый профиль встречается лишь раз, и ссылка на профиль верна.
//TODO добавить ссылку на таблицу в yt, когда точно будет известно, что за табличка.

1. Запрашиваем из таблицы в yt данные по профилям пользователей
2. Добавляем в таблицу `profiles` профили, которых в ней нет, со статусом `new`

#### Поллинг Scruping hub
1. Достаем из таблицы `parsed` хэши пользователей (они все в данный момент должны обрабатываться Scraping hub). // Тут я подразумеваю, что таблица будет маленькой. Иначе имеет смысл ввести время последней проверки профиля, и поллить scraping hub насчет профиля раз в 10 минут, например.
2. Берем из s3 Scraping hub-а успешно спаршенные json-ы профилей для хешей, которые ранее получили
3. Для каждого успешно обработанного профиля конвертируем json с профилем в фид формата классифайда, с которого переносятся объявления
    // TODO как конвертируем? Отсутствие каких полей нефатально и допустимо?
4. При фатальных ошибках конвертации нотифицируем о невозможности переноса // TODO как? Подумать над механизмом обработки таких случаев`
5. После успешной конвертации отправляем полученные фиды в сервис фидов
6. Для каждого профиля удаляем запись из таблицы `parsed`. Для успешно конвертированных ставим статус `sent_to_feeds`, для остальных `new` в таблице `profiles`

На данный момент определять успешность парсинга профиля можно только по тому, появился файл в s3 с результатами или нет.
Предлагается запустить параллельный с описанным выше процесс, который для профилей, слишком долго висящих в обработке Scraping hub будет наводить суету (логи, метрики?)



## Хранилище
Храним данные в postgres

`profiles` - таблица с общей информацией о профилях пользователей
- `hash: Utf8` - hash от ссылки на профиль пользователя
- `profile_url: Utf8` - ссылка на профиль пользователя
- `status: Uint32` - идентификатор статуса обработки профиля пользователя, один из `new`, `waits_parsed`, `sent_to_feeds`, `processed`. Не храним enum, чтобы не ломать схему при неаккуратном добавлении нового enum-а.
- `seller_id: Utf8?` - соответствующий идентификатор пользователя в Яндекс.Объявлениях
`PK = (hash)`
    Индексы:
    - sellers: `(seller_id)` - для поиска профилей по sellerId

`parsed` - таблица с профилями, которые в данный момент парсятся Scraping hub
- `hash: Utf8` - hash пользователя
- `started_at: Uint64` - таймстемп загрузки профиля на обработку, в миллисекундах
`PK = (hash)`
    Индексы:
    - started_at_idx: `(started_at)` - для поиска профилей, которые слишком долго не парсятся // не уверен, что тут нужен индекс, предполагаю, что таблица будет не особо большой
