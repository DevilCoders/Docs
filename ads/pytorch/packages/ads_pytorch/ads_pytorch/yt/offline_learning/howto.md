# BatchPyTorch

Пакет решает следующую задачу: предоставляет интерфейс для определения модели, которую надо выучить на статической табличке с определенным числом эпох/итераций. После обучения надо применить модель к некоторым валидационным таблицам.

Реализованы следующие фичи:
1. Снапшоты: раз в N секунд модель сохраняет свое состояние в YT. При падениях и перезапусках модель возьмет свое состояние оттуда
2. Сохранение модели и кеширование: если модель выучилась, то при повторном перезапуске ничего не произойдет
3. Стримовое чтение из YT: поддержаны таблицы любых объемов
4. Поколоночное чтение из YT: можно читать только часть из проваренных фичей, если хочется выучить более легкую модель

# Как пользоваться
## model_yt_dir

Первой и самой важной штукой является **директория модели**. Считается, что каждой модели однозначно соответствует своя директория. В этой директории хранятся все артефакты модели:
* Скрипт ```./script.py``` и конфиг модели ```./model_config.json```, чтобы можно было воспроизвести эксперимент
* Директория с финальной моделью (```./model```). Заполняется, только когда модель закончила обучение. На эту директорию навешивается специальный атрибут, говорящийЮ что модель в данной директории финишировала. Если вдруг случится повторный запуск, модель просто возьмется из кеша и ничего не произойдет
* Директория со снапшотом ```./snapshot```. Если модель финишировала, то директория удалится

Кеширование модели как в пайплайне, так и в самой обучалке реализовано через атрибут на директории модели.

Любые дополнительные артефакты должны складываться в YT в директорию с моделью

Директория модели задается в top-level конфига:
```json
{
    "model_yt_dir": "//home/ads/users/alxmopo3ov/chatbot_benchmark/DEBUG/fake4"
}
```

## URI Callback

Второй очень важной концепцией в принципе в ads_pytorch является URI и URI callback. Весь поток входных данных представляется в виде потока URI. В конце обработки каждого URI вызывается набор callback'ов - штук, которые принимают на вход модель, оптимизатор, loss и URI и которые реализуют дополнительную логику для обучения. Все фичи ниже (за исключением чтения обучающих данных) - это callback'и.

Лучше дробить свою таблицу на достаточно небольшие чанки так, чтобы временные callback'и типа снапшоттера часто запускались и часто проверяли свои условия. Параметр гранулярности разбиения стоит крутить только если callback'и запускаются слишком часто или, наоборот, слишком редко.

## Snapshotter

Снапшотер включен всегда. Надо только выставить параметр frequency - частота (в секундах), с которой модель сохраняет снапшот. Не стоит делать слишком частой - сейчас джобы в Нирване, как правило, редко вытесняются.

```json
{
  "snapshotter": {
    "frequency": 3600
  }
}
```

## Train data

Настройка скачивания обучающей таблицы и числа итераций.

* table - путь к таблице
* epoch_count - сколько эпох хочется сделать. Одна эпоха - это проход по датасету.
* max_iter - максимальное число обработанных батчей. Сначала считается максимальное число, равное epoch_count * batch_count,  потом берется min(max_iter, epoch_count * batch_count). Существует исключительно во имя обратной совместимости с DSSM
* features - какие тензоры надо достать из таблицы и засунуть в фичи
* targets - какие тензоры надо достать из таблицы и засунуть в таргеты

Большая часть параметров подбирается автоматически. Вот так выглядит минимальный json-конфиг

```json
{
  "train_data": {
    "epoch_count": 10,
    "table": "//home/ads/users/alxmopo3ov/chatbot_benchmark/train_parsed_with_idx_mb_200",
    "features": [
      "context_0_parsed_0",
      "context_0_parsed_1",
      "context_0_parsed_2",
      "reply_parsed_0",
      "reply_parsed_1",
      "reply_parsed_2"
    ],
    "targets": []
  }
}
```

Также есть параметры, которые подбираются автоматически (если не указать их явно) и которые вам вряд ли захочется крутить:

* uri_granularity - на сколько батчей дробить URI. СМ URI Callback
* num_downloaders - число параллельных соединений с YT для загрузки данных
* num_parsers - число потоков на парсинг данных (3 практически всегда более чем достаточно)
* max_cache_size - размер буфера для стримовой читалки. После заполнения буфера скачивание прекращается до момента, когда модель обработает скачанные таблицы и освободит место в буффере

```json
{
  "train_data": {
    "epoch_count": 10,
    "table": "//home/ads/users/alxmopo3ov/chatbot_benchmark/train_parsed_with_idx_mb_200",
    "max_iter": 100,
    "uri_granularity": 3000,
    "features": [
      "context_0_parsed_0",
      "context_0_parsed_1",
      "context_0_parsed_2",
      "reply_parsed_0",
      "reply_parsed_1",
      "reply_parsed_2"
    ],
    "targets": [],
    "num_downloaders": 6,
    "num_parsers": 3,
    "max_cache_size": 536870912
  }
}
```

Фичи, указанные в features, поедут в inputs в MinibatchRecord. targets - соответственно, в таргеты. Подробнее нужно почитать в доке how_to_create_models.

## EntryPoint

```bash
--entry_point ads_pytorch.yt.offline_learning.entry_point
```
