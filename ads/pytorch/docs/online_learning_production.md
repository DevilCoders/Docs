<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [Оглавление](#%D0%BE%D0%B3%D0%BB%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5)
- [Аркадия](#%D0%B0%D1%80%D0%BA%D0%B0%D0%B4%D0%B8%D1%8F)
- [Про логи](#%D0%BF%D1%80%D0%BE-%D0%BB%D0%BE%D0%B3%D0%B8)
- [Архитектура](#%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0)
  - [Общие положения](#%D0%BE%D0%B1%D1%89%D0%B8%D0%B5-%D0%BF%D0%BE%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F)
  - [model_yt_dir <a name="model_yt_dir"></a>](#model_yt_dir-a-namemodel_yt_dira)
  - [model_package <a name="model_package"></a>](#model_package-a-namemodel_packagea)
  - [model_state <a name="model_state"></a>](#model_state-a-namemodel_statea)
  - [Схема запуска обучения](#%D1%81%D1%85%D0%B5%D0%BC%D0%B0-%D0%B7%D0%B0%D0%BF%D1%83%D1%81%D0%BA%D0%B0-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)
    - [Обучение](#%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5)
    - [train_lock <a name="train_lock"></a>](#train_lock-a-nametrain_locka)
    - [control_lock <a name="control_lock"></a>](#control_lock-a-namecontrol_locka)
  - [Публикация артефактов <a name="artifacts"></a>](#%D0%BF%D1%83%D0%B1%D0%BB%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D0%B0%D1%80%D1%82%D0%B5%D1%84%D0%B0%D0%BA%D1%82%D0%BE%D0%B2-a-nameartifactsa)
    - [Branches <a name="branches"></a>](#branches-a-namebranchesa)
      - [Branch <a name="branch"></a>](#branch-a-namebrancha)
      - [Смена бранчей](#%D1%81%D0%BC%D0%B5%D0%BD%D0%B0-%D0%B1%D1%80%D0%B0%D0%BD%D1%87%D0%B5%D0%B9)
  - [Мониторинги и алерты <a name="monitoring"></a>](#%D0%BC%D0%BE%D0%BD%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%BD%D0%B3%D0%B8-%D0%B8-%D0%B0%D0%BB%D0%B5%D1%80%D1%82%D1%8B-a-namemonitoringa)
    - [Виды сенсоров](#%D0%B2%D0%B8%D0%B4%D1%8B-%D1%81%D0%B5%D0%BD%D1%81%D0%BE%D1%80%D0%BE%D0%B2)
    - [Дашборды метрик](#%D0%B4%D0%B0%D1%88%D0%B1%D0%BE%D1%80%D0%B4%D1%8B-%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA)
  - [Алерты](#%D0%B0%D0%BB%D0%B5%D1%80%D1%82%D1%8B)
    - [Мониторинги на отставание модели](#%D0%BC%D0%BE%D0%BD%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%BD%D0%B3%D0%B8-%D0%BD%D0%B0-%D0%BE%D1%82%D1%81%D1%82%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8)
    - [Мониторинги на качество модели](#%D0%BC%D0%BE%D0%BD%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%BD%D0%B3%D0%B8-%D0%BD%D0%B0-%D0%BA%D0%B0%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%BE-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8)
- [Как запускать](#%D0%BA%D0%B0%D0%BA-%D0%B7%D0%B0%D0%BF%D1%83%D1%81%D0%BA%D0%B0%D1%82%D1%8C)
  - [Как собрать model_package <a name="build_model_package"></a>](#%D0%BA%D0%B0%D0%BA-%D1%81%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D1%8C-model_package-a-namebuild_model_packagea)
  - [train_ctl <a name="train_ctl"></a>](#train_ctl-a-nametrain_ctla)
    - [Важные общие аргументы](#%D0%B2%D0%B0%D0%B6%D0%BD%D1%8B%D0%B5-%D0%BE%D0%B1%D1%89%D0%B8%D0%B5-%D0%B0%D1%80%D0%B3%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B)
      - [```--model_package``` <a name="train_ctl_model_package"></a>](#--model_package-a-nametrain_ctl_model_packagea)
      - [```--model_state``` <a name="train_ctl_model_state"></a>](#--model_state-a-nametrain_ctl_model_statea)
      - [```--model_yt_dir``` <a name="train_ctl_model_yt_dir"></a>](#--model_yt_dir-a-nametrain_ctl_model_yt_dira)
    - [Команды](#%D0%BA%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D1%8B)
      - [create_model_yt_dir <a name="train_ctl_create_model_yt_dir"></a>](#create_model_yt_dir-a-nametrain_ctl_create_model_yt_dira)
      - [start <a name="train_ctl_start"></a>](#start-a-nametrain_ctl_starta)
      - [retry_on_failure <a name="train_ctl_retry_on_failure"></a>](#retry_on_failure-a-nametrain_ctl_retry_on_failurea)
      - [change_state <a name="train_ctl_change_state"></a>](#change_state-a-nametrain_ctl_change_statea)
      - [stop <a name="train_ctl_stop"></a>](#stop-a-nametrain_ctl_stopa)
  - [Запуск модели <a name="how_to_launch_model"></a>](#%D0%B7%D0%B0%D0%BF%D1%83%D1%81%D0%BA-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-a-namehow_to_launch_modela)
- [HOW-TO: релизы, отладка, дежурство...](#how-to-%D1%80%D0%B5%D0%BB%D0%B8%D0%B7%D1%8B-%D0%BE%D1%82%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0-%D0%B4%D0%B5%D0%B6%D1%83%D1%80%D1%81%D1%82%D0%B2%D0%BE)
  - [Как запустить модель](#%D0%BA%D0%B0%D0%BA-%D0%B7%D0%B0%D0%BF%D1%83%D1%81%D1%82%D0%B8%D1%82%D1%8C-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C)
  - [Где смотреть на мониторинги](#%D0%B3%D0%B4%D0%B5-%D1%81%D0%BC%D0%BE%D1%82%D1%80%D0%B5%D1%82%D1%8C-%D0%BD%D0%B0-%D0%BC%D0%BE%D0%BD%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%BD%D0%B3%D0%B8)
  - [Как быстро доучить модель до текущей даты <a name="how_to_fast_reach_curtime"></a>](#%D0%BA%D0%B0%D0%BA-%D0%B1%D1%8B%D1%81%D1%82%D1%80%D0%BE-%D0%B4%D0%BE%D1%83%D1%87%D0%B8%D1%82%D1%8C-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D0%B4%D0%BE-%D1%82%D0%B5%D0%BA%D1%83%D1%89%D0%B5%D0%B9-%D0%B4%D0%B0%D1%82%D1%8B-a-namehow_to_fast_reach_curtimea)
  - [Как посмотреть историю релизов модели и/или смены model_state <a name="how_to_release_history"></a>](#%D0%BA%D0%B0%D0%BA-%D0%BF%D0%BE%D1%81%D0%BC%D0%BE%D1%82%D1%80%D0%B5%D1%82%D1%8C-%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D1%8E-%D1%80%D0%B5%D0%BB%D0%B8%D0%B7%D0%BE%D0%B2-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-%D0%B8%D0%B8%D0%BB%D0%B8-%D1%81%D0%BC%D0%B5%D0%BD%D1%8B-model_state-a-namehow_to_release_historya)
  - [Как выкатить релиз <a name="how_to_release_model_package"></a>](#%D0%BA%D0%B0%D0%BA-%D0%B2%D1%8B%D0%BA%D0%B0%D1%82%D0%B8%D1%82%D1%8C-%D1%80%D0%B5%D0%BB%D0%B8%D0%B7-a-namehow_to_release_model_packagea)
    - [Выкатка релиза, ломающего обратную совместимость с предыдущим model_state](#%D0%B2%D1%8B%D0%BA%D0%B0%D1%82%D0%BA%D0%B0-%D1%80%D0%B5%D0%BB%D0%B8%D0%B7%D0%B0-%D0%BB%D0%BE%D0%BC%D0%B0%D1%8E%D1%89%D0%B5%D0%B3%D0%BE-%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D1%83%D1%8E-%D1%81%D0%BE%D0%B2%D0%BC%D0%B5%D1%81%D1%82%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D1%8C-%D1%81-%D0%BF%D1%80%D0%B5%D0%B4%D1%8B%D0%B4%D1%83%D1%89%D0%B8%D0%BC-model_state)
    - [Подстрока слома обратной совместимости <a name="break_backward_compatibility_string"></a>](#%D0%BF%D0%BE%D0%B4%D1%81%D1%82%D1%80%D0%BE%D0%BA%D0%B0-%D1%81%D0%BB%D0%BE%D0%BC%D0%B0-%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B9-%D1%81%D0%BE%D0%B2%D0%BC%D0%B5%D1%81%D1%82%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D0%B8-a-namebreak_backward_compatibility_stringa)
  - [Как откатить модель на некоторый снапшот параметров модели <a name="how_to_rollback_model_state"></a>](#%D0%BA%D0%B0%D0%BA-%D0%BE%D1%82%D0%BA%D0%B0%D1%82%D0%B8%D1%82%D1%8C-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D0%BD%D0%B0-%D0%BD%D0%B5%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D1%8B%D0%B9-%D1%81%D0%BD%D0%B0%D0%BF%D1%88%D0%BE%D1%82-%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-a-namehow_to_rollback_model_statea)
  - [Как починить баг и выкатить релиз, чтобы модель смогла переучиться за некоторый промежуток времени](#%D0%BA%D0%B0%D0%BA-%D0%BF%D0%BE%D1%87%D0%B8%D0%BD%D0%B8%D1%82%D1%8C-%D0%B1%D0%B0%D0%B3-%D0%B8-%D0%B2%D1%8B%D0%BA%D0%B0%D1%82%D0%B8%D1%82%D1%8C-%D1%80%D0%B5%D0%BB%D0%B8%D0%B7-%D1%87%D1%82%D0%BE%D0%B1%D1%8B-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D1%81%D0%BC%D0%BE%D0%B3%D0%BB%D0%B0-%D0%BF%D0%B5%D1%80%D0%B5%D1%83%D1%87%D0%B8%D1%82%D1%8C%D1%81%D1%8F-%D0%B7%D0%B0-%D0%BD%D0%B5%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D1%8B%D0%B9-%D0%BF%D1%80%D0%BE%D0%BC%D0%B5%D0%B6%D1%83%D1%82%D0%BE%D0%BA-%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%B8)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# Оглавление
to be filled, но разработка этой документации в том числе велась в тикете https://st.yandex-team.ru/TORCHPS-264, при нехватке подробностей можно попросить досыпать этих подробностей в тикете. Также вот здесь есть быстрый мониторинг среднего значения царей.
Текущий нейросетевой царь там обозначен как TsarResult4 https://grafana.yandex-team.ru/d/jexID3BZk/fast_lm_tsar_dashboard?orgId=1&from=now-7d&to=now

Дашборд с отставаниями в графане https://grafana.yandex-team.ru/d/pytorch-online-tsar

# Аркадия
директория с описанием модели в аркадии https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/production_models/tsar_ffn_v2.1


# Про логи
* таски в логосе - посмотреть через yt операции https://yt.yandex-team.ru/hahn/operations?filter=TsarPytorchTask&
* код таски в логосе https://a.yandex-team.ru/arc_vcs/logos/projects/ads/tasks/tsar_pytorch_log
* посмотреть триггеры на таски в реакторе https://reactor.yandex-team.ru/reaction?id=11806454&reactionId=3056984&mode=triggers только надо выбрать последний релиз в пути /logos/graphs/ads/prod/release ...
* логи на yt https://yt.yandex-team.ru/hahn/navigation?sort=asc-false,field-name&path=//home/bs/logs/TsarPyTorchTensorLog/1h&
* логи by design бывают с дырками, потому что таска варки лога требует много CPU, и проще отстрелить варку нескольких старых логов, чем догнать всю очередь. Плюс это быстрее дает свежесть.



# Архитектура
## Общие положения
Перед началом работы стоит усвоить несколько простых архитектурных "столпов", вокруг которых все строится.

Эти столпы:
1. Каждая модель однозначно идентифицируется путем в YT. Это директория модели - **model_yt_dir**
2. **model_yt_dir** содержит в себе абсолютно всю информацию о модели. Если нужно что-то узнать о модели, то надо в первую очередь искать информацию в ее директории
3. Результаты (до)обучения модели называются [артефактами](#artifacts). Артефакты - это ноды в YT (файлы, таблицы, директории - неважно), публикуются **только** на YT в директорию модели. Забирать потребителям их нужно тоже из директории модели.

## model_yt_dir <a name="model_yt_dir"></a>
FIXME: Филипп, перенеси модель в //home/ads/tzar/pytorch_online_models
UPD 2020-03-16 модель сейчас находится в папке https://yt.yandex-team.ru/hahn/navigation?sort=asc-false,field-modification_time&path=//home/ads/robot-online-pytorch/new_production_online_learning_tsar_ffn_2&
Вот пример директории модели //home/ads/users/ya-philya/new_production_online_learning_tsar_ffn_2

В ней есть:
* nirvana_stuff - файл с информацией о Nirvana ресурсах для этой модели (в данном случае - просто workflow_guid, где учится модель)
* sandbox_stuff - файл с информацией о sandbox ресурсах и шедулерах для этой модели
* artifacts_processor_info - файл с информацией обо всех потребителях артефактов модели
* train_lock, control_lock - системные файлы, использующийся как mutex'ы, контролируют эксклюзивность того или иного действия
* branches - папочки, где содержатся артефакты модели и стейт модели для дообучения

Все, кроме train_lock, control_lock и branches, опционально и выполняет исключительно функцию хранения информации о модели, чтобы директория была единой точкой входа для поиска чего-либо.

## model_package <a name="model_package"></a>

Файл, содержащий **все** необходимое для запуска обучения модели. Имея этот файл, всегда можно запустить обучение.

Поскольку пакет ads_pytorch сейчас не собирается через аркадийный ya.make (но лежит и возится через Аркадию), то сейчас model_package описывается некоторым ```package.json``` и собирается командой ```ya package```. Там лежат:
* Скрипт модели
* Конфиг модели
* Все неаркадийно собранные пакеты. Несмотря на наличие возможности добавлять сюда произвольные пакеты, по-хорошему, такой пакет должен быть единственный: собственно, сам ads_pytorch. Все остальное должно возиться через контейнеры
* Контейнеры: список ресурсов в sandbox, в которых лежат porto слои

В светлом будущем, когда pytorch завезут в аркадию, необходимость ```ya package``` пропадет и [model_package](#model_package) станет просто аркадийным бинарем.

Чтобы переезд был безболезненным, везде, где только, возможно, [model_package](#model_package) - это просто файлик, который надо куда-то переложить. При заезде в аркадию надо будет сделать новые реализации запуска модели (или другого кода) из model_package в соответствующих абстракциях, весь остальной код менять не нужно.

О том, где лежит  [model_package](#model_package), подробно рассказано в [описании бранчей](#branches)

## model_state <a name="model_state"></a>

Снапшот модели. Изначально получается предобучением модели. Хранит в себе веса модели плюс некоторый файлик с метаинформацией (к примеру, какая последняя дата логов была обработана).

О том, где лежит  [model_state](#model_state), подробно рассказано в [описании бранчей](#branches)

## Схема запуска обучения
### Обучение

Онлайн модель - это бинарь, который непрерывно где-то крутится и poll'ит появление новых логов. Такая схема позволяет масштабироваться для обработки довольно мелких логов, не тратя время на рестарты модели, чтение модели из снапшота и т.д.

Как правило, над бинарем, который непрерывно крутится, есть еще и крон, который смотрит, не упал ли бинарь, и перезапускает его. Кронов может быть несколько на разных сервисах и т.д., они могут не синхронизироваться друг с другом и совершить одновременный дублирующий запуск. Чтобы таких запусков не было, в [model_yt_dir](#model_yt_dir) есть файл [train_lock](#train_lock).

**ВАЖНО**: вся работа с [train_lock](#train_lock) и [control_lock](#control_lock) ведется как с мьютексами. В терминах YT это означает взятие **exclusive lock** под транзакцией. Здесь и далее под "взять train/control lock" понимается именно **exclusive_lock**.

### train_lock <a name="train_lock"></a>

Схема с локом следующая:
1. При запуске модели первое, что происходит - попытка взять лок
2. Если попытка успешная, то мы запускаем обучение
3. Если лок уже кем-то взят, то выходим и ничего не делаем

**Сейчас** запуск модели - это просто запуск обучения прямо внутри того же запущенного бинарника. В данной схеме под "запуском обучения" может пониматься что угодно - вплоть до запуска какой-то пачки реплик модели, синхронизированных каким-то способом.

[train_lock](#train_lock) используется не только для того, чтобы синхронизировать запуски модели - он используется еще и **для остановки обучения**. Бинарь держит лок под транзакцией и часто пингует ее. При необходимости убить обучение, нужно убить эту транзакцию, тогда бинарь автоматически все остановит и завершит работу. Чтобы не давать обучению стартануть заново, достаточно взять лок самому. В это время можно что-нибудь сделать с директорией.

Главный плюс такой реализации убийства обучения - абстрагированность от того, где запускается модель. Nirvana, YP, YT Vanilla Operation и т.д. - все это будет работать.

**Сейчас** обучение запускается в Нирване в некотором workflow. Его GUID можно посмотреть в файлике nirvana_stuff в директории модели.

### control_lock <a name="control_lock"></a>

Бывает, что с моделью (а следовательно, и с директорией модели) нужно что-нибудь сделать - например, выкатить релиз, рестартануть обучение, удалить что-нибудь, проставить флажок и т.д. и т.п. Часто такие действия делаются автоматическими бинарями в тех же кронах, обладающих всеми теми же недостатками: возможность дублирующих запусков, возможность наличия нескольких кронов и т.д.

Для решения этой проблемы сделан [control_lock](#control_lock). Любой бинарь, который может делать какие-то действия с моделью, должен сначала взять control_lock. Будет он пытаться взять эксклюзивный доступ, отстрелить других владельцев лока или нет - это все уже вариантивно. **Сейчас** все бинари равноправны: они пытаются взять лок, если он кем-то уже взят - завершают работу либо встают в очередь на лок.

**Почему [control_lock](#control_lock) != [train_lock](#train_lock)?** Во-первых, потому что у train_lock есть еще функция отстреливания обучения в случае аборта транзакции, не хочется (а может, и просто невозможно) смешать всю эту логику в один лок. Во-вторых, не всегда доступ какого-то крона бинаря к директории должен отстреливать обучение.

Если бинарю нужен совсем-совсем эксклюзивный доступ, то алгоритм такой:
1. Берется train_lock
2. В цикле отстреливаются все транзакции, держащие train_lock, и одновременно делается попытка взять train_lock
3. train_lock взят - делаем, что хотим делать

## Публикация артефактов <a name="artifacts"></a>

Напомним, что артефакт - это любой результат обучения модели. Все артефакты публикуются в YT в директорию модели, откуда они уже забираются сторонними потребителями. Есть простое разграничение ответственности за артефакты:
* Бинарь обучения только **публикует** новые артефакты. Он **никогда** ничего не удаляет
* Любые потребители **читают и/или удаляют** артефакты. Они **никогда** сами не досыпают новые артефакты в директорию. Как они при этом друг с дружкой синхронизируются - это уже их дело

Сейчас публикация артефактов реализована через схему бранчей

### Branches <a name="branches"></a>
#### Branch <a name="branch"></a>
Ветка - это **текущая рабочая директория модели**. Бинарь обучения пишет **исключительно** в эту директорию и никуда больше.

Структура ветки следующая:
- branch
  - model_state
  - [model_package](#model_package)
  - [artifacts](#artifacts)
    - art_name1
    - art_name2
    - art_name3
    - ...

* model_state - текущий снапшот параметров модели
* model_package - бинарь с моделью, который был тогда запущен и порождал артефакты и снапшот модели
* artifacts - директория с любыми артефактами. Если хочется опубликовать новый артефакт, нужно создать здесь подпапочку и начинать писать туда какие-нибудь ноды

Для нод внутри директории артефакта какого-то вида (art_name1, к примеру) имена выбираются автоматически: это просто числа. Позволяет сразу понять, в каком порядке они были опубликованы. Если нужна метаинформация - пишите в атрибуты ноды. Чтобы получить следующее имя артефакта, если специальный код (об этом ниже).

На тип нод артефактов не накладывается никаких ограничений: это может быть все, что YT позволяет хранить в Кипарисе.

#### Смена бранчей
В течение работы бинаря все понятно: вот директория, туда все публикуется, оттуда все забирается. Что делать, если что-то пошло не так?
1. Обучение умерло и рестартовало
2. После смерти обучения директория осталась в некорректном состоянии (к примеру, где-то в коде что-то пишется не под общей транзакцией)
3. Надо выкатить релиз, делающий какие-то изменения в публикуемых артефактах (возможно, обратно-несовместимые)
4. Надо выкатить релиз, добавляющий что-то в саму модель и меняющий (возможно, обратно-несовместимо) ее state (**ОЧЕНЬ** плохо, но в жизни всякое бывает)
5. Надо откатить модель на предыдущее состояние, потому что модель училась неправильно (например, битые логи были, потом их пофиксили)

Задачи 3-5, по сути, решают задачу версионирования (причем полноценного). В YT есть реализации версионирования, но они все про работы с транзакциями.

**Золотое правило** очень простое: **любая ветвь используется ровно один раз**.

* На старте бинарь обучения смотрит, была ли использована ранее эта ветвь (с т.з. реализации смотрит атрибут в YT)
* Если уже была использована - отстреливается с ошибкой. Ждет, пока внешний крон, ответственный за перезапуск модели, не создаст корректную свежую ветвь
* Если не была использована - то помечает ее как активную и начинает работу. Пометка, как и любое действие бинаря обучения, совершается после успешного взятия лока

Потребители артефакта имеют код, который позволяет определить текущий активный бранч. Более новые бранчи имеют публиковать артефакты, обработанные с более старыми логами, чем предыдущий активный бранч, потому что можно откатывать состояния модели.

## Мониторинги и алерты <a name="monitoring"></a>
В качестве бекенда для метрик используется Solomon. Метрики пишутся изнутри кода обучения, никакие артефакты ниоткуда забирать не нужно. Мы считаем, что любой мониторинг, который не работает прямо непосредственно из кода, ненадежен и может не отражать полную картину.

Для запуска любой модели в прод **обязательно** указывать секрет Solomon, без мониторингов запустить новую модель в прод нельзя. После запуска модели, метрики начинают писаться из коробки.

Шард для записи метрик в соломон можно настраивать. По дефолту - это https://solomon.yandex-team.ru/?project=ads-pytorch-online-prod&cluster=pclick&service=models

* project = ads-pytorch-online-prod
* cluster = pclick
* service = models

### Виды сенсоров
В Solomon все сенсоры имеют метки (обязательная - собственно, имя сенсора). В ads_pytorch у каждого сенсора минимум 3 метки:
* sensor - имя сенсора
* model_yt_dir - позволяет определить, к какой модели относится этот сенсор
* type - группа сенсоров

Есть следующие обязательные типы (type) сенсоров:
* lag - сенсоры отставания обучения. На них настроены важные алерты об отставании модели
* system - системные сенсоры, логгируют метрики работы разнообразных деталей реализации обучения
* progress - то, что обучение пишет в лог при обработке батчей. Сюда входят метрики модели, лоссы, метрики скорости обработки батчей и т.д.

Пользователи вольны добавлять любые собственные сенсоры в модель (в том числе и новые type). Запрещено писать свои сенсоры в ```type = lag```.

### Дашборды метрик
В соломоне есть собственные рисовалки графиков, но они, как правило, не слишком удобны. Сейчас предлагается использовать графит. Готовый вариант дашборда можно подглядеть, например, [тут](https://grafana.yandex-team.ru/d/VPG2cklZkd/home-ads-users-ya-philya-new_production_online_learning_tsar_ffn_2-pytorch-online-dashboard?orgId=1) (это старая версия дашборда, которая была, когда модель еще лежала у него в хомяке в //home/ads), и [тут](https://grafana.yandex-team.ru/d/pytorch-online-tsar/pytorch-online-tsar-dashboard-home-ads-robot-online-pytorch-new_production_online_learning_tsar_ffn_2?orgId=1) (это новая версия дашборда после переноса модельки в https://yt.yandex-team.ru/hahn/navigation?sort=asc-false,field-modification_time&path=//home/ads/robot-online-pytorch/new_production_online_learning_tsar_ffn_2&).

Для создания дашборда для новой модели нужно скачать json этого (или любого другого) дашборда и подменить там везде model_yt_dir. Автоматики для этого сейчас нет :(

## Алерты

### Мониторинги на отставание модели

Мониторингов на отставание модели два: ```LagByTimestamp```  и ```ProcessingTimeByTimestamp```. Оба имеют ```type = lag```. ByTimestamp означает, что эти метрики пишутся раз в N секунд (по умолчанию 15) в background корутине.
* ```LagByTimestamp``` - это отставание модели. Вычисляется как разность ```datetime.now()``` и даты последнего обработанного лога
* ```ProcessingTimeByTimestamp``` - это время обработки текущего лога. Если модель ничего не учит (нет входных логов) - пишутся нули. Если идет обработка лога (обучение + callback'и) - пишется время, прошедшее с момента приезда этого лога на обучение (в это время входит время вызова callback'ов с ```call_before_train=True```)

Алерты для них следующие:
* Алерт на значение ```LagByTimestamp```. Это, собственно, чистое отставание модели. Триггерит, если отставание модели больше какого-то порога. Загорается ```ALARM```
* Алерт на отсутствие метрик для ```LagByTimestamp``` в течение какого-то времени (сейчас это один час). Загорается ```NO DATA```
* Алерт на значение ```ProcessingTimeByTimestamp```. Должно быть не больше, чем timedelta между логами. Сейчас это один час. Загорается ```ALARM```

Если загорается ```NO DATA```, то тут два варианта:
1. Обучение упало и не может перезапуститься
2. Обучение зависло (deadlock/sleep(100500)/etc.)
3. Умерла background корутина, пишущая метрики

Все три случая необходимо оперативно чинить.

### Мониторинги на качество модели
TBD

# Как запускать

Чтобы запустить торчовую модель и/или что-то сделать с обучением текущей модели, создан специальный бинарь [train_ctl (ads/pytorch/lib/online_learning/production/train_ctl)](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/lib/online_learning/production/train_ctl). Он рассматривается как единая точка входа для любых операций с продакшен моделью и/или ее директорией. Для отладки нового кода может быть полезным умение запускать обучение модели локально.

## Как собрать [model_package](#model_package) <a name="build_model_package"></a>
Перед запуском любой модели в прод нужно иметь собранный пакет модели.

Любая модель, запущенная в продакшен, должна лежать в аркадии. Сейчас мы складываем модели вот [сюда](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/production_models). Сейчас, поскольку ads_pytorch не собирается через ya.make, все собирается и катается через ```ya.package```:
* ```script.py``` - это код модели
* ```config.json``` - конфиг модели
* ```package.json``` - описание пакета для ya.package

Про то, как писать код модели и как заполнять конфиг, читайте отдельно в [TBD заполнить](None)

```package.json``` имеет строго определенную структуру (которая прямо сейчас валидируется только уже в проде после запуска FIXME):
* ```/script.py``` - скрипт модели
* ```/additional_files/``` - дополнительные файлы модели. Аналог files для запуске торча. Здесь именем файла выступает его ключ
* ```/packages/``` - python пакеты для модели, не собираемые через ya.make, но лежащие в Аркадии. При заезде в аркадию эти пакеты просто будут аркадизированы.
* ```/containers/``` - список porto layers для модели
* ```script_wrapper.py``` - legacy поле, всегда должен быть равен "ads/pytorch/packages/ads_pytorch/ads_pytorch/nirvana/wrapper.py". Отпилим в ближайшее время.

После того, как составлен package.json, а моделька закоммичена в аркадию, ее надо собрать. Ее можно собрать следующими способами:
* Локально с помощью команды ```ya package package.json```
* В Sandbox с помощью таски YA_PACKAGE

Теперь о "хорошем" и "плохом". Катать локально собранную модель в боевой продакшен с непонятно какой аркадией - **плохо**. Это допускается только в случае каких-то срочно фиксов. Также бывает полезно, если вы любитель подебажить код прямо на живом продакшене, все как мы любим, а сборка через Sandbox работает слишком долго. Но после починки с помощью локальной сборки необходимо собрать пакет через Sandbox со слепка Аркадии и со всеми своими вкоммиченными правками и перевыкатить релиз через [train_ctl change_state](#train_ctl_change_state).

Модели, которые не ездят ни в какой транспорт, можно катать как угодно. Это могут быть тестовые модели, которые тестируют поведение какой-нибудь фичи в той же Нирване.

## train_ctl <a name="train_ctl"></a>

train_ctl - это бинарь, у которого есть набор команд и аргументы для этих команд. Команды бывают такие:
* [create_model_yt_dir](#create_model_yt_dir) - создать новую продакшен директорию модели
* start - запустить модель для уже созданной директории модели
* retry_on_failure - посмотреть, обучается ли модель (=взят ли [train_lock](#train_lock)) и перезапустить, если нужно
* change_state - поменять state или package модели
* stop - остановить продакшен обучение модели
* continue - продолжить обучение после команды stop (FIXME бесполезная команда, надо выпилить и все через start делать)

Большая часть этих команд шарит некоторые типы аргументов, остановимся на них подробнее

### Важные общие аргументы
#### ```--model_package``` <a name="train_ctl_model_package"></a>

Путь к [model_package](#model_package). Формируется как строка вида ```тип_пути:путь```. Типов пути 3:
* local - локальный файл
* yt - путь к файлу в YT
* sandbox - айдишник ресурса в sandbox

Золотое правило:
* Релизы и новые модели катаются через сборку [model_package](#model_package) в Sandbox с чистого слепка Аркадии определенной ревизии
* Откаты на предыдущие версии делаются на model_package в YT в предыдущих бранчах, там, где они лежат рядышком с соответствующим стейтом модели

#### ```--model_state``` <a name="train_ctl_model_state"></a>

Путь к стейту модели. Это всегда путь к модели в YT. Он может как содержать файл с метаинформацией о продакшен обучении, так и не содержать (это если мы катим новую модель из стейта, полученного на обучении по историческим данным)

#### ```--model_yt_dir``` <a name="train_ctl_model_yt_dir"></a>

Путь к директории модели в YT

### Команды
#### create_model_yt_dir <a name="train_ctl_create_model_yt_dir"></a>

Создает новую директорию модели. Имеет два обязательных аргумента:
* [--model_package](#train_ctl_model_package)
* [--model_state](#train_ctl_model_state)

Здесь нет аргумента [model_yt_dir](#train_ctl_model_yt_dir), поскольку директория берется из [model_package](#model_package)

#### start <a name="train_ctl_start"></a>

Запускает обучение модели. **ВАЖНО**: сейчас должен быть запущен только при первом запуске на свежесозданной директории модели от команды [create_model_yt_dir](#train_ctl_create_model_yt_dir).

Сейчас делает ровно одно действие: запускает шедулер в Sandbox с бинарем ```train_ctl``` и командой [retry_on_failure](#train_ctl_retry_on_failure). Перед запуском валидирует, что командная строка, переданная в свежесозданный шедулер, корректно распарсится текущим парсером ```train_ctl```.

У этой команды тьма аргументов, но их можно легко разбить на группы по названию. Главный аргумент - [model_yt_dir](#train_ctl_model_yt_dir) - путь к директории модели в YT. Названия остальных аргументов построены по принципу ```<имя_сервиса>_<сущность>```.  Примеры:
* nirvana_solomon_token_secret: Первое - nirvana - говорит о том, что это сущность Нирваны. Остаток говорит о том, что это имя секрета с токеном в Solomon
* sandbox_task_owner: Первое - sandbox - говорит о том, что это сущность Sandbox. Остаток - task_owner - говорит о том, что это владелец таска

Ну и, конечно, у всех аргументов есть help в бинаре.

#### retry_on_failure <a name="train_ctl_retry_on_failure"></a>

Перезапуск обучения, если все попадало. Сейчас выполняет роль крона для бинаря обучения, train_ctl бинарь с командой ```retry_on_failure``` крутится в Sandbox шедулере. Для каждой модели шедулер свой. Айдишник шедулера написан в файлике ```sandbox_stuff``` в [model_yt_dir](#model_yt_dir).

Сейчас обучение запускается в Нирване, в одном и том же workflow_guid, в разных инстансах. workflow_guid можно посмотреть в файлике ```nirvana_stuff``` в [model_yt_dir](#model_yt_dir).

1. Берет control_lock
2. Смотрит, взят ли train_lock. Если взят - выход
3. Если нет, то читает содержимое train_lock. Если там написана команда STOP - выход
4. Берет train_lock
5. Создает новый [branch](#branch). [model_state](#model_state) и [model_package](#model_package) берутся из предыдущего активного бранча, чтобы продолжить с последней точки
6. Перезапускает обучение
7. Отпускает train_lock
8. Отпускает control_lock

#### change_state <a name="train_ctl_change_state"></a>

Остановить обучение и перезапустить его с подмененными [model_state](#model_state) и/или [model_package](#model_package). Выкатка релизов, равно как и откат на предыдущие версии, делаются этой командой.

У команды 4 аргумента: model_yt_dir, model_state, model_package и reason. model_state и model_package опциональны. Если они указаны, то команда вместе соответствующего model_state или model_package предыдущей ветки подсунет туда указанную сущность. Можно не указывать оба аргумента - тогда, по сути, просто отстрелится обучение, при ретрае оно восстановится с предыдущей точки. Такое бывает полезно для отладки чего-либо.

1. Берет control_lock
2. Отстреливает обучение и сам берет train_lock
3. Создает новый бранч. По дефолту, [model_state](#model_state) и [model_package](#model_package) берутся из предыдущего бранча. Если пользователь указал один из аргументов, то подменяет на сущности из аргументов.
4. Отпускает train_lock
5. Отпускает control_lock

#### stop <a name="train_ctl_stop"></a>

Останавливает обучение и не дает ему заново восстановиться.

1. Берет control_lock
2. Отстреливает обучение и сам берет train_lock
3. Пишет в train_lock команду STOP
4. Отпускает train_lock
5. Отпускает control_lock

## Запуск модели <a name="how_to_launch_model"></a>
1. Собираем [model_package](#build_model_package)
2. Создаем директорию модели командой [train_ctl create_model_yt_dir](#train_ctl_create_model_yt_dir)
3. Запускаем модель на обучение командой [train_ctl start](#train_ctl_start)

# HOW-TO: релизы, отладка, дежурство...

## Как запустить модель
См [запуск модели](#how_to_launch_model)

Запуски в prod надо делать от имени робота robot-online-pytorch, перед этим собрав model_package через Sandbox из trunk.

Примеры [команд](https://paste.yandex-team.ru/9655839) для запуска через train_ctl (автор: [Роберт Дрынкин](staff.yandex-team.ru/robdrynkin)).
В качестве `YT_TOKEN`, `SANDBOX_TOKEN`, `SOLOMON_TOKEN` надо использовать [секреты](https://yav.yandex-team.ru/secret/sec-01ey5w02w74tcbt2y32x8eefh8/explore/versions) робота. Доступ на чтение может дать Роберт или обратитесь к кому-то, у кого уже есть доступ на чтение.

Notes:
* сейчас в train_ctl есть [хардкод](https://a.yandex-team.ru/arcadia/ads/pytorch/lib/online_learning/production/train_ctl/lib/start.py?rev=r9530848#L89) на ads-pytorch-online-prod в Solomon. Поэтому опция `--without_solomon` у `train_ctl start` не особо работает.
* в примере команд нужно вместо `--nirvana_project` писать `--project`

Выполнение `train_ctl start` создаст 3 scheduler на Sandbox:
1. [retry_on_failure](https://sandbox.yandex-team.ru/scheduler/712400/view)
2. [backup collector](https://sandbox.yandex-team.ru/scheduler/712401/view)
3. [garbage collector](https://sandbox.yandex-team.ru/scheduler/712402/view)

После запуска регулярного обучения **проверьте глазами**, что:
1. в `{model_yt_dir}/branches` модели нормально пишутся и учатся, с ожидаемой регулярностью
2. в Nirvana ничего не падает. (или падает и перезапускается, но по "нормальным" причинам). В `{model_yt_dir}/nirvana_stuff` лежит `workflow_guid`, скопируйте его и откройте `https://nirvana.yandex-team.ru/flow/{workflow_guid}`.
3. в Solomon пишутся метрики ([пример](https://solomon.yandex-team.ru/?project=ads-pytorch-online-prod&cluster=pclick&service=models&model_yt_dir=%2F%2Fhome%2Fads%2Frobot-online-pytorch%2Fmultitarget_model_trainable&sensor=Loss&graph=auto&b=1d&e=)). Если ваш loss в `script.py` возвращает одно значение, то loss запишется в sensor `Loss`, если возвращает `dict`, то у каждого ключа свой sensor.

Например, может быть такое, что в квоте кончилось GPU, из-за чего обучение будет постоянно перезапускаться и создавать пустые branch'и.

## Выгрузка модели в C++ формат в ML Storage
Для inference'а надо конвертировать модель в C++ формат и сохранить в [ML Storage](https://logos.yandex-team.ru/docs/ml_logos/storage/about), откуда её заберут потребители. Для этого есть [deploy_v2_processor](https://a.yandex-team.ru/arcadia/ads/pytorch/lib/online_learning/production/processors/deploy_v2_processor/__main__.py), который делает следующее:
1. Скачивает дамп модели из YT  из папки `ideployable_model`.
2. Конвертирует модель в C++ формат согласно `deploy_config.json` ([пример](https://a.yandex-team.ru/arcadia/ads/pytorch/production_models/multitarget_model_trainable/deploy_config.json?rev=ba3b936c8d82f7269dcb580017babb61c4b51d5a#L110)). Для каждой хэш-таблицы можно задать своё сжатие. Если ваша модель получается большая, можно зайти в `ideployable_model/{banner, user, whatever}`, отсортировать таблицы по размеру и для самых больших задать сжатие `"half"`.
3. Валидирует качество модели:
   1. Проверяет, что C++ и Python модель предсказывают одинаково с точностью до `eps`. Для этого в папке модели лежит `validator_predictions`.
   2. Проверяет, что модель имеет разумные метрики и loss в нормальных пределах (см. секция `validatio_rules` в `deploy_config.json`).
4. Загружает модель в C++ формате на Sandbox и создаёт новый ресурс в ML Storage.

Сейчас шедулер для `deploy_v2_processor` запускается отдельно от `train_ctl`. После запуска регулярного обучения скопируйте [этот](https://sandbox.yandex-team.ru/scheduler/704274/view) шедулер, задайте своё описание, путь к `deploy_config.json` в `config_path`, свои `--model_yt_dir` и `--ml_storage_key` в `cmd_template`.

## Где смотреть на мониторинги
См [мониторинги](#monitoring)

## Как быстро доучить модель до текущей даты <a name="how_to_fast_reach_curtime"></a>
В продакшен обучении реализован важный механизм [force skip](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/ads_pytorch/ads_pytorch/online_learning/production/uri.py?rev=5845018#L29). В коде обучения модели сидит "шедулер", отдающий данные на обучение. Если он видит, что накопилась огромная очередь на обработку, он включает этот флаг. При включении этого флага **отключается вся тяжелая обработка артефактов модели**. Модель просто старается максимально быстро догнаться до текущей даты, пропуская всякие транспорты и т.д. Такая механика бывает полезна в первую очередь при починке критичных багов обучения, когда надо дать модели достаточно данных на "переучивание".

## Как посмотреть историю релизов модели и/или смены model_state <a name="how_to_release_history"></a>
В [model_yt_dir](#model_yt_dir) есть файл ```branch_history```. Там пишется история смены [бранчей](#branches). ```retry_on_failure``` - это шедулер, который смотрит за рестартами модели. Любые другие причины - это ручные действия по релизам/откатам модели.

## Как выкатить релиз <a name="how_to_release_model_package"></a>
1. Делаем необходимые правки и коммитим их
2. Собираем [model_package](#build_model_package) **в Sandbox**. Получаем айдишник ресурса (```<resource_id>```)
3. Запускаем [train_ctl change_state](#train_ctl_change_state) с аргументом```--model_package sandbox:<resource_id>```. В ```--reason``` пишем, что делаем релиз такой-то ревизии.

### Выкатка релиза, ломающего обратную совместимость с предыдущим [model_state](#model_state)
**ТАК ДЕЛАТЬ ОЧЕНЬ ПЛОХО**. Но если все-таки есть крайняя необходимость, то надо, чтобы в ```--reason``` была подстрока [слома обратной совместимости](#break_backward_compatibility_string). Это нужно, чтобы какое-то время спустя дежурные при необходимости [откатить модель](#how_to_rollback_model_state) знали, что нужно взять еще и правильную версию кода.

### Подстрока слома обратной совместимости <a name="break_backward_compatibility_string"></a>
```BREAK_BACKWARD_COMPATIBILITY```

## Как откатить модель на некоторый [снапшот параметров модели](#model_state) <a name="how_to_rollback_model_state"></a>
Смотрим [историю релизов](#how_to_release_history) и ищем там [строку слома совместимости](#break_backward_compatibility_string).

Если ее нет, то запускаем [train_ctl change_state](#train_ctl_change_state) с аргументом ```--model_state```.

Если она есть и релиз был **после** снапшота, на который вы откатываетесь, то необходимо взять соответствующий данному снапшоту [model_package](#model_package) и запустить [train_ctl change_state](#train_ctl_change_state), указав **оба** аргумента ```--model_package``` и ```--model_state```.

## Как починить баг и выкатить релиз, чтобы модель смогла переучиться за некоторый промежуток времени
Порядок действий тот же, что в [выкатке релиза](#how_to_release_model_package), только теперь еще надо указать снапшот модели в аргументе ```--model_state``` из бекапа за нужную вам дату. Также нужно проверить, что этот бекап будет [совместим](#how_to_rollback_model_state). Для этого достаточно запустить [train_ctl change_state](#train_ctl_change_state), указав оба аргумента ```--model_state``` и ```--model_package```.

При откате можно не бояться давать достаточно большие промежутки на дообучение, см [догонка модели](#how_to_fast_reach_curtime)
