<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [–ß—Ç–æ —Ç–∞–∫–æ–µ ads_pytorch –∏ –∫–æ–º—É –æ–Ω–∞ –Ω—É–∂–Ω–∞](#%D1%87%D1%82%D0%BE-%D1%82%D0%B0%D0%BA%D0%BE%D0%B5-ads_pytorch-%D0%B8-%D0%BA%D0%BE%D0%BC%D1%83-%D0%BE%D0%BD%D0%B0-%D0%BD%D1%83%D0%B6%D0%BD%D0%B0)
- [–†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏](#%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-%D1%81-%D1%8D%D0%BC%D0%B1%D0%B5%D0%B4%D0%B4%D0%B8%D0%BD%D0%B3%D0%B0%D0%BC%D0%B8)
  - [HashEmbedding](#hashembedding)
    - [–§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö](#%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%82-%D0%B2%D1%85%D0%BE%D0%B4%D0%BD%D1%8B%D1%85-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)
    - [compute_mode](#compute_mode)
    - [–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤](#%D0%BF%D0%BE%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D1%8D%D0%BC%D0%B1%D0%B5%D0%B4%D0%B4%D0%B8%D0%BD%D0%B3%D0%BE%D0%B2)
    - [–û–±–æ–±—â–µ–Ω–Ω—ã–π –∫–æ–¥ –ª—é–±—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ HashEmbedding](#%D0%BE%D0%B1%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9-%D0%BA%D0%BE%D0%B4-%D0%BB%D1%8E%D0%B1%D1%8B%D1%85-%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D0%B9-%D0%B2%D0%BD%D1%83%D1%82%D1%80%D0%B8-hashembedding)
  - [BaseEmbeddingModel –∏ IDeployableModel](#baseembeddingmodel-%D0%B8-ideployablemodel)
- [–§–∞–±—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π](#%D1%84%D0%B0%D0%B1%D1%80%D0%B8%D0%BA%D0%B8-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9)
  - [BaseEmbeddingModel json](#baseembeddingmodel-json)
  - [SubNetworks](#subnetworks)
    - [–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –ø–æ—Å—Ç—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä](#%D1%80%D0%B5%D0%BA%D1%83%D1%80%D1%81%D0%B8%D0%B2%D0%BD%D0%BE%D0%B5-%D0%BF%D0%BE%D1%81%D1%82%D1%80%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%B8%D1%85-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80)
    - [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–±—Ä–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä](#%D0%BA%D0%BE%D0%BD%D1%84%D0%B8%D0%B3%D1%83%D1%80%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D1%84%D0%B0%D0%B1%D1%80%D0%B8%D0%BA-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80-%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80)
    - [–ë–∏–±–ª–∏–æ—Ç–µ—á–Ω—ã–µ —Å–ª–æ–∏](#%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D1%87%D0%BD%D1%8B%D0%B5-%D1%81%D0%BB%D0%BE%D0%B8)
      - [GenericFeedForwardWithNormalizers](#genericfeedforwardwithnormalizers)
      - [GenericFeedForwardWithHeads](#genericfeedforwardwithheads)
      - [MultisequenceTransformerEncoder2](#multisequencetransformerencoder2)
        - [key_mask](#key_mask)
        - [–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã TransformerEncoder](#%D0%B2%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5-%D1%82%D0%B8%D0%BF%D1%8B-transformerencoder)
          - [torch](#torch)
          - [ads_pytorch](#ads_pytorch)
          - [–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π TransformerEncoder](#%D1%80%D0%B5%D0%BA%D0%BE%D0%BC%D0%B5%D0%BD%D0%B4%D1%83%D0%B5%D0%BC%D1%8B%D0%B9-transformerencoder)
        - [–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ multisequence_tranformer_input_sequence_builder](#%D0%B2%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5-%D1%82%D0%B8%D0%BF%D1%8B-%D0%BF%D0%BE%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B9-%D0%BF%D0%BE%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B5%D0%B9-%D0%B2-multisequence_tranformer_input_sequence_builder)
          - [plain_sum](#plain_sum)
          - ["densenet"](#densenet)
          - [–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏](#%D1%80%D0%B5%D0%BA%D0%BE%D0%BC%D0%B5%D0%BD%D0%B4%D1%83%D0%B5%D0%BC%D1%8B%D0%B5-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B8)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

**–í–ê–ñ–ù–û:** –¥–∞–Ω–Ω—ã–π tutorial –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –≤—ã —É–∂–µ –∑–Ω–∞–µ—Ç–µ, —á—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –∏–º–µ–µ—Ç–µ –±–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö –∏ –Ω–∞–∏–±–æ–ª–µ–µ —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –∑–Ω–∞–∫–æ–º—ã —Å —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–º PyTorch, –∑–Ω–∞–∫–æ–º—ã —Å –∞–∑–∞–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

{{toc}}

# –ß—Ç–æ —Ç–∞–∫–æ–µ ads_pytorch –∏ –∫–æ–º—É –æ–Ω–∞ –Ω—É–∂–Ω–∞

ads_pytorch - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞-–Ω–∞–¥—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞–¥ PyTorch –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—Ü–µ–ª - **—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏ —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**. –í —Ü–µ–ª–æ–º, –Ω–∏–∫–∞–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–µ–π –Ω–µ—Ç, –º–æ–∂–Ω–æ —É—á–∏—Ç—å –∏ CV, –∏ BERT, –∏ —Ç.–¥., –Ω–æ –ø—Ä–∏–º–µ—Ä–Ω–æ –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Ñ–∏—à–µ—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –∏–º–µ–Ω–Ω–æ –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

–ü–æ–¥ —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –º—ã –ø–æ–Ω–∏–º–∞–µ–º —Å–ª–∞–±–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –Ω–µ –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ–º –Ω–∏–∫–∞–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ —Ç–æ, –∫–∞–∫ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –¢–∞–º –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —á–µ–≥–æ-—Ç–æ, —Ç–µ–∫—Å—Ç—ã, —Å–ø–∏—Å–∫–∏ –∞–π–¥–∏—à–Ω–∏–∫–æ–≤, –∏–ª–∏ –¥–∞–∂–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤.

–ü—Ä–∏–º–µ—Ä—ã (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏):
1. –ê–π–¥–∏—à–Ω–∏–∫ —Ç–æ–≤–∞—Ä–∞, –∞–π–¥–∏—à–Ω–∏–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∞–π–¥–∏—à–Ω–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è —Ç–æ–≤–∞—Ä–æ–≤ (one-hot –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏)
2. –î–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞, —Ü–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–æ –º–∞–≥–∞–∑–∏–Ω–∞ (float —Ñ–∏—á–∞)
3. –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (multi-value –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Ñ–∏—á–∞)
4. –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Ç–æ–≤–∞—Ä–∞ (float –º–∞—Ç—Ä–∏—Ü–∞)
5. –°—á–µ—Ç—á–∏–∫ —Ç–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–π–¥–∏—à–Ω–∏–∫–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–π–¥–∏—à–Ω–∏–∫–∞ –µ—Å—Ç—å –µ—â–µ float —Å—á–µ—Ç—á–∏–∫–∏ –≤–∏–¥–∞ "–∫–∞–∫ –¥–∞–≤–Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ —Å–º–æ—Ç—Ä–µ–ª, —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ —Ä–∞–∑ —Å–º–æ—Ç—Ä–µ–ª –∏ —Ç.–¥." (–º–Ω–æ–∂–µ—Å—Ç–≤–æ categorical –∫–ª—é—á–µ–π + float –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞)
6. –ü–æ—Å–ª–µ–¥–Ω–∏–µ N –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ø–æ–∏—Å–∫–µ –ø–æ —Ç–æ–≤–∞—Ä–∞–º - –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –∏ timestamp'–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
7. to be continued...

–í –±–æ–ª—å—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –µ—Å—Ç—å –≤—Å–µ —ç—Ç–∏ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥—Ä—É–≥–∏—Ö. –°–µ–π—á–∞—Å –≤ –º–∏—Ä–µ –≥–æ—Å–ø–æ–¥—Å—Ç–≤—É–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—è **end-to-end –æ–±—É—á–µ–Ω–∏—è**: –∑–∞—Å—É–Ω—É—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–æ–º –≤ –æ–¥–Ω—É –º–æ–¥–µ–ª—å –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –º–æ–¥–µ–ª–∏ —Å–∞–º–æ–π –≤—ã—É—á–∏–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. ads_pytorch - —ç—Ç–æ, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏ –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã–º–∏ —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤.

–í —Ç–µ—á–µ–Ω–∏–µ tutorial –º—ã –±—É–¥–µ–º –ø—Ä–æ—Ö–æ–¥–∏—Ç—å—Å—è –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –ø–æ–ø—É—Ç–Ω–æ —Ä–µ—à–∞—Ç—å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É. –î–æ–≥–æ–≤–æ—Ä–∏–º—Å—è —Å—Ä–∞–∑—É –æ –≤–µ—Ä—Ö–Ω–µ—É—Ä–æ–≤–Ω–µ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: —ç—Ç–æ –±—É–¥–µ—Ç DSSM-like –º–æ–¥–µ–ª—å, –¥–ª—è —Ç–æ–≤–∞—Ä–∞ –∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±—É–¥–µ—Ç —Å–≤–æ—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–∞–∂–¥–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≤—ã–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –±—É–¥–µ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ç–æ–≤–∞—Ä–∞ `ùõº * u^Ti + b`, –≥–¥–µ:
* u - –≤–µ–∫—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (user)
* i - –≤–µ–∫—Ç–æ—Ä —Ç–æ–≤–∞—Ä–∞ (item)
* b - –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ —Å–¥–≤–∏–≥–∞ (–æ–Ω –∂–µ bias)
* ùõº - –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–æ—Ä –ø—Ä–∏ —Å–∫–∞–ª—è—Ä–Ω–æ–º –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏, —á–∏—Å–ª–æ

# –†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
–†–∞–±–æ—Ç–∞ —Å –ª—é–±—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏ –∏–∑ –∫–æ—Ä–æ–±–∫–∏ - —ç—Ç–æ, –Ω–∞–≤–µ—Ä–Ω–æ–µ, –≥–ª–∞–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞–¥ –≤—Å–µ–º–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –≤ —Ç.—á. –Ω–∞–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –±—É—Å—Ç–∏–Ω–≥–æ–º. CatBoost –ø—ã—Ç–∞–ª—Å—è —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –æ–¥–Ω–∞–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π (—Å–∫–∞–∂–µ–º, 30-40 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π) —Ç–∞–º —Ä–∞–±–æ—Ç–∞—é—Ç –ø–ª–æ—Ö–æ. –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ —Ä–∞–∑ —Å–ª–∞–≤—è—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ–º –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π, –∏ –º—ã —Å–¥–µ–ª–∞–ª–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—Ä–∞–±–æ—Ç–∫–∏.

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö –ø—Ä–∏–º–µ—Ä–Ω–æ —Å–ª–µ–¥—É—é—â–∏–π:
1. **–í–∞—Ä–∏–º —Å–ª–æ–≤–∞—Ä—å**: –ø—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É, —Å–æ–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π –∏ –∏—Ö –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å, —É–¥–∞–ª—è–µ–º —á–∞—Å—Ç—å –∫–ª—é—á–µ–π –∏ —Ç.–¥., –æ—Å—Ç–∞–≤–ª—è–µ–º N —Ñ–∏—á–µ–π –≤ —Å–ª–æ–≤–∞—Ä–µ. –ò–Ω–æ–≥–¥–∞ –≤ —ç—Ç–æ—Ç –∂–µ –ø—É–Ω–∫—Ç –≤—Ö–æ–¥–∏—Ç –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Ç–µ–∫—Å—Ç–∞ –≤ –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–π, –Ω–æ –º—ã –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è, –Ω–∞–ø—Ä—è–º—É—é –Ω–µ —Å–≤—è–∑–∞–Ω–Ω–∞—è —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é, –∏ –≤—Å–µ –∫–∞—Ç —Ñ–∏—á–∏/—Ç–µ–∫—Å—Ç—ã —É –Ω–∞—Å - –∫–ª—é—á–∏.
2. –ù—É–º–µ—Ä—É–µ–º –∫–ª—é—á–∏ –Ω–∞—à–∏—Ö —Ñ–∏—á–µ–π –æ—Ç 0 –¥–æ N-1
3. –°–æ–∑–¥–∞–µ–º –ø–ª–æ—Ç–Ω—ã–π –¥–≤—É–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ —Ä–∞–∑–º–µ—Ä–∞ N x D, –≥–¥–µ D - —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞, —Ñ–∏–∫—Å–∏—Ä—É–µ–º –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–π –≤ —Å–ª–æ–≤–∞—Ä–µ
4. –§–∏–∫—Å–∏—Ä—É–µ–º –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–π –≤ —Å–ª–æ–≤–∞—Ä–µ, –¥–æ–æ–±—É—á–∞–µ–º –¥–ª—è –Ω–∏—Ö –≤–µ—Å–∞. **–ù–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–π –≤—Å—Ç–∞–≤–∫–∏ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏—è –∫–ª—é—á–∞**

–¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –Ω–µ–ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –±–æ–ª–µ–µ-–º–µ–Ω–µ–µ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö NLP –∑–∞–¥–∞—á, –≥–¥–µ –µ—Å—Ç—å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Ñ–∏—á–∞ - —Ç–µ–∫—Å—Ç —Å BPE —Å–ª–æ–≤–∞—Ä–µ–º, —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π –∑–∞—Ä–∞–Ω–µ–µ –ø–æ –æ–≥—Ä–æ–º–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –û–¥–Ω–∞–∫–æ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è—é—â–∏—Ö—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ç–µ–∫—É—â–µ–π —Ä–µ–∫–ª–∞–º–Ω–æ–π –º–æ–¥–µ–ª–∏ —É–∂–µ 130 –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á (TODO —Å—Å—ã–ª–∫–∞). –í —Ç–∞–∫–æ–º —Å–µ—Ç—Ç–∏–Ω–≥–µ –ø–æ–¥—Ö–æ–¥ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ç—Ä–µ—â–∞—Ç—å –ø–æ –≤—Å–µ–º —Ñ—Ä–æ–Ω—Ç–∞–º:
* **–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**. –í —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –º–æ–¥–µ–ª–∏ –æ–±—ã—á–Ω–æ –¥–æ–æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Å–≤–µ–∂–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –í –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ –∫–ª—é—á–∏ –¥–ª—è —É–∂–µ –∏–º–µ—é—â–∏—Ö—Å—è —Ñ–∏—á–µ–π (–ø—Ä–æ—Å—Ç–µ–π—à–∏–π –ø—Ä–∏–º–µ—Ä - –Ω–æ–≤—ã–π –∞–π–¥–∏—à–Ω–∏–∫ —Ç–æ–≤–∞—Ä–∞). –ò–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—Å—Ç–∞–≤–∫–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –∑–∞–Ω–æ–≤–æ –ø–µ—Ä–µ–≤–∞—Ä–∏–≤–∞—Ç—å 100+ —Å–ª–æ–≤–∞—Ä–µ–π –ø–æ–¥ –∫–∞–∂–¥–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –∫–∞–∫—É—é-—Ç–æ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫—É—é –ª–æ–≥–∏–∫—É –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –∫–ª—é—á–µ–π –≤–Ω–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
* **–†–æ—Å—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞**. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –º—ã –ø—ã—Ç–∞–µ–º—Å—è –∑–∞—Ä–∞–Ω–µ–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–æ–º –∏ –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –Ω–∞–º —Ñ–∏—á–∏ –Ω—É–∂–Ω—ã, –∞ –∫–∞–∫–∏–µ –Ω–µ—Ç. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç end2end –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: –≥–æ—Ä–∞–∑–¥–æ –ª—É—á—à–µ –∏ —É–¥–æ–±–Ω–µ–µ –ø–æ–¥–∞–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –≤–æ–æ–±—â–µ –≤—Å–µ –∏ –∏–º–µ—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ—Ç–±–æ—Ä–∞ —Ñ–∏—á–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏. –¢–∞–∫, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Å–∞–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –Ω—É–∂–Ω—ã–π –µ–π –Ω–∞–±–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏. –¢–∞–∫–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–µ —Ç–æ–ª—å–∫–æ –≤—Å—Ç–∞–≤–∫–∏, –Ω–æ –∏ —É–¥–∞–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑ –æ–±—É—á–µ–Ω–∏—è

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ads_pytorch –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä—è–¥ –Ω–∞—Ä–∞–±–æ—Ç–æ–∫, —Ä–µ—à–∞—é—â–∏—Ö —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã. –§—É–Ω–¥–∞–º–µ–Ω—Ç–æ–º –¥–ª—è –≤—Å–µ—Ö –Ω–∞—Ä–∞–±–æ—Ç–æ–∫ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–π HashEmbedding

## HashEmbedding
HashEmbedding - –ø—Ä—è–º–æ–π –∞–Ω–∞–ª–æ–≥ torch.nn.EmbeddingBag —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–∞–∂–Ω—ã–º–∏ –æ—Ç–ª–∏—á–∏—è–º–∏:
1. –ü–æ–¥ –∫–∞–ø–æ—Ç–æ–º - —á–µ—Å—Ç–Ω–∞—è —Ö–µ—à —Ç–∞–±–ª–∏—Ü–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –∫–ª—é—á–æ–º —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π uint64, –∑–Ω–∞—á–µ–Ω–∏–µ–º - –≤–µ–∫—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
2. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ RAM –∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –Ω–∞ CPU, –¥–∞–±—ã —ç–∫–æ–Ω–æ–º–∏—Ç—å GPU –ø–∞–º—è—Ç—å –ø–æ–¥ –±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω—ã–µ –≤–µ—â–∏
3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ C++, –Ω–∞ –ø–æ—Ä—è–¥–∫–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∞—è CPU —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é torch.nn.EmbeddingBag
4. –û–ø–µ—Ä–∞—Ü–∏—è –≤—Å—Ç–∞–≤–∫–∏: –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ ```optimizer.step()```
5. –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, [rmsprop_norm](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/ads_pytorch/ads_pytorch/hash_embedding/optim.py?rev=r7742332#L206). –†–∞–±–æ—Ç–∞—é—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Ç–∏–ø–∞ adam –∏ —ç–∫–æ–Ω–æ–º—è—Ç –ø–∞–º—è—Ç—å (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ adam'—ã, –≤–ø—Ä–æ—á–µ–º, —É –Ω–∞—Å —Ç–æ–∂–µ –µ—Å—Ç—å)
6. –ú–µ—Ö–∞–Ω–∏–∑–º—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ö–µ—à-—Ç–∞–±–ª–∏—Ü—ã: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ, —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–ª—é—á–µ–π, **future** L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ, –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å HashEmbedding –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤–∞—à–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å uint64 –∫–ª—é—á–µ–π. –ì–æ—Ç–æ–≤–∏—Ç—å –∑–∞—Ä–∞–Ω–µ–µ —Å–ª–æ–≤–∞—Ä—å –Ω–µ –Ω—É–∂–Ω–æ.

### –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
HashEmbedding –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –¥–≤–∞ —Ç–µ–Ω–∑–æ—Ä–∞: —Ç–µ–Ω–∑–æ—Ä –∞–π–¥–∏—à–Ω–∏–∫–æ–≤ —Ñ–∏—á–µ–π —Ç–∏–ø–∞ int64 –∏ —Ç–µ–Ω–∑–æ—Ä –¥–ª–∏–Ω –æ–±—ä–µ–∫—Ç–æ–≤ —Ç–∏–ø–∞ int32.

```python
for i, feaid in enumerate([1, 2, 3, 4, 5, 67, 458476, 478273409843]): # this is 'vocabulary'
    item = AdamItem(3)
    item.w = torch.FloatTensor([i] * 3)
    hash_table.insert_item(feaid, item)
    
# feaid			/	item.w
# 1				/	[0, 0, 0]
# 2				/	[1, 1, 1]
# 3				/	[2, 2, 2]
# ...
# 478273409843	/	[7, 7, 7]

data = torch.LongTensor([1, 2, 3, 4, 5, 67, 458476, 1, 478273409843]) # this is concrete sample, using ids from 'vocabulary'
data_len = torch.IntTensor([3, 4, 2])

hash_table(data, data_len, compute_mode="sum")
 
# then sum of first 3 embeddings in data would be
# [3, 3, 3] = ([0, 0, 0] + [1, 1, 1] + [2, 2, 2])
#
# the last group is (1, 478273409843), their embeddings are [0, 0, 0] and [7, 7, 7], so the sum is [7, 7, 7]

tensor([[3., 3., 3.],                          # 1 + 2 + 3
        [22., 22., 22.],                       # 4 + 5 + 6 + 7
        [7., 7., 7.]], requires_grad=True)     # 1 + 8
```

### compute_mode

–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π —á–∏—Ç–∞—Ç–µ–ª—å –∑–∞–º–µ—Ç–∏–ª –≤ –ø—Ä–∏–º–µ—Ä–µ –≤—ã—à–µ –∞—Ä–≥—É–º–µ–Ω—Ç ```(python) compute_mode="sum"```.

–í HashEmbedding –µ—Å—Ç—å –¥–≤–∞ —Ä–µ–∂–∏–º–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–ø–æ—Å–ª–µ–¥–Ω–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç –≤ forward):
* ```sum``` - —Å—É–º–º–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

* ```mean``` - —Å—Ä–µ–¥–Ω–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (sum / –∫–æ–ª-–≤–æ —ç–º–±–µ–¥–æ–≤ –≤ —Ñ–∏—á–µ, —Ä–∞–≤–Ω–æ–µ —á–∏—Å–µ–ª–∫–µ –≤ data_len —Ç–µ–Ω–∑–æ—Ä–µ)

  (_–≤ –ø—Ä–∏–º–µ—Ä–µ –Ω–∏–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç–ª–∏—á–Ω—ã–µ –æ—Ç –∫–æ–¥–∞ –≤—ã—à–µ_)

```
In [13]: embed(torch.LongTensor([1, 2, 3]), torch.IntTensor([2, 1]))  # default is "sum"
Out[13]:
tensor([[ 0.0130,  0.0004, -0.0123],
        [ 0.0128, -0.0051, -0.0048]], requires_grad=True)

In [15]: embed(torch.LongTensor([1, 2, 3]), torch.IntTensor([2, 1]), compute_mode="mean")
Out[15]:
tensor([[ 0.0065,  0.0002, -0.0062],
        [ 0.0128, -0.0051, -0.0048]], requires_grad=True)
```

### –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
–ó–∞—á–∞—Å—Ç—É—é –ø–æ–ª–µ–∑–Ω–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å –¥–≤—É–º–µ—Ä–Ω—ã–π BagOfWords, –∞ –≤—ã—á–∏—Å–ª–∏—Ç—å N-–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –ø–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å - —ç–º–±–µ–¥–¥–∏–Ω–≥.
–ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ RNN/Transformer –Ω—É–∂–µ–Ω —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (batch_size, sequence_size, embedding_dim). ads_pytorch –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π.

–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä (–≤–∑—è—Ç –∏–∑ —Ç–µ—Å—Ç–æ–≤ https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/ads_pytorch/tests/hash_embedding/test_hash_embedding.py?rev=7149351#L88):
```python
hash_table = HashEmbedding(AdamEmbeddingHashTable(dim))
# This method of inserting features is nice only for testing or playgrounds
# Insertion in optimizer while training works much faster in C++
for i in range(6):
    item = AdamItem(dim)
    item.w = torch.FloatTensor([i] * dim)
    hash_table.insert_item(i, item)

data = torch.LongTensor([1, 2, 1, 3, 4, 3, 4, 1, 5, 2, 4, 5, 3])
data_len = torch.IntTensor([[1, 1, 3], [1, 1, 1], [1, 1, 0], [2, 0, 1]])
res = hash_table.forward(data, data_len, compute_mode="sum")

reference = torch.FloatTensor([
    [
        [1] * dim,
        [2] * dim,
        [8] * dim  # sum of 1 + 3 + 4
    ],
    [
        [3] * dim,
        [4] * dim,
        [1] * dim
    ],
    [
        [5] * dim,
        [2] * dim,
        [0] * dim
    ],
    [
        [9] * dim,  # sum of 4 + 5
        [0] * dim,
        [3] * dim
    ]
])
assert torch.allclose(res, reference)
```

### –û–±–æ–±—â–µ–Ω–Ω—ã–π –∫–æ–¥ –ª—é–±—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ HashEmbedding
–í–æ—Ç –∫–æ–¥, —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π —Ç–æ–º—É, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ C++ –∫–æ–¥–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –¥–ª—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
```python
M = 3  # ur seq len
N = 4  # batch size
dim = 5  # embedding_dim
data = torch.LongTensor([1, 2, 1, 3, 4, 3, 4, 1, 5, 2, 4, 5, 3])
data_len = torch.IntTensor([[1, 1, 3], [1, 1, 1], [1, 1, 0], [2, 0, 1]])
assert data_len.sum() == data.numel()
assert data_len.size() == (N, M)
result = torch.zeros(N, M, dim)
compute_mode = "sum"

# –≠—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—é —Ö–µ—à-–º–∞–ø—ã –≤—ã—à–µ
embedding: Dict[int, torch.Tensor] = {key: torch.full((dim, ), fill_value=float(key)) for key in data.tolist()}

# –í–æ—Ç —ç—Ç–æ—Ç –∫–æ–¥ "–≤ –ª–æ–±" –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ –ª—é–±—ã–µ data_len —Ç–µ–Ω–∑–æ—Ä—ã: –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –ø–æ –∫–∞–∂–¥–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
data_offset = 0
for i in range(N):
    for j in range(M):
        # –ó–Ω–∞—á–µ–Ω–∏—è –≤ data_len - —Å–∫–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω—É–∂–Ω–æ —Å–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ –¥–∞–Ω–Ω–æ–π —è—á–µ–π–∫–µ —Ç–µ–Ω–∑–æ—Ä–∞
        # –ú–æ—Ç–∏–≤–∞—Ü–∏—è - —Ç–∞ –∂–µ, —á—Ç–æ —É –∞–≤—Ç–æ—Ä–æ–≤ torch.nn.EmbeddingBag: —Ç–∞–∫–æ–π –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        cur_obj_count = data_len[i][j]
        for obj_offset in range(cur_obj_count):
            feature_id = int(data[data_offset])
            result[i][j] += embedding[feature_id]
            data_offset += 1
        if compute_mode == "mean":
            result[i][j] /= max(cur_obj_count, 1)

# result
tensor([[[1., 1., 1., 1., 1.],
         [2., 2., 2., 2., 2.],
         [8., 8., 8., 8., 8.]],

        [[3., 3., 3., 3., 3.],
         [4., 4., 4., 4., 4.],
         [1., 1., 1., 1., 1.]],

        [[5., 5., 5., 5., 5.],
         [2., 2., 2., 2., 2.],
         [0., 0., 0., 0., 0.]],

        [[9., 9., 9., 9., 9.],
         [0., 0., 0., 0., 0.],
         [3., 3., 3., 3., 3.]]])
```

–û–±—Ä–∞—Ç–∏—Ç–µ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, –∫–∞–∫ –∏–º–µ–Ω–Ω–æ –æ–±—Ö–æ–¥–∏—Ç—Å—è data_len —Ç–µ–Ω–∑–æ—Ä –∏ –∫–∞–∫ –¥–æ—Å—Ç–∞—é—Ç—Å—è —Ñ–∏—á–∏ –∏–∑ data, —ç—Ç–æ –∑–Ω–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏ –≤–∞—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö.

## BaseEmbeddingModel –∏ IDeployableModel
BaseEmbeddingModel - —ç—Ç–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è –Ω–∞–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏. –í 99% —Å–ª—É—á–∞–µ–≤ –∫–æ–Ω–µ—á–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å –∏–º–µ–Ω–Ω–æ —Å –Ω–µ–π. BaseEmbeddingModel - —ç—Ç–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ **–≤—Å–µ—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –≤ –µ–¥–∏–Ω—ã–π —Å–ª–æ–π**. –¢–∞–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å:
1. –£–¥–æ–±–Ω–æ —à–∞—Ä–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏
2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
3. –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ - –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ñ–∏—á–∏ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ª–∏—à—å –ø–æ–º–µ–Ω—è—Ç—å –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —Ñ–∞–±—Ä–∏–∫–∏, —á—Ç–æ–±—ã –æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –Ω–∞—Ä–∞–±–æ—Ç–∫–∏

IDeployableModel - –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–ª–æ—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ—Ç–æ–º –∑–∞–¥–µ–ø–ª–æ–∏—Ç—å –≤ –∞—Ä–∫–∞–¥–∏–π–Ω—ã–π C++ inference.

BaseEmbeddingModel –∏–º–µ–µ—Ç –º–∞–ª–æ —Å–º—ã—Å–ª–∞ –≤ –æ—Ç—Ä—ã–≤–µ –æ—Ç IDeployableModel (–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç), –ø–æ—ç—Ç–æ–º—É —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∏—Ö –≤–º–µ—Å—Ç–µ. –õ—É—á—à–∏–º –ø—Ä–∏–º–µ—Ä–æ–º –∑–¥–µ—Å—å –±—É–¥–µ—Ç end2end –∫–æ–¥ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ—Å—Ç–µ–Ω—å–∫–æ–≥–æ DSSM.

```(python)
from typing import Dict, Union, List, Any

from ads_pytorch.nn.module.base_embedding_model import (
    BaseEmbeddingModel,
    FeatureOrderHolder,
    EmbeddingDescriptor,
    EmbeddingComputeDescriptor,
    embedding_descriptors_to_dim_dict
)
from ads_pytorch.deploy.deployable_model import (
    IDeployableModel,
    TorchModuleDeployDescriptor,
    ParameterServerModelDeployDescriptor
)
import torch


# –≠—Ç–æ –∏–≥—Ä—É—à–µ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞. –î–ª—è –±–æ–µ–≤—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤ –≤ –∫–∞—á–µ—Å—Ç–≤–µ feed-forward –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –≤—ã–¥–∞—é—â–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∏,
# –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ
# ads_pytorch.nn.module.densenet.WeightNormalizedEmbeddingNetwork
class EmbeddingNetwork(torch.nn.Module):
    def __init__(
        self,
        feature_order_holder: FeatureOrderHolder,
        features: List[str],
        deep_net: torch.nn.Module
    ):
        super(EmbeddingNetwork, self).__init__()
        self.feature_order_holder = feature_order_holder
        self.features = features
        self.deep_net = deep_net

    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:
        inputs = [inputs[i] for i in self.feature_order_holder.get_ids(self.features)]
        tensor = torch.cat(inputs, dim=1)
        return self.deep_net(tensor)


# –í–æ–æ–±—â–µ –≥–æ–≤–æ—Ä—è, –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ torch.nn.Module, —Ä–∞–≤–Ω–æ –∫–∞–∫ –∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤—Ö–æ–¥–æ–≤
# –û–¥–Ω–∞–∫–æ, —Ç–æ–ª—å–∫–æ IDeployableModel –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑ –∫–æ—Ä–æ–±–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ C++ –∫–æ–¥ inference, —Ç–∞–∫ —á—Ç–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ –µ–≥–æ
class DSSM(IDeployableModel):
    def __init__(
        self,
        embedding_model: BaseEmbeddingModel,
        query: EmbeddingNetwork,
        document: EmbeddingNetwork
    ):
        super(DSSM, self).__init__(embedding_model=embedding_model)
        self.query = query
        self.document = document

        self.alpha = torch.nn.Parameter(torch.ones(1))
        self.bias = torch.nn.Parameter(torch.zeros(1))

    def deployable_model_forward(self, embedded_inputs: List[torch.Tensor]) -> Any:
        query = self.query(embedded_inputs)
        document = self.document(embedded_inputs)

        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ –≤–∞–º –Ω—É–∂–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ç–æ –æ–Ω–∞ –¥–µ–ª–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—â–∏—Ö –ø–æ–¥—Å–µ—Ç–µ–π
        return self.alpha + (query * document).sum(dim=1) + self.bias

    # –ü—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –∑–∞–¥–µ–ø–ª–æ–∏—Ç—å –¥–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –î–µ–ø–ª–æ–π —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø–æ–¥—Ä–æ–±–Ω–µ–µ –Ω–∏–∂–µ
    def get_serializable_models(self) -> Dict[str, Union[TorchModuleDeployDescriptor, ParameterServerModelDeployDescriptor]]:
        return {
            "query": ParameterServerModelDeployDescriptor(
                features_order=self.query.features,
                model=self.query
            ),
            "document": ParameterServerModelDeployDescriptor(
                features_order=self.document.features,
                model=self.document
            )
        }


def build_feed_forward_impl(in_features: int, out_features: int) -> torch.nn.Module:
    return torch.nn.Sequential(
        torch.nn.Linear(in_features=in_features, out_features=in_features),
        torch.nn.ReLU(),
        torch.nn.LayerNorm(in_features),
        torch.nn.Linear(in_features=in_features, out_features=out_features)
    )


if __name__ == '__main__':
    final_embedding_dim = 50

    # –î–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –í–°–ï–• —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –º–æ–¥–µ–ª–∏. –°–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É IDeployableModel,
    # –≤ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å *—Ä–æ–≤–Ω–æ –æ–¥–∏–Ω* —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π BaseEmbeddingModel, –Ω–∞—Å—á–∏—Ç—ã–≤–∞—é—â–∏–π –≤—Å–µ —Ñ–∏—á–∏
    embedding_model = BaseEmbeddingModel(
        embeddings=[
            # –û–¥–∏–Ω EmbeddingDescriptor –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Ö–µ—à-—Ç–∞–±–ª–∏—Ü—É —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            # –û—Ç–º–µ—Ç–∏–º, —á—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—ã—á–Ω—ã—Ö —Ç–æ—Ä—á–æ–≤—ã—Ö —Å–ª–æ–µ–≤,
            # –∑–¥–µ—Å—å —Å—Ç–µ–π—Ç –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä–∞ —Ö—Ä–∞–Ω–∏—Ç—Å—è —Ä—è–¥–æ–º —Å –≤–µ—Å–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é —Ö–µ—à–º–∞–ø—É
            # —Å –∫—É—á–µ–π –±–∞–∫–µ—Ç–æ–≤ –∏ –ª–∏—à–Ω–∏–º–∏ —É–∫–∞–∑–∞—Ç–µ–ª—è–º–∏
            EmbeddingDescriptor(
                name="Categories",
                # –î–∞–Ω–Ω–∞—è —Ö–µ—à-—Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π
                features=["QueryCategory", "DocumentCategory"],
                dim=4,  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                algo_type="adam"  # adam - –æ—Ç–ª–∏—á–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç, –Ω–æ –∂–∏—Ä–Ω—ã–π –ø–æ –ø–∞–º—è—Ç–∏. SOTA - rmsprop_norm
            ),
            EmbeddingDescriptor(
                name="Texts",
                features=[
                    # –ö–∞–∂–¥–∞—è —Ñ–∏—á–∞, –∑–∞–ø–∏—Å–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ—á–∫–æ–π, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ç—Å—è –≤
                    # EmbeddingComputeDescriptor(feature=<name>, compute_mode="mean")
                    # –° –ø–æ–º–æ—â—å—é EmbeddingComputeDescriptor, –º–æ–∂–Ω–æ —É–ø—Ä–∞–≤–ª—è—Ç—å –≤—Å–µ–º–∏ —Ñ–∏—á–∞-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    # —ç–º–±–µ–¥–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ–∏—á–∏.
                    EmbeddingComputeDescriptor(
                        feature="DocumentText",
                        compute_mode="mean",
                        # –ë—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—É—é —Ñ–∏—á—É —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 0.2, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö –±—ã–≤–∞–µ—Ç
                        # —Ä–µ–¥–∫–∏–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –º—É—Å–æ—Ä
                        add_prob=0.2,
                    ),
                    # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
                    "DocumentTitle",
                    "QueryText"
                ],
                dim=7,
                algo_type="adam"
            ),
        ],
        # *External* - –≤—Å–µ —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –Ω–∞—Å—á–∏—Ç–∞–ª–∏ –∏–∑–≤–Ω–µ –∏ –∫–æ—Ç–æ—Ä—ã–º –Ω–µ –Ω—É–∂–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–∏
        external_factors=["DocumentCounters"]
    )

    # –ü—Ä–µ–∂–¥–µ —á–µ–º –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –æ–ø–∏—Å–∞–Ω–∏—é –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞–∑–±–µ—Ä–µ–º—Å—è —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –≤—Ö–æ–¥–∞. –ü–æ—Å–∫–æ–ª—å–∫—É BaseEmbeddingModel
    # –¥–æ–ª–∂–µ–Ω –ª–µ–∂–∞—Ç—å –≤ –æ—Å–Ω–æ–≤–µ –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–æ –∏ —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–∞ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ–Ω—è—Ç—å –∏–º–µ–Ω–Ω–æ –ø–æ–¥ –Ω–µ–≥–æ

    def build_inputs(batch_size: int) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        # –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–æ–≤ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –ø—Ä–æ—Å—Ç–æ–π: —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å—Ç—Ä–æ–∫–∞ ->:
        # 1. –ï—Å–ª–∏ —Ñ–∏—á–∞ external, —Ç–æ –±—É–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ —Ç–µ–Ω–∑–æ—Ä
        # 2. –ï—Å–ª–∏ —Ñ–∏—á–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è, —Ç–æ –±—É–¥–µ—Ç —Å–ø–∏—Å–æ–∫ –ª–∏–±–æ –∏–∑ [data, data_len], –ª–∏–±–æ –∏–∑ [data, data_len, weight] —Ç–µ–Ω–∑–æ—Ä–æ–≤
        # [data, data_len, weight] - —ç—Ç–æ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π BoW

        def build_randcat_2dim(batch_size: int):
            return [
                torch.randint(100000, size=(batch_size,), dtype=torch.int64),  # data
                torch.tensor([1] * batch_size, dtype=torch.int32)  # data_len
            ]

        def build_randexternal_2dim(batch_size: int, dim: int):
            return torch.randn((batch_size, dim), dtype=torch.float32)

        return {
            "QueryText": build_randcat_2dim(batch_size=batch_size),
            "QueryCategory": build_randcat_2dim(batch_size=batch_size),
            "DocumentTitle": build_randcat_2dim(batch_size=batch_size),
            "DocumentText": build_randcat_2dim(batch_size=batch_size),
            "DocumentCategory": build_randcat_2dim(batch_size=batch_size),
            "DocumentCounters": build_randexternal_2dim(batch_size=batch_size, dim=20)
        }
    inputs_example = build_inputs(batch_size=3)

    # BaseEmbeddingModel –≤—ã–¥–∞–µ—Ç...–Ω–µ —Å–ª–æ–≤–∞—Ä—å, –∞ —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤
    # –ú–Ω–æ–≥–∏–µ –ø—Ä–∏–º–µ–Ω—è–ª–∫–∏ –≤ C++ –æ—Ä—É–¥—É—é—Ç –∏–º–µ–Ω–Ω–æ –º–∞—Å—Å–∏–≤–∞–º–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤, –∞ –Ω–µ —Ö–µ—à-—Ç–∞–±–ª–∏—Ü–∞–º–∏
    # –ï—Å–ª–∏ –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ö–µ—à-–º–∞–ø—ã, —Ç–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π –∏ —Å–ª–æ–µ–≤ –æ–≤–µ—Ä—Ö–µ–¥ –Ω–∞ –ª—É–∫–∞–ø—ã –º–æ–∂–µ—Ç –±—ã—Ç—å
    # —Å—Ä–∞–≤–Ω–∏–º —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º —Å–∞–º–æ–≥–æ —Å–ª–æ—è. –ê –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ü–µ–ª–∏–º—Å—è –≤ highload —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã,
    # —É –Ω–∞—Å –∫–∞–∂–¥–∞—è –º–∏–ª–∏—Å–µ–∫—É–Ω–¥–∞ –Ω–∞ —Å—á–µ—Ç—É.
    # –û–¥–Ω–∞–∫–æ, –ø—Ä–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–Ω–æ –≤—Å–µ –º–æ–¥—É–ª–∏ –¥–æ–ª–∂–Ω—ã –æ—Ä—É–¥–æ–≤–∞—Ç—å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏,
    # –∞ –Ω–µ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –≤ —Ö—Ä–µ–Ω –ø–æ–π–º–∏ –∫–∞–∫–æ–º –º–∞—Å—Å–∏–≤–µ. –ü—Ä–∏ 100+ —Ñ–∏—á–∞—Ö —Ç–∞–∫–æ–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å.
    # –í –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º FeatureToOrderHolder. –°—É—Ç—å –≤ —Ç–æ–º, —á—Ç–æ–±—ã –≤—Å–µ-–≤—Å–µ-–≤—Å–µ —Å–ª–æ–∏
    # –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∏ —Å—Å—ã–ª–∫—É –Ω–∞ –æ–±—ä–µ–∫—Ç-—Ö–æ–ª–¥–µ—Ä —Å –ø–æ—Ä—è–¥–∫–æ–º —Ñ–∏—á–µ–π. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∞
    # –≤ C++, —Å —Ç–æ–π –ª–∏—à—å —Ä–∞–∑–Ω–∏—Ü–µ–π, —á—Ç–æ –≤ C++ –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö —Ñ–∏—á–µ–π –≤ –∏–Ω–¥–µ–∫—Å—ã –º–∞—Å—Å–∏–≤–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç
    # –∑–∞—Ä–∞–Ω–µ–µ –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞—Ö, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –ª—É–∫–∞–ø—ã –Ω–∞ inference
    embedding_outputs: List[torch.Tensor] = embedding_model(inputs_example)

    feature_holder: FeatureOrderHolder = embedding_model.get_feature_order_holder()

    # –î–æ–ø—É—Å—Ç–∏–º, –º—ã —Ö–æ—Ç–∏–º –¥–æ—Å—Ç–∞—Ç—å —Ñ–∏—á—É x. –ü–∏—à–µ–º
    print(embedding_outputs[feature_holder.get_ids(["QueryText"])[0]])

    # –ò–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∏—á–µ–π
    print([embedding_outputs[i] for i in feature_holder.get_ids(["QueryText", "QueryCategory"])])

    # –†–∞—Å–ø–∏—à–µ–º –æ–¥–∏–Ω —Ä–∞–∑ —Ä—É—á–∫–∞–º–∏ forward-pass –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞:
    _partial_manual_outputs = {
        # –≠–º–±–µ–¥–∏–º —Ç–µ–∫—Å—Ç—ã
        "QueryText": embedding_model.embeddings["Texts"](*inputs_example["QueryText"]),
        "DocumentTitle": embedding_model.embeddings["Texts"](*inputs_example["DocumentTitle"]),
        # –ò –Ω–µ –∑–∞–±—ã–≤–∞–µ–º –ø—Ä–æ add_prob –∏–∑ –¥–µ–∫—Å—Ä–∏–ø—Ç–æ—Ä–∞
        "DocumentText": embedding_model.embeddings["Texts"](*inputs_example["DocumentText"], add_prob=0.2),

        # –≠–º–±–µ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        "QueryCategory": embedding_model.embeddings["Categories"](*inputs_example["QueryCategory"]),
        "DocumentCategory": embedding_model.embeddings["Categories"](*inputs_example["DocumentCategory"]),

        # –≤—Å–µ external —Ñ–∏—á–∏ –ø—Ä–æ—Å—Ç–æ —Ñ–æ—Ä–≤–∞—Ä–¥–∏–º –Ω–∞–≤–µ—Ä—Ö
        "DocumentCounters": inputs_example["DocumentCounters"]
    }
    _list_manual_outputs = [_partial_manual_outputs[x] for x in feature_holder.get_order()]
    for t1, t2 in zip(embedding_outputs, _list_manual_outputs):
        assert torch.allclose(t1, t2)

    # –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Ü–µ–ª—å —Ç–∞–∫–∞—è: —Å–¥–µ–ª–∞—Ç—å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π —Å C++ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ inference –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –Ω–µ —É–¥–∞—Ä–∏–≤ –ø–æ —É–¥–æ–±—Å—Ç–≤—É
    # 1. –§–∞–±—Ä–∏–∫–∏-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã –º–æ–¥—É–ª–µ–π –æ–ø–µ—Ä–∏—Ä—É—é—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏ —Ñ–∏—á–µ–π => –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç—å –∏
    # —É–¥–æ–±—Å—Ç–≤–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ —Å—Ç—Ä–∞–¥–∞–µ—Ç
    # 2. forward –æ—Ä—É–¥—É–µ—Ç –º–∞—Å—Å–∏–≤–∞–º–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ => –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ä–∞–∑—É –ø–∏—Å–∞—Ç—å –∫–æ–¥ —Ç–∞–∫, —á—Ç–æ–±—ã —Å–ª–æ–π
    # –º–æ–∂–Ω–æ –±—ã–ª–æ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º –∑–∞–¥–µ–ø–ª–æ–∏—Ç—å –≤ –ø–ª—é—Å—ã
    # 3. –ß—Ç–æ–±—ã –¥–æ—Å—Ç–∞—Ç—å —Ñ–∏—á—É, –Ω—É–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å... –Ω–∞ 30 —Å–∏–º–≤–æ–ª–æ–≤ –±–æ–ª—å—à–µ, —á–µ–º –ª—É–∫–∞–ø –≤ —Å–ª–æ–≤–∞—Ä—å.
    # –ú—ã —Ä–µ—à–∏–ª–∏, —á—Ç–æ —ç—Ç–æ –¥–æ—Å—Ç–æ–π–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å

    # –û–∫–µ–π, –≤–µ—Ä–Ω–µ–º—Å—è –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π

    query_embeddings = [
        "QueryText",
        "QueryCategory"
    ]
    query_external = {}

    document_embeddings = [
        "DocumentTitle",
        "DocumentText",
        "DocumentCategory"
    ]

    document_external = {
        "DocumentCounters": 20
    }

    dim_dict = embedding_descriptors_to_dim_dict(embedding_descriptors=embedding_model.embedding_descriptors)

    # –ü–µ—Ä–µ–¥–∞–µ–º –Ω–∞—à feature_order_holder –≤–æ –≤—Å–µ –º–æ–¥–µ–ª—å–∫–∏
    document_network = EmbeddingNetwork(
        feature_order_holder=embedding_model.get_feature_order_holder(),
        features=document_embeddings + list(document_external.keys()),
        deep_net=build_feed_forward_impl(
            in_features=sum(dim_dict[name] for name in document_embeddings) + sum(document_external.values()),
            out_features=final_embedding_dim
        )
    )

    query_network = EmbeddingNetwork(
        feature_order_holder=embedding_model.get_feature_order_holder(),
        features=query_embeddings + list(query_external.keys()),
        deep_net=build_feed_forward_impl(
            in_features=sum(dim_dict[name] for name in query_embeddings),
            out_features=final_embedding_dim
        )
    )

    dssm = DSSM(
        embedding_model=embedding_model,
        query=query_network,
        document=document_network
    )

    inputs = build_inputs(batch_size=10)
    prediction = dssm(inputs)
    print(prediction)
```

# –§–∞–±—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π
–û–∫–µ–π, –≤–æ—Ç —Ç—É—Ç –º—ã —É–∂–µ –∑–∞–ø–∏–ª–∏–º –Ω–µ—á—Ç–æ –ø–æ—Å–µ—Ä—å–µ–∑–Ω–µ–µ. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ, –º–∞—Ä–∞—Ö–∞–π–∫–∞ —Å ```FeatureOrderHolder``` —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∞—Å—å –ø–æ–¥ —Å–ª–æ–∂–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.

–ü–æ—Å–∫–æ–ª—å–∫—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –≤ –Ø–Ω–¥–µ–∫—Å–µ –æ–±—ã—á–Ω–æ –∑–∞–Ω–∏–º–∞—é—Ç—Å—è –±–µ–∫–µ–Ω–¥–µ—Ä—ã, –∞ –Ω–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞ —Å–∞—Ç–∞–Ω–∏—Å—Ç—ã, –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –æ–ø–∏—Å–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–ª–∂–µ–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç—å –ø–æ–¥ —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–æ–∫–∞—á–∞–Ω–Ω—ã–π —Å–∫–∏–ª–ª –±–µ–∫–µ–Ω–¥–µ—Ä–æ–≤: —É–º–µ–Ω–∏–µ –ø–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞—Ç—å json.

## BaseEmbeddingModel json
–î—É–º–∞—é, –∑–¥–µ—Å—å –ø—Ä–∏–º–µ—Ä –±—É–¥–µ—Ç –ª—É—á—à–µ 1000 —Å–ª–æ–≤.

–í–æ—Ç —Ç–∞–∫–æ–π –≤–æ—Ç json –∫–æ–Ω—Ñ–∏–≥
```(json)
"features": {
    "external": [
        "CountersAggregatedValues",
        "BigBQueryFactors",
    ],
    "embeddings": {
        "default_algo": "rmsprop_norm",
        "default_algo_params": {
            "lr": 0.0003,
            "expiration_ttl": 336
        },
        "features": [
            {
                "dim": 64,
                "name": "UserRegionID"
            },
            {
                "dim": 64,
                "name": "UserBestInterests",
                "algo": "adam"
            },
            {
                "dim": 72,
                "name": "AffinitiveSites",
                "features": [
                    "KryptaTopDomain",
                    {"name": "UserCryptaAffinitiveSitesIDs", "add_prob": 0.1}
                ]
            },
            {
                "dim": 64,
                "name": "QueryTexts",
                "features": [
                    "BigBQueryTexts",
                    "QueryHistoryTexts",
                    "SearchQueryTextTokenLemma"
                ],
                "add_prob": 0.3
            },
            {
                "dim": 64,
                "name": "Urls",
                "features": [
                    "LandingPageUrl",
                    {"name": "BannerUrl", "add_prob": 0.2},
                ],
                "add_prob": 0.3
            }
        ]
    }
}
```


–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤–æ—Ç –≤ —Ç–∞–∫–æ–π python –∫–æ–¥

```(python)

    model = BaseEmbeddingModel(
        external_factors=[
            "CountersAggregatedValues",
            "BigBQueryFactors",
        ],
        embeddings=[
            EmbeddingDescriptor(
                name="UserRegionID",
                features=["UserRegionID"],
                dim=64,
                algo_type="rmsprop_norm"
            ),
            EmbeddingDescriptor(
                name="UserBestInterests",
                features=["UserBestInterests"],
                dim=64,
                algo_type="adam"
            ),
            EmbeddingDescriptor(
                name="AffinitiveSites",
                features=[
                    "KryptaTopDomain",
                    EmbeddingComputeDescriptor(
                        feature="UserCryptaAffinitiveSitesIDs",
                        add_prob=0.1
                    )
                ],
                dim=72,
                algo_type="rmsprop_norm"
            ),
            EmbeddingDescriptor(
                name="QueryTexts",
                features=[
                    EmbeddingComputeDescriptor(
                        feature="BigBQueryTexts",
                        add_prob=0.3
                    ),
                    EmbeddingComputeDescriptor(
                        feature="QueryHistoryTexts",
                        add_prob=0.3
                    ),
                    EmbeddingComputeDescriptor(
                        feature="SearchQueryTextTokenLemma",
                        add_prob=0.3
                    )
                ],
                dim=64,
                algo_type="rmsprop_norm"
            ),
            EmbeddingDescriptor(
                name="Urls",
                features=[
                    EmbeddingComputeDescriptor(
                        feature="LandingPageUrl",
                        add_prob=0.3
                    ),
                    EmbeddingComputeDescriptor(
                        feature="BannerUrl",
                        add_prob=0.2
                    ),
                ],
                dim=64,
                algo_type="rmsprop_norm"
            ),
        ]
    )
```


## SubNetworks
###  –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –ø–æ—Å—Ç—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
–ó–¥–µ—Å—å –Ω–∞–¥–æ –±—É–¥–µ—Ç –ø–æ–≥–ª—É–±–∂–µ –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç –æ–ø–∏—Å–∞–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö —á–∞—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π. –ì–ª—É–±–æ–∫–∞—è —á–∞—Å—Ç—å - –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫—Ä–æ–º–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–µ–≤.

–£ –∫–∞–∂–¥–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –µ—Å—Ç—å –¥–≤–∞ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–∏–∫–∞: ```sub_networks``` –∏ ```normalizers```:
* ```normalizers``` - —ç—Ç–æ –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–¥—Å–µ—Ç–∏ —Å –æ–¥–Ω–∏–º –≤—Ö–æ–¥–æ–º –∏ –æ–¥–Ω–∏–º –≤—ã—Ö–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–µ **–ø–æ–¥–º–µ–Ω—è—é—Ç —Å–≤–æ–∏–º –≤—ã—Ö–æ–¥–æ–º —Ñ–∏—á—É**
* ```sub_networks``` - —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–µ –ø–æ–¥—Å–µ—Ç–∏ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –≤—Ö–æ–¥–æ–≤.

–û–±–æ–±—â–µ–Ω–Ω—ã–π –∫–æ–¥ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥—Å–µ—Ç–µ–π –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫-—Ç–æ —Ç–∞–∫:

```(python)
class MyModuleWithNorm(torch.nn.Module):
    def __init__(
        self,
        feature_holder: FeatureOrderHolder,
        features: List[str],
        normalizers: Dict[str, torch.nn.Module],
        sub_networks: Dict[str, torch.nn.Module]
    ):
        super(MyModuleWithNorm, self).__init__()
        self.normalizers = torch.nn.ModuleDict(normalizers)
        self.sub_networks = torch.nn.ModuleDict(sub_networks)
        self.feature_holder = feature_holder
        self.features = features[:]

    def forward(self, embedding_inputs):
        # –°–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç–∞–µ–º –ø–æ–¥—Å–µ—Ç–∏ –Ω–∞ —á–∏—Å—Ç—ã—Ö –Ω–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–∞—Ö —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–∞–π–¥-—ç—Ñ—Ñ–µ–∫—Ç–æ–≤.
        # –ü–æ –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É, –º—ã –¥–æ–ª–∂–Ω—ã –≤—ã—á–∏—Å–ª—è—Ç—å —Å–µ—Ç–∏ —Å —Å–∞–º—ã—Ö –Ω–∏–∂–Ω–∏—Ö —Å–ª–æ–µ–≤,
        # –Ω–æ, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–º –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –≤—Ö–æ–¥—ã –±—É–¥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å—Å—è —Å–Ω–∞—á–∞–ª–∞ –≤ —Å–∞–º—ã–π –≤–µ—Ä—Ö–Ω–∏–π —Å–ª–æ–π
        sub_network_outputs = {
            # –ú—ã –≤—Å–µ–≥–¥–∞ –ø–æ–¥–∞–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫ embedding_inputs –≤ –ø–æ–¥—Å–µ—Ç–∏. –≠—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ª—é–±–∞—è
            # –ø–æ–¥—Å–µ—Ç—å –¥–æ–ª–∂–Ω–∞ –æ–ø–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º –∂–µ feature_holder'–æ–º
            name: module(embedding_inputs)
            for name, module in self.sub_networks.items()
        }

        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤—ã–≤–∞–µ–º –≤—Ö–æ–¥—ã. –ù–æ—Ä–º–∞–ª–∞–π–∑–µ—Ä–æ–≤ –Ω–∞ –ø–æ–¥—Å–µ—Ç–∏ –Ω–µ—Ç - —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ,
        # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ –ø–æ–¥—Å–µ—Ç–∏
        inputs = []
        for name, idx in zip(self.features, self.feature_holder.get_ids(self.features)):
            tensor = embedding_inputs[idx]
            if name in self.normalizers:
                tensor = self.normalizers[name](tensor)
            inputs.append(tensor)

        # –Æ–∑–µ—Ä –¥–µ–ª–∞–µ—Ç —Å —ç—Ç–∏–º –¥–æ–±—Ä–æ–º –≤—Å–µ —á—Ç–æ —Ö–æ—á–µ—Ç
        return self._forward_impl(sub_network_outputs=sub_network_outputs, normalized_inputs=inputs)

    def _forward_impl(self, sub_network_outputs, normalized_inputs):
        raise NotImplementedError
```

–û—Ç–º–µ—Ç–∏–º, —á—Ç–æ –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å —Å —Ç–∞–∫–æ–π –≤–æ—Ç –ª–æ–≥–∏–∫–æ–π –º—ã —Ä–µ—à–∏–ª–∏ –Ω–µ –¥–µ–ª–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ —Å—Ä–∞–∑—É –Ω–∞—á–∏–Ω–∞–ª–∏ —Å—ã–ø–∞—Ç—å—Å—è –≤—Å—è–∫–∏–µ –∫—Ä–∞–µ–≤—ã–µ —Å–ª—É—á–∞–∏. –û—Å—Ç–∞–≤–ª—è–µ–º —ç—Ç–æ –ø—Ä–∞–≤–∏–ª–æ –Ω–∞ –æ—Ç–∫—É–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é. –í ads_pytorch –µ—Å—Ç—å –Ω–µ–∫–∏–π –Ω–∞–±–æ—Ä "–æ–±—â–∏—Ö" –ø–æ–¥—Å–µ—Ç–µ–π, –≥–¥–µ —Ç–∞–∫–∞—è –ª–æ–≥–∏–∫–∞ —Å—Ç—Ä–æ–≥–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∏ —Ä–µ–≤—å—é–∏—Ç—Å—è. –í —Å–≤–æ–∏—Ö —Å–ª–æ—è—Ö –º–æ–∂–µ—Ç–µ –¥–µ–ª–∞—Ç—å —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ.

–ë–æ–µ–≤–æ–π –ø—Ä–∏–º–µ—Ä: fully-connected —Å–µ—Ç—å —Å –ø–æ–¥—Å–µ—Ç—å—é-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º
```(python)
import torch
from typing import List, Dict
from ads_pytorch.nn.module.base_embedding_model import FeatureOrderHolder


class FeedForward(torch.nn.Module):
    def __init__(
        self,
        feature_holder: FeatureOrderHolder,
        features: List[str],
        normalizers: Dict[str, torch.nn.Module],
        sub_networks: Dict[str, torch.nn.Module],
        deep: torch.nn.Module
    ):
        super(FeedForward, self).__init__()
        self.normalizers = torch.nn.ModuleDict(normalizers)
        self.sub_networks = torch.nn.ModuleDict(sub_networks)
        # –ó–∞—Ä–∞–Ω–µ–µ —Å–µ–ª–µ–∫—Ç–∏–º —Å–µ–±–µ –ø–æ—Ä—è–¥–æ–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ–º –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥—Å–µ—Ç–∏
        # –í "–±–æ–µ–≤—ã—Ö" –∞–Ω–∞–ª–æ–≥–∞—Ö feed-forward —Å–µ—Ç–µ–π —ç—Ç–æ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ state_dict
        # –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–¥–∞–µ—Ç –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä —Å–ª–æ–≤–∞—Ä–∏–∫
        # —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–ª—é—á–µ–π (–≤ PY3.6+ –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ ordered) –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        # –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –±—ã–ª –¥—Ä—É–≥–æ–π
        # –í –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã –æ–ø—É—Å—Ç–∏–º –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π state_dict/load_state_dict
        self.sub_networks_order = list(self.sub_networks.keys())
        self.feature_holder = feature_holder
        self.features = features[:]
        self.deep = deep

    def forward(self, embedding_inputs: List[torch.Tensor]) -> torch.Tensor:
        sub_network_outputs = {
            name: module(embedding_inputs)
            for name, module in self.sub_networks.items()
        }

        inputs = []
        for name, idx in zip(self.features, self.feature_holder.get_ids(self.features)):
            tensor = embedding_inputs[idx]
            if name in self.normalizers:
                tensor = self.normalizers[name](tensor)
            inputs.append(tensor)

        deep_input = torch.cat(inputs + [sub_network_outputs[name] for name in self.sub_networks_order], dim=1)
        return self.deep(deep_input)


class TransformerSubnetwork(torch.nn.Module):
    def __init__(
        self,
        feature_holder: FeatureOrderHolder,
        features: List[str],
        normalizers: Dict[str, torch.nn.Module],
        sub_networks: Dict[str, torch.nn.Module],
        encoder_impl: torch.nn.TransformerEncoder
    ):
        super(TransformerSubnetwork, self).__init__()
        assert len(sub_networks) == 0
        self.normalizers = torch.nn.ModuleDict(normalizers)
        self.features = features[:]
        self.feature_holder = feature_holder
        self.encoder_impl = encoder_impl

    def forward(self, embedding_inputs: List[torch.Tensor]):
        inputs = []
        for name, idx in zip(self.features, self.feature_holder.get_ids(self.features)):
            tensor = embedding_inputs[idx]
            if name in self.normalizers:
                tensor = self.normalizers[name](tensor)
            inputs.append(tensor)

        # –í –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –≤–∏–¥–∞ "–∞ –µ—Å–ª–∏ –ø—É—Å—Ç–æ—Ç–∞", "–∞ –µ—Å–ª–∏ –º–∞—Å–∫–∞" –∏ —Ç.–¥.
        transformer_input = torch.cat(inputs, dim=2)  # 3-dim —Ç–µ–Ω–∑–æ—Ä
        transformer_output = self.encoder_impl(transformer_input)
        return torch.mean(transformer_output, dim=0)  # average along sequence_len dimension


if __name__ == '__main__':
    # –ó–∞–±—å–µ–º –Ω–∞ BaseEmbeddingModel
    feature_holder = FeatureOrderHolder({"SearchQueryHistory": 0, "UserID": 1, "BannerID": 2})

    transformer = TransformerSubnetwork(
        feature_holder=feature_holder,
        features=["SearchQueryHistory"],
        normalizers={},
        sub_networks={},
        encoder_impl=torch.nn.TransformerEncoder(
            encoder_layer=torch.nn.TransformerEncoderLayer(
                d_model=512,
                nhead=8
            ),
            num_layers=10
        )
    )

    feed_forward = FeedForward(
        feature_holder=feature_holder,
        features=[
            "UserID",
            "BannerID"
        ],
        normalizers={},
        sub_networks={
            "query_history": transformer
        },
        deep=torch.nn.Linear(512 + 64 + 64, 1)
    )

    inputs = [
        # torch.nn.TransformerEncoder accepts (sequence_len, batch_size, embedding_dim)
        # –≤ ads_pytorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ—Ä—É—Ç (batch_size, sequence_len, embedding_dim)
        torch.randn(50, 10, 512),  # SearchQueryHistory
        torch.randn(10, 64),       # UserID
        torch.randn(10, 64)        # BannerID
    ]

    print(feed_forward(inputs))
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–±—Ä–∏–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏–∑ json –¥–µ–ª–∞–µ—Ç—Å—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ—Ç —Å–∞–º–æ–≥–æ –≤–µ—Ä—Ö–Ω–µ–≥–æ —Å–ª–æ—è –∫ –Ω–∏–∂–Ω–∏–º. [–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –Ω–æ–≤—ã–º–∏ —Ñ–∞–±—Ä–∏–∫–∞–º–∏](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2). –û–±—â–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–ø–∏—Å–∞–Ω—ã —Ç—É—Ç:
* [network_factory.py](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2/network_factory.py) - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∏ –ø–æ–¥—Å–µ—Ç–µ–π
* [normalizer_factory.py](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2/normalizer_factory.py) - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∞–π–∑–µ—Ä–æ–≤

–§–∞–±—Ä–∏–∫–∏ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞—é—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –Ω–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ—é —Ñ–∞–±—Ä–∏—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏ –æ–±–µ—Ä–Ω—É—Ç—å –µ–µ –≤ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä, –≤–æ—Ç —Ç–∞–∫
```(python)
from densenet_tsar_query_attention_v2.normalizer_factory import register_normalizer_factory
from densenet_tsar_query_attention_v2.network_factory import register_network_factory

@register_normalizer_factory("dummy_normalizer_identity")
def _build_identity_normalizer(cfg: Dict[str, Any]):
    return torch.nn.Identity()


@register_network_factory("dummy_linear")
def generic_feed_forward_builder_impl(
    cfg: Dict[str, Any],
    sub_networks: Dict[str, torch.nn.Module],
    normalizers: Dict[str, torch.nn.Module],
    embedding_descriptors: List[EmbeddingDescriptor],
    feature_order_holder,
    unique_prefix: str
) -> FeedForward:
    return FeedForward(
        feature_order_holder=feature_order_holder,
        sub_networks=sub_networks,
        normalizers=normalizers,
        features=cfg["features"],
        deep=torch.nn.Linear(1000, 1)
    )
```

–ò –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ—Ç–æ–º —ç—Ç–∏ —Å–µ—Ç–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
```(json)
{
    "user": {
        "network_type": "dummy_linear",
        "features": [
            "UserID",
            "UserSocDem",
            "UserTopCategories",
            "CountersAggregated"
        ],
        "normalizers": {
            "CountersAggregated":{
                "type": "dummy_normalizer_identity"
            }
        },
        "sub_networks": {
            "QueryTransformer": {
                "network_type": "transformer_encoder",
                "embeddings": [
                    "QueryTexts"
                ]
            }
        }
    }
}
```

–ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ - —Ñ–∞–±—Ä–∏–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã –±—É–¥–µ—Ç–µ —á–∏—Ç–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥. –û–±—ã—á–Ω–æ —Ç—É—Ç –¥–≤–∞ —Å–ª—É—á–∞—è:
1. –ë–∏–±–ª–∏–æ—Ç–µ—á–Ω—ã–π —Å–ª–æ–π, –ø–æ–∫—Ä—ã—Ç—ã–π —Ç–µ—Å—Ç–∞–º–∏ –∏ –¥–æ–≤–æ–ª—å–Ω–æ –æ–±—â–∏–π, –æ–Ω –ø–∏—à–µ—Ç—Å—è –≤ —Å–∞–º—É –±–∏–±–ª–∏–æ—Ç–µ–∫—É ```densenet_tsar_query_attention_v2``` –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≤ ```__init__.py```
2. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Å–ª–æ–π, –æ–ø–∏—Å–∞–Ω–Ω—ã–π –ø—Ä—è–º–æ –≤ —Å–∫—Ä–∏–ø—Ç–µ –æ–±—É—á–µ–Ω–∏—è. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –∑–∞–ø—É—Å–∫–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–æ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª—å–∫–∏

### –ë–∏–±–ª–∏–æ—Ç–µ—á–Ω—ã–µ —Å–ª–æ–∏
–ó–¥–µ—Å—å –æ–ø–∏—Å–∞–Ω—ã *–≤–∞–∂–Ω—ã–µ* —Å–ª–æ–∏, –ø–æ–∫—Ä—ã—Ç—ã–µ —é–Ω–∏—Ç —Ç–µ—Å—Ç–∞–º–∏ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ ads_pytorch –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–ª—è —ç—Ç–∏—Ö —Å–ª–æ–µ–≤

#### [GenericFeedForwardWithNormalizers](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2/generic_feed_forward_with_normalizers.py?rev=r8036830)
–û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π feed-forward –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é. –ü–æ–¥ feed-forward –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –∑–¥–µ—Å—å –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç—Å—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å –æ–¥–Ω–∏–º —Ç–µ–Ω–∑–æ—Ä–æ–º –Ω–∞ –≤—Ö–æ–¥ –∏ –æ–¥–Ω–∏–º –Ω–∞ –≤—ã—Ö–æ–¥, –ø—Ä–∏—á–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä **–¥–≤—É–º–µ—Ä–Ω—ã–π**.

–í –ø—Ä–∏–Ω—Ü–∏–ø–µ, –Ω–∏—á–µ–≥–æ —Ü–µ–Ω–Ω–æ–≥–æ, –∫—Ä–æ–º–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, —Ç–∞–º –Ω–µ—Ç. –í—Å–µ sub_networks –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞–º–∏ [SingleMatrixOutputMixin](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/ads_pytorch/ads_pytorch/nn/module/single_matrix_output_network.py), –≥–æ–≤–æ—Ä—è—â–µ–π, —á—Ç–æ:
1. –í–∞—à —Å–ª–æ–π –≤—ã–¥–∞–µ—Ç –æ–¥–Ω—É –º–∞—Ç—Ä–∏—Ü—É –Ω–∞ –≤—ã—Ö–æ–¥–µ
2. –£ —Å–ª–æ—è –µ—Å—Ç—å –º–µ—Ç–æ–¥, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –º–æ–∂–Ω–æ –∑–∞—Ä–∞–Ω–µ–µ —É–∑–Ω–∞—Ç—å —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –≤—ã—Ö–æ–¥–µ

–≠—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–Ω—Ñ–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è fully-connected —Å–µ—Ç–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ feed-forward —Å–µ—Ç–∏, –µ—Å—Ç—å [–¥–≤–∞ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ—è](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2/generic_feed_forward_factory.py?rev=r8036830#L52):
* DenseNetEmbeddingNetwork - —Ö–æ—Ä–æ—à–æ –∑–Ω–∞–∫–æ–º—ã–µ concat-—Å–ª–æ–∏ –∏–∑ DSSM.
* WeightNormalizedEmbeddingNetwork - —É–ª—É—á—à–µ–Ω–∏–µ DenseNet —Å–µ—Ç–∏, –ø–æ–∑–≤–æ–ª—è—é—â–µ–µ —É—á–∏—Ç—å –Ω–æ—Ä–º—É –≤–µ–∫—Ç–æ—Ä–∞ –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ. –î–∞–µ—Ç –±–æ–ª—å—à–µ –ø—Ä–æ—Ñ–∏—Ç–∞, —á–µ–º –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –ø–æ —á–∏—Å–ª—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DenseNet —Å–µ—Ç—å –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –∑–∞–¥–∞—á. **–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–µ—Ñ–æ–ª—Ç–æ–º**

[–ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥–∞](https://a.yandex-team.ru/arc/trunk/arcadia/junk/alxmopo3ov/ad_transformer_v2/warmup_without_deep_part_long/config.json?rev=r7915854#L234)

#### [GenericFeedForwardWithHeads](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2/generic_feed_forward_with_normalizers.py?rev=r8409022#L196)
–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ GenericFeedForwardWithNormalizers —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–π —Å–µ—Ç–∏.
–ù–∞ –≤—Ö–æ–¥ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è shared_deep - –æ–±—â–∞—è –¥–ª—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≥–ª—É–±–æ–∫–∞ —á–∞—Å—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–ª–∂–Ω–∞ —è–≤–ª—è—Ç—å—Å—è SingleMatrixOutputMixin,
–≤—Ö–æ–¥ —É –Ω–µ–µ –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω GenericFeedForwardWithNormalizers - –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –æ–±—â–µ–π —á–∞—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–æ–± —ç—Ç–æ–º –¥–∞–ª–µ–µ) –∏ –≤—ã—Ö–æ–¥–æ–≤ –ø–æ–¥—Å–µ—Ç–µ–π.

–ü–æ–ª—É—á–∏–≤—à–∞—è—Å—è –¥–≤—É–º–µ—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ—Å–ª–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π –ø–æ–¥–∞–µ—Ç—Å—è –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª—è–º-–≥–æ–ª–æ–≤–∞–º.
* –ü–µ—Ä–≤–∞—è –≤–æ–∑–º–æ–∂–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–æ–ª—å–∫–æ –≤ –º–æ–¥–µ–ª–∏-–≥–æ–ª–æ–≤–µ, —á—Ç–æ–±—ã —ç—Ç–∞ —á–∞—Å—Ç—å –≤—ã—É—á–∏–≤–∞–ª–∞ –∫–∞–∫–æ–µ-—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.
–î–ª—è —ç—Ç–æ–≥–æ –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –æ–±—â—É—é –≥–ª—É–±–æ–∫—É—é –º–æ–¥–µ–ª—å —á–∞—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –æ—Ç–¥–µ–ª—è–µ—Ç—Å—è –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ–±—â–µ–π –≥–ª—É–±–æ–∫–æ–π –º–æ–¥–µ–ª–∏,
–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç—Å—è —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å-–≥–æ–ª–æ–≤—É.
–†–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è —ç—Ç–æ —Ñ–ª–∞–≥–æ–º "split_embeddings" –≤ –∫–æ–Ω—Ñ–∏–≥–µ –∏ –ø–æ–ª–µ–º "embedding_ratio" –≤ –∫–æ–Ω—Ñ–∏–≥–µ –≥–æ–ª–æ–≤—ã,
–∫–æ—Ç–æ—Ä–æ–µ –≥–æ–≤–æ—Ä–∏—Ç –∫–∞–∫—É—é —á–∞—Å—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç–¥–µ–ª—è—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –≥–æ–ª–æ–≤–µ.
* –í—Ç–æ—Ä–∞—è –≤–æ–∑–º–æ–∂–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è - —Å–∂–∞—Ç–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π. –û–Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞ –¥–ª—è –¥–µ–Ω—Å–Ω–µ—Ç–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—â–µ–π –≥–ª—É–±–æ–∫–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–¥–µ–ª–∏—Ç—å –æ—Ç –≤—ã—Ö–æ–¥–∞ –¥–µ–Ω—Å–Ω–µ—Ç–∞ –µ–≥–æ –≤—Ö–æ–¥,
—Å–∂–∞—Ç—å —ç—Ç–æ—Ç –≤—Ö–æ–¥ –∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ, —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —É–º–µ–Ω—å—à–∞—è —à–∏—Ä–∏–Ω—É —Å–ª–æ–µ–≤ –≤ –º–æ–¥–µ–ª–∏-–≥–æ–ª–æ–≤–µ. –ó–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç "input_compression_impl" –≤ –∫–æ–Ω—Ñ–∏–≥–µ –≥–æ–ª–æ–≤—ã.

[–ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥–∞](https://a.yandex-team.ru/arc/trunk/arcadia/junk/ya-philya/multitarget/pclick_conv/splitted_models_splitted_embeds_conf.json?rev=r8407110#L499)

#### [MultisequenceTransformerEncoder2](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/densenet_tsar_query_attention_v2/densenet_tsar_query_attention_v2/multisequence_transformer_encoder2.py)
–û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ads_pytorch. MultiSequence –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ—Ç —Å–ª–æ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –≤ –æ–¥–Ω—É "–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å" —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–º —Å–º—ã—Å–ª–µ —ç—Ç–æ –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç –±—ã—Ç—å, –Ω–æ –≤–µ–¥—å –º—ã –≤—Å–µ –∑–Ω–∞–µ–º, —á—Ç–æ transformer encoder - —Ç—É–ª–∑–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ –Ω–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º, –∞ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω–æ—Å—Ç—å, "–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–Ω–æ—Å—Ç—å" –∏ —Ç.–¥. –≤–≤–æ–¥–∏—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ö–∞–∫–∞–º–∏.

–ù–∞—á–Ω–µ–º —Å –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ([—Ç–∞–±–ª–∏—Ü–∞ —Å —Ñ–∏—á–∞–º–∏](https://yt.yandex-team.ru/hahn/navigation?path=//home/ads/tzar/docs/plain_sum_multisequence_transformer_encoder2_history_keys)):
```json
"sub_networks": {
    "NeuralCounterByAdGroup": {
        "network_type": "multisequence_transformer_encoder2",
        "transformer_type": "ads_pytorch",
        "attention": "linear_symmetric",
        "linear_type": "orthogonal",
        "d_model": 32,
        "num_layers": 3,

        "sub_networks": {
            "UserFeatures": {
                "embeddings": [
                    "HistoryKeys"
                ],
                "realvalue": {
                    "HistoryAge": 1,
                    "HistoryClicks": 1,
                },
                "key_mask": "HistoryLength",
                "max_seq_len": 128,
                "combine_type": "plain_sum",
                "network_type": "multisequence_tranformer_input_sequence_builder"
            }
        },
    }
}
```

–í–æ-–ø–µ—Ä–≤—ã—Ö - **–≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤ sub_networks**. –ö–∞–∂–¥–∞—è sub_network –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –≤—ã–¥–∞–≤–∞—Ç—å –æ–¥–∏–Ω —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ (batch_size, sequence_len, embedding_dim). –í –ø–æ–¥–∞–≤–ª—è—é—â–µ–º –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –≤–∞–º —Å—Ç–æ–∏—Ç —é–∑–∞—Ç—å **multisequence_tranformer_input_sequence_builder** –∏ –±—Ä–∞—Ç—å –∏–º–µ—é—â–∏–µ—Å—è **combine_type** –ª–∏–±–æ, –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ, –¥–µ–ª–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ combine_type. –° –≤—ã—Ö–æ–¥–∞–º–∏ –≤—Å–µ—Ö sub_network –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª–µ–¥—É—é—â–µ–µ:
1. –î–µ–ª–∞–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ ```d_model```
2. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 1, –æ—Ç–≤–µ—á–∞—é—â–µ–π –∑–∞ –Ω–æ–º–µ—Ä —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
3. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è ```key_mask``` –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
4. –û–¥–Ω–∞ —Å–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–æ–ª—å—à–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–æ–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ transformer encoder
5. –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —É—Å—Ä–µ–¥–Ω—è–µ—Ç—Å—è —Å —É—á–µ—Ç–æ–º ```key_mask``` –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 1 (—ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏). –ù–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É

multisequence_transformer_encoder2 —è–≤–ª—è–µ—Ç—Å—è –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–æ–º SingleMatrixOutputMixin, –∞ –∑–Ω–∞—á–∏—Ç - –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ –ø–æ–¥—Å–µ—Ç—å –¥–ª—è GenericFeedForward –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.

##### key_mask

```key_mask``` - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É—á–∞—Å—Ç–≤—É—é—â–∏—Ö –≤ Attention'–µ –∫–ª—é—á–µ–π. –ù–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π —Å–ª—É—á–∞–π - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ zero padding'–∞. –í –æ–¥–Ω–æ–º –±–∞—Ç—á–µ, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã, –ø–æ—ç—Ç–æ–º—É —á—Ç–æ–±—ã –¥–æ–±–∏—Ç—å –∏—Ö –¥–æ –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∑–∞–ø–∏—Ö–Ω—É—Ç—å –≤ –æ–¥–∏–Ω —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä, "—Ö–≤–æ—Å—Ç—ã" –ø—Ä–æ—Å—Ç–æ –∑–∞–±–∏–≤–∞—é—Ç –Ω—É–ª—è–º–∏. –≠—Ç–∏ –Ω—É–ª–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Ä–∞—Å—á–µ—Ç–µ –º–æ–¥–µ–ª–∏, –¥–∞–±—ã –Ω–µ –ø–æ—Ä—Ç–∏—Ç—å

##### –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã TransformerEncoder
–ó–∞ –≤—ã–±–æ—Ä —Ç–∏–ø–∞ TransformerEncoder'–∞ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ–ª–µ ```"transformer_type"```.

###### torch
–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π torch.nn.TransformerEncoder. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
* ```d_model``` - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ
* ```num_layers``` - —á–∏—Å–ª–æ —Å–ª–æ–µ–≤
* ```num_heads``` - —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π, –¥–µ—Ñ–ª–æ—Ç 1)
* ```dim_feedforward``` - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –º–µ–∂–¥—É attention'–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, —Ç–æ –±—É–¥–µ—Ç —Ä–∞–≤–µ–Ω d_model)

###### ads_pytorch
[ReZeroTransformerEncoder](https://arxiv.org/abs/2003.04887) —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –≤—ã–±–æ—Ä–æ–º Attention'–æ–≤. –°–µ–π—á–∞—Å –ø–æ–¥–¥–µ—Ä–∂–∞–Ω—ã —Ç—Ä–∏ –≤–∏–¥–∞:
1. [SoftmaxAttention](https://arxiv.org/abs/1706.03762), ```"attention_type": "softmax"```
2. [LinearAttention](https://arxiv.org/abs/2006.16236), ```"attention_type": "linear"```
3. [LinearSymmetricAttention](https://st.yandex-team.ru/TORCHPS-405#6075df4ee98305599d6e119a), ```"attention_type": "linear_symmetric"```

–ü–æ–∫–∞ –Ω–µ –Ω–∞–±–ª—é–¥–∞–ª–æ—Å—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª—É—á–∞—è, —á—Ç–æ–±—ã linear_symmetric –ø—Ä–æ–∏–≥—Ä–∞–ª –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—ã—á–Ω–æ–º—É linear, –Ω–æ –∑–∞—Ç–æ –æ–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–æ –ø–∞–º—è—Ç–∏. –ü—Ä–∏ –≤—ã–±–æ—Ä–µ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ –µ–≥–æ.

–ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
* ```linear_type``` - —Ç–∏–ø –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è. –ú–æ–∂–µ—Ç –±—ã—Ç—å –ª–∏–±–æ "orthogonal" - [OrthogonalLinear](https://a.yandex-team.ru/arc/trunk/arcadia/ads/pytorch/packages/ads_pytorch/ads_pytorch/nn/module/orthogonal_linear.py), –ª–∏–±–æ "usual" - ```torch.nn.Linear```

–û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã - d_model, d_feedforward, num_layers, num_heads - –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º torch —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.

###### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π TransformerEncoder
```json
{
    "transformer_type": "ads_pytorch",
    "attention": "linear_symmetric",
    "linear_type": "orthogonal"
}
```

##### –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ multisequence_tranformer_input_sequence_builder
–ó–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä ```"combine_type"```. –î–ª—è **–≤—Å–µ—Ö** –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∏–∂–µ –¥–∞–≤–∞–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ "–≤—Ö–æ–¥—ã":

```python
import torch
batch_size = 10
seq_len = 256
dim = 64

embeddings = {
    "HistoryKeys": torch.rand(batch_size, seq_len, dim),
    "HistoryPositionalEmbeddings": torch.rand(batch_size, seq_len, dim),
}
realvalue = {
    "HistoryAge": torch.rand(batch_size, seq_len, 1),
    "HistoryStatistics": torch.rand(batch_size, 17)
}
```

–∞ —Ç–∞–∫–∂–µ —Å–ª–µ–¥—É—é—â—É—é —á–∞—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥–∞, **–æ–±—â—É—é** –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∏–∂–µ
```json
{
    "embeddings": [
        "HistoryKeys",
        "HistoryPositionalEmbeddings"
    ],
    "realvalue": {
        "HistoryAge": 1,
        "HistoryStatistics": 17
    },
    "network_type": "multisequence_tranformer_input_sequence_builder"
}
```

###### plain_sum
–°—É–º–º–∏—Ä—É–µ—Ç –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∑–∞—Ç–µ–º –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç –∫ –Ω–∏–º realvalue —Ñ–∏—á–∏.

```python
embedding_tensor = sum([
    embeddings["HistoryKeys"],
    embeddings["HistoryPositionalEmbeddings"],
])
result = torch.cat([embedding_tensor] + [realvalue["HistoryAge"], realvalue["HistoryStatistics"]], dim=1)
```

###### "densenet"
Feed-forward –Ω–µ–π—Ä–æ—Å–µ—Ç—å. Feed-forward –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å **—Ç–æ–ª—å–∫–æ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏**.

```python
from ads_pytorch.nn.module.densenet import DenseNet
densenet = DenseNetEmbeddingNetwork()

inputs = torch.cat(list(embeddings.values()) + list(realvalue.values()), dim=1)
result = densenet(inputs)
```

–ï—Å—Ç—å –µ—â–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ "weightnorm_net" - –æ–Ω–∞ –∂—Ä–µ—Ç **—Å–∏–ª—å–Ω–æ** –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –∏ –Ω–µ –¥–∞–µ—Ç –ø—Ä–æ—Ñ–∏—Ç–∞ –ø–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º. WeightNormalizedEmbeddingNetwork –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞–≤–µ—Ä—Ö—É DSSM'–∞.

###### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
–ï—Å–ª–∏ —É –≤–∞—Å **–Ω–µ–º–Ω–æ–≥–æ** —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Å–∫–∞–∂–µ–º, <= 2), —Ç–æ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ```plain_sum```. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ realvalue –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –∏–º–µ–µ—Ç. –í –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö - —Å–µ—Ä–µ–±—Ä—è–Ω–æ–π –ø—É–ª–∏ –Ω–µ—Ç, –Ω—É–∂–Ω–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å –∏ densenet, –∏ plain_sum, –∏ –≤—ã–±–∏—Ä–∞—Ç—å –ª—É—á—à–∏–π.
