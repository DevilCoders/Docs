# Инфра кросс валидации прогноза

## Концепция

Проект для прогнозирования CDF спроса.
Для нужд оптимизации пополнения (попадания в высокую доступность или низкую оборачиваемость) требуется более детальный промер распределения прогноза как случайной величины.

Гуглить по
* Probabilistic forecasting
* Implicit Quantile Networks
* Stochastic optimization
* Simulation-based optimization

Метриками качества для такого прогноза будем считать рассчитанные по упрощенной схеме метрики репленишмента

https://wiki.yandex-team.ru/users/astepanel/proksi-metriki-replenishmenta/

https://wiki.yandex-team.ru/users/astepanel/proksi-metriki-replenishmenta/ocenka-kachestva-prognoza-cherez-proksi-metriki-replenishmenta/

## Основы

### Глоссарий
* msku — она же market_sku, единое обозначение для карточки товара на маркете,
   в отличие от ssku не завязана на поставщика. Прогноз должен происходить на уровне msku
* forecasted_date — дата факта, единое обозначение для даты в которое произошло то или иное событие или агрегированый факт,
  например, количество продаж, или средняя выставленная цена
* forecast_date — дата составления прогноза, может быть явно использована для таких фактов как корзины на
  момент составления прогноза, или ценовые стратегии на момент составления прогноза. forecast_date > date
* horizon - разница в днях между forecasted_date и forecast_date
* таргет/target — датасет, содержащий ключ (может варьироваться от реализации) и значение предсказуемой метрики.
К таргету джоинятся все фичи лефт джоином.
* фича/feature — датасет, содержащий подмножество ключей таргета и значение (или несколько) какого то признака.
* таска — реализация интерфейса market.monetize.stapler.v1.tasks.task.Task которая транслируется степлером в кубик нирваны. Может содержать как произвольный python код, так и nile/spark/yql/chyt код.
* степлер — фреймворк для построения графов в нирване, на основе аркадийного кода подробнее https://wiki.yandex-team.ru/users/arakhmatulin/sozdanie-grafov-s-pomoshhju-monetizationutils/
* ground_truth - эталонный датасет для кросс валидации, собираемый независимо от модели

### Структура ground_truth датасета
* warehouse_id - ид скдала (только фулфилмент склады)
* msku - msku
* date - дата факта, соответствует forecast_date
* restored_demand - оформленные 1р продажи msku, предполагаемые к отгрузке со склада warehouse_id, с заполнением промежутков OOS

#### Алгоритм заполнения OOS

Основным алгоритмом заполнения является сглаженное скользящее среднее, параметризованное шириной окна (lags) далее WMA

Допустим мы хотим восстановить спрос в точке t.

Для этого мы берем все t' такие что t-t' входят в lags и спрос (demand) в t' считается известным.

Тогда восстановленный спрос в t = сумма по t' (demand(t') * (t'- t)**2) / сумма по t' ((t'- t)**2)

Т.е. в качестве веса при сглаживании берется квадрат разницы в днях между восстанавливаемой точкой и соседней точкой из lags

Помимо этого используется алгоритм возвращения в датасет примечательных точек.

А именно:
1. Формируется датасет warehouse_id, msku, date, sales
2. Флагом known_demand размечаются точки где не было oos, скрытий, автораспродаж
3. Считается baseline алгоритмом WMA с lags [-56..-1], спрос считаем известным по **known_demand** т.е. спрос оценивается по 56 дням в прошлом.
4. Флагом well_known_demand размечаются точки с known_demand либо точки где были oos либо скрытия но sales>baseline
5. Таким образом мы возвращаем в датасет точки, где несмотря на скрытия/oos спрос был выше среднего, таким образом мы вовзращаем точки, где происходило выпадение в OOS в период промо.
6. Считается baseline алгоритмом WMA с lags [-56..-1], спрос считаем известным по **well_known_demand**, т.е. спрос оценивается по 56 дням в прошлом с возвращением дней аномалньо высокого спроса. Эту величину можно использовать в фичах модели, т.к. она не содержит ликов.
7. Считаем restored_demand алгоритмом WMA с lags [-56..-1] + [1..56], спрос считаем известным по **well_known_demand**, т.е. спрос оцениваем как по 56 дням в прошлое, так и по 56 дням в будущее. Эту величину нельзя использовать в фичах, т.к. она содержит лик, зато можно использовать для валидации моделей, т.к. она будет давать более качественное заполнение пропусков.
8. Считаем missed_orders (упущенные продажи)
* если well_known_demand=True то 0 - Если спрос известен, то упущенных продаж нет
* иначе если restored_demand<sales то sales*0.05 - Если спрос полностью неизвестен, но наша оценка все равно ниже факта, то считаем упущенные как факт + 5% "для градиента"
* иначе restored_demand-sales - Если спрос неизвестен, но восстановленый спрос выше факта то возьмем дельту между ними
9. Для валидации и других оценок будем использовать в качестве восстановленного спроса величину sales+missed_orders


## Основные классы/общие

* AbstractCdfModel - класс, который должен реализовать каждый алгоритм для кросс валидации, подробнее в документации класса.
* CdfTrainTask - таска, принимающая в конструкторе экземпляр AbstractCdfModel и строящая кубик обучения соответствующей модели, возвращает путь до обученной модели
* CdfValidateTask - таска, принимающая в конструкторе экземпляр AbstractCdfModel а так же путь до обученной модели, путь до валидационного датасета и даты, обозначающие гранцы фолдов, возвращает путь до детальной кросс валидации. Подробнее см

## Алгоритмы, основанные на катбустах и их классы

Суть - конечным количествам катбустов промеряем конечное количество квантилей и далее их складываем в CDF разными алгоритмами

Основной алгоритм - линейная интерполяция cdf и сумма семплированием
1. Предсказываем отдельные квантили на каждый день
2. Линейно интерполируем промежутки между квантилями, получаем инверс CDF
3. Случайно чемплируем из полученых CDF большое количество семплов для каждого дня
4. В рамках семпла суммируем дни, таким образом получаем выборку суммы, из нее можно получать cdf суммы

Классы в порядке наследования
* AbstractCatboostMultiQuantileModel - абстрактный класс, умеющий обучать множество катбустов для разных квантилей, при том распределенно-параллельно
* AbstractSampleCdfModel - абстрактный класс, строящий cdf на основе семплирования (необходимо определить в потомках)
* AbstractCatboostCdfLinearInterplatedModel - абстрактный класс, реализующий семплирование, через семплирование из линейно интерполированных замеров отдельных квантилей (инверс CDF)
* AbstractCatboostLinearInterpolateMultModel - абстрактный класс, реализующий преобразование таргета "деление на бейзлайн", т.е. фактически катбусты обучаются на отношение к фиче "бейзлайн"
* *CatboostLinearInterpolateMultModel - конкретные классы-реализации, минимально требуют реализации метода get_features, т.е. определения списка фичей

## Предусмотреные сценарии

### Добавление фичи
Для добавления фичи необходимо реализовать таск сборки этой фичи.
Для этого необходимо
1. **UNDER CONSTRUCTION** Модифицировать код датасета, чтобы он начал содержать новую фичу
2. Реализовать класс модели, содержащий новую фичу, например отнаследовавшись от AbstractCatboostLognormCdfMultModel, см пример SimpleCatboostLinearInterpolateMultModel
* ```python
  class NewCoolModel(AbstractCatboostCdfLinearInterplatedModel):
      def get_features(self) -> List[str]:
          return [..., 'new_cool_feature']
  ```
4. Добавить в cli.py генерацию трейн и валидейт таски
* ```python
   cdfModels = [
    model(target='sales', distribute_fit=True, catboost_hyperparams={'iterations': 100})
    for model in [
        NewCoolModel,
        ...
    ]
   ]
   ```
5. Запустить `run.sh --command train_cdf`
