Создать проект для идеи:
```
bin/create-idea-project-with-libs.sh
```

Термины
=======
- тип задания - виды заданий, модерация маппингов, синие логи, ...
- группа тасков, потребитель - система-источник + тип задания, т.е. хочу я в КИ работать с модерацией - это один потребитель, штука дорогая по ресурсам, не планируем сотни
- таск - единица задания

На всякий случай различные аспекты архитектуры:
===============================================

Основная задача - раздача задач операторам.

Чиселки, в которые целимся:
- 1М активных заданий
- 100 RPS в прыжке сделанных заданий, 10 в среднем
- несколько десятков типов заданий
(это ориентир, пока никаких стрельб/специальных расчётов не делаем)

Что планируется:
----------------
Техническое:
- мульти-дц-шность (ДЦ -1)
- достаточно быстрый старт (не 20+ минут)
- push заданий, pull результатов потребителями, как default модель (но, в качестве исключения, часть текущих процессов КИ будет завёрнута по pull+push, как отдельный слой внутри, в перспективе можно отселить и это не должно мешать переиспользовать)

Продуктово:
- переиспользование заданий разными потребителями
- переиспользование биллинга, но не как самоцель (сейчас), как следствие переиспользования заданий
- системный контроль дублей (дубли это больно) - можно не использовать

Что **не планируется**:
-----------------------
Технически **не планируется**:
- шардирование - простой вариант вида отселить такие-то задания (или связки задание-потребитель) на отдельную группу инстансов/БД, кажется, просто будет добавить, если понадобится. Более серьёзное сейчас - не планируется
- тяжелые CPU-bound задания - выполнение идёт внутри одного процесса, тяжёлые вещи - ок, можно, грубо говоря, шагом задания запустить sandbox таску и подождать
- длинные шаги - целимся в секунды на шаг, до нескольких минут, длительные обработки придётся куда-то выносить
- использовать YT или YDB, на данном этапе речь строго про одну базу PG
- держать десятки и сотни миллионов открытых заданий
- контроль доступов между потребителями, на данном шаге

Продуктово **не планируется**:
- настройки пайплайнов процессов заданий через UI (а-ля Нирвана) - шаги и взаимодействия строго прописаны в коде
- управлять приоритезацией внутри заданий через UI. Приоритезация заданий друг относительно друга - это знания приложения. Грубо говоря, клиент передаёт свою циферку-приоритет заданию, оно с ним выдаёт задание в Янг/Толоку
- управлять приоритезацией между разными потребителями - по сути делаем, чтобы они выдавались в разные пулы в Янге, дальше приоритезация орг. средствами на данном этапе. Возможно это как-то будет развиваться.

**Специфическое по гарантиям:**
- потоки - в текущей задумке - на одного потребителя/задание, это значит:
    - что потребитель модерации Х не блокирует потребителя модерации Y
    - НО внутри одной группы потоки все события обрабатываются последовательно (пока что, м.б. сделаем приоритеты, чтобы, скажем, отменять таски быстрее, чем выдавать новые)

**Ещё про особенности:**
- Янг/Толока - имеют хорошую поддержку в приложении, но не являются ядром, вокруг которого всё работает, следствия:
    - можно делать 1, 2, 3, 10 или 0 заданий в Янге на одно задание (постприёмки и т.п.)
    - можно делать какие-то шаги/операции или задания без Янга/Толоки
    - но, скорее всего, будет несколько сложнее кроссприоритезировать группы или делать что-то сложное со связью янг+задание

Технические мелочи
==================

Непосредственно архитектура
---------------------------
По сути - персистентная actor модель поверх БД.

Немного про структуры данных:
- TaskType (на самом деле нет, потому что это строчка)
- TaskGroup - связка task_type + кто шлёт туда данные + настройки
- Task - это данные одного задания - input, с чем запустили и текущее состояние - как enum-чик и данные, которые надо перенести от одного шага к другому
- TaskEvent - основная штука исполнения, некоторое событие для таска
- TaskResult - 0..N - то, что таск может записывать по мере своей работы, и то, как его результаты забирает вызвавшая система

Как работает:
- единственный способ активации таска - получить event, грубо говоря, в начале получает Init событие
- таск может что-то делать, куда-то ходить, создавать задания в Янге и т.п.
- Янг и прочие штуки крутятся снаружи, в tms-ке и по мере готовности результатов - складывают event-и для тасков, которые надо "оживить"
- при желании, связанные таски так же могут коммуницировать (если одно задание сделало другое и знает его id, или передало ему свой) - хз, какие use-case-ы тут
- задание может послать event самому себе, чтобы разбить работу на несколько атомарных шагов (например, запись биллинга или ещё что-то)
- воздействия снаружи, например, переупорядочивание или отмена таска - так же через события
- есть гарантия, что 1 таск обрабатывает в момент времени только 1 событие
- **нет гарантии, что события обрабатываются строго в порядке поступления** - по умолчанию так, но retry может привести к тому, что событие будет перешедулено на +10 минут и порядок изменится (если вызывает вопросы - давайте обсуждать кейсы/как бороть)

Про шедулинг тасков
-------------------
Принципиально шедулинг тасков можно свести к:
1. как мы приоритезируем - тут ответ просто FIFO (на самом деле по таймстампам), внутри каждой группы тасков
2. как мы работаем с ресурсами - на текущем шаге ответ - наивно, просто делаем всё, что получится, приоритеты групп тасков по CPU рулим количеством обрабатывающих потоков
3. как мы лочим задания, и тут же вопрос, "как отпускаем локи" тут немного теории, как оно бывает:
    - внутри БД - т.е. средствами блокировки БД, локи отпускаются +/- автоматически концом транзакции или коннекта
    - снаружи (переживает смену коннекта к БД) - либо помечая записи, которые забрали в БД (кварц), либо вообще внешними средствами, за локами надо следить, отпускать, сбрасывать, чекинить инстансы, помечать умершие

На текущем этапе плановое решение - трекать локи внутри БД, на уровне транзакций. Следствия/ограничения.
Плюсы:
- это просто в реализации, всё работает примерно само
- по этой же причине это надёжно
- задание работает атомарно, это довольно удобно

Минусы:
- ресурсы, в первую очередь коннекты к БД - нужно коннектов по числу тредов-исполнителей заданий
- время работы задания, эта схема плохо живёт для длинных заданий в десятки минут (для коротких - локи локальные, advisory и не мешают жить)

Альтернативы:
- свой шедулинг с разметкой "взятых" заданий - позволит делать батчинг захвата заданий, отпускать коннекты на длинные операции
    - возможное развитие, сейчас кажется дорогим в реализации
- кварц/базинга - возможно, рабочий вариант, но, скорее всего будет больше оверхед per-задание, плюс я не знаю, как это живёт с тысячами активных триггеров, по времени реализации кажется дольше, можно поисследовать в будущем, если найдём текущий вариант плохим
- очередь вообще где-нибудь "в кустах", например, Redis-е или LogBroker-е - выглядит так, что надо будет довольно таки много всего сделать/проверить

