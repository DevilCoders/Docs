
## Креши и ООМ
Как работает мониторинг крешей и оомов?

Няня предоставляет свои метрики для мониторинга сервисов, креши и оомы - как раз одни из них.
Сервис считается закрешенным, если он был убит сигтермом и отложил корку.
При этом няня примерно раз в минуту смотрит, запущен ли сервис на инстансе, и если нет - рестартит его.
Логи запусков/остановок няней можно посмотреть в файле loop.log.
Логи корок в файле cores.
По корке обычно понятно, какой именно кусок крешится.

Оомы расследовать сложнее. Они могут возникнуть, например, если не учитываем ограничения на размер батча, либо офферы пожирнели, либо мы в коде делаем лишние копирования. Можно попробовать посопоставлять графики оомов с графиками других собыйтий.
Сложный путь решения: выкатить бинарник в profile-сборке и попрофилировать потребление ресурсов.

MaxInflight - кол-во обрабатываемых офферов. Если загорается мониторинг на этот показатель, значит мы не успеваем отправлять изменения, которые к нам поступают. Возможно, что-то замедлилось, можно посмотреть на графиках соломона.
Норма этого показателя ожидаемо меняется в зависимости от нагрузки (например, если нам залбют кучу новых магазинов).

Общие наставления:
Наша цель: восстановить работоспособность сервиса, а уже потом разбираться в проблеме.
Если есть подозрение на релиз - сразу откатываем.
Можно попробовать порестартить сервис: pkill -9 {service} - и посмотреть, что будет. Убить сразу на всех контейнерах можно с помощью sky portorun.
Если дело не в релизе, то обычно проблемы возникают с io-сервисами: логброкером или yt-ем. 
Нужно попробовать локализовать проблему: поотключать топики.
Можно откатить все контейнеры кроме одного, отключить от него другие топики, добавить дебаг.
Если горит все, значит, скорее всего сломалось ядро - yt, например. Нужно решать проблему глобально, смотреть, почему пятисотит ручка строллера, бессмысленно.

## Проблемы с обновлением shopsdat (check-getter-data-freshness)

### Причина 1
Шопсдат приезжает динамическим ресурсом на контейнеры nanny. Если этого не происходит, то скорее всего сломан task [RUN_MARKET_DATA_GETTER](https://sandbox.yandex-team.ru/tasks?children=false&created=14_days&hidden=false&type=RUN_MARKET_DATA_GETTER). Возможные причины:
- task запускается с некорректным бинарем data-getter. Нужно удалить некорректный ресурс [MARKET_SRE_DEB_PACKAGE](https://sandbox.yandex-team.ru/resources?type=MARKET_SRE_DEB_PACKAGE&limit=20&created=14_days) бинаря из sandbox. Перевыкладка [релиза data-getter](https://tsum.yandex-team.ru/pipe/projects/indexer/delivery-dashboard/getter-stages) не помогает (подробное описание в [тикете](https://st.yandex-team.ru/MARKETINDEXER-40013)).

### Причина 2
Один или несколько контейнеров не стартуют из-за железных проблем хоста, висят в статусе **Unknown**. Это приводит к тому, что задача обновления геттера на сервисе останавливается, так как рецепт Няни не позволяет опускать слишком много контейнеров для обновления.
1. Для линковки находим автоматический тикет на починку машины через кнопку WALL-E и переход по ссылке в первой строке дроп-дауна, как правило это очередь ITDC.
2. Если с WALL-E все в порядке и хост в статусе **HOOK_SEMI_FAILED**, то может быть проблема со статусом ISS. Проверить через ./iss_hook_status и остановить через ./iss_hook_stop.
3. Увеличиваем [operating_degrade_level](https://wiki.yandex-team.ru/runtime-cloud/nanny/alemate/yaml-recipes/#operating-degrade-level-desc) и stop_degrade_level в [рецепте сервиса](https://nanny.yandex-team.ru/ui/#/services/catalog/production_market_datacamp_parser_white_sas/recipes). Также можно оперативно увеличить в самой задаче по [инструкции](https://wiki.yandex-team.ru/mbi/development/howto/how-to-decrease-degrade-level-temporary/).

## offers-count-limit-exceeded
Мы начали квотировать поставщиков данных по количеству офферов в стейте хранилища. При превышении квоты загорается мониторинг, а автоматика
переключает эти сервисы в update-only режим. То есть все новые офферы будут роняться на пол. Что же делать? 
1. Пойти к ответственным за сервис и договориться об удалении офферов. Для удаления офферов можно воспользоваться [плохими практиками](market/idx/datacamp/dev/delete_offers/README).
2. Поднять квоту. Текущую квоту и потребление можно посмотреть в сообщении алерта. Для увеличениия квоты редактируем [consts_testing.json](https://a.yandex-team.ru/arc/trunk/arcadia/market/idx/datacamp/routines/tasks/statistics/data/consts_testing.json). **Квота изменится после отработки таски stats-cals с новым релизом рутин** 
3. Можно для конкретного сервиса выключить режим квотирования через ITS piper и scanner (в stroller квотирование не включается). Переменные, отвечающие за включение update only режима, лежат в [inner_state_quota_limitations.cfg](https://a.yandex-team.ru/arc/trunk/arcadia/market/idx/datacamp/controllers/etc/inner_state_quota_limitations.cfg). То есть для игнорирования квотирования маркетных офферов в its достаточно поставить
```
{
  "env_conf": [{
      "file": "env/its.cfg",
      "values": {
        "IGNORE_NEW_OFFERS_BY_QUOTA_FOR_MARKET": "false"
      }
  }]
 }
```

## saas-docs-count
Количество документов в каталожном SaaS превысило квоту, это не означает что новые документы не будут индексироваться. Это скорее предупреждение, что в скором времени SaaS может деградировать. Первым делом следует проверить таблицы [datacamp/united_saas_diff/lostie](https://yt.yandex-team.ru/arnold/navigation?path=//home/market/production/indexer/datacamp/united/saas_diff/lostie). Сюда попадают все документы, которые есть в SaaS, но которых на самом деле быть не должно(например из-за фильтров по цвету, SaaS предназначен только для маркетных офферов). Их можно удалить с помощью тулзы saas-push.

В случае естественного роста надо согласовать с командой SaaS новые квоты и принести железо.

## saas-docs-indexation-lag
SaaS не успевает индексировать документы, которые генерирует хранилище. Следует проверить поток на запись с нашей стороны, [основная дока по подпискам](support/subscription).

На данный мониторинг настроена автоматика, которая включает режим деградации через ITS: триггер на отправку будет срабатывать на ограниченное количество полей.

## promo-saas-docs-count
Аналогично saas-docs-count

## promo-saas-docs-indexation-lag
Аналогично saas-docs-indexation-lag

## data_getter_freshness
Мониторинг следит за свежестью ресурсов облачного data-getter на контейнерах. Мониторинг НЕ следит за свежестью конкретного файла, а лишь проверяет, что sandbox-ресурсы генерируются и приезжают на контейнеры. Подробнее про логику работы можно почитать в [тикете](https://st.yandex-team.ru/MARKETINDEXER-46923).

### Причина 1
Сломан шедулер в sandbox, и новые ресурсы не генерируются. Например, часто ломается генерация одного из ресурсов из всего пакета, и деградирует свежесть всех файлов. Ссылки на шедулеры можно найти на странице самого мониторинга. В логах генератора можно найти проблемный файлик/ресурс.

### Причина 2
Ресурсы генерируются, но не доезжают до контейнеров. Если это новый сервис/тип ресурса, могли забыть и не настроить [релизную интеграцию](https://deploy.yandex-team.ru/docs/concepts/release-integration/release-integration) для ресурса. Но новые сервисы создаются не так часто, поэтому надо посмотреть на сервис в Деплое. Скорее всего, сломался сам сервис и не стартует на новом релизе/ресурсах.

## check_blue_system_feed_freshness
[Код мониторинга](https://a.yandex-team.ru/arc/trunk/arcadia/market/idx/datacamp/routines/lib/monitorings/check_system_feed_freshness.py#L228)

MBI с некоторой периодичностью создает задания на парсинг с обновлением цены оффера, внутри мониторинга проверяется свежесть цены в оффере.

### Проверить лаг в топиках
Проверяем лаги в топиках с заданиями на парсинг и топиках от парсера в piper. Если есть, разбираемся в причинах.

### Проверить задания на парсинг
Если лагов нет, проверяем свежесть цены в оффере и логи парсинга. Если фид давно не парсился, смотрим логфеллер топика datacamp-update-tasks, и идем в чатик MBI с вопросами про отсутствующие задания на парсинг.


## anomalous-yt-dyntables-write-rate
Сработала разладка на поток записи в динамическую таблицу, то есть мы начали писать сильно меньше данных либо же наоборот сильно больше.

1. На [графике](https://nda.ya.ru/t/Hg6XXRE75GYoKS) указываем путь таблицы, где сработала разладка
2. Если по ней можно понять, какой из роботов поменял свой темп записи, идем разбираться с нужным сервисом
3. Если на маркове разладки не заметно, на том же графике выбираем проблемный кластер с разладкой, и копаем дальше. Как вариант, замедлился репликатор для асинхронных реплик. 


## scanner-table-read-lag
Эпспериментальный мониторинг, который следит за лагом чтения таблиц, `read_lag = 0 if table_is_read() else now() - table_creation_ts`. Тут сразу же стоит предупредить, что `table_creation_ts` считается НЕ с момента проставления recent линки на таблицу. Поэтому стоит учитывать, что у части источников возможен моментальный скачек лага.

Первым делом стоит на [дашборде](https://monitoring.yandex-team.ru/projects/market.datacamp/dashboards/mons9kcqril5aej4fg4a), тут возможны следующие варианты:
* Растет лаг по всем ридерам, значит проблема более глобальная, и стоит сразу же смотреть логи сканера
* Растет лаг только в одном ридере:
  * Могли принести слишком большую таблицу, на один шард оптимально иметь 7 млн строк 
  * Деградировали из-за [квотировщика](components/piper#kvotirovanie), стоит смотреть на [yasm дашборд](https://yasm.yandex-team.ru/template/panel/Market_Datacamp_ScannerBlue_stable/)

С помощью [запроса](https://yql.yandex-team.ru/Operations/YuEyqq5OD9vWsY9cK_NQtuTismtowhwSGb2BMUqqfVo=) можно найти хост, который читал таблицу и сколько было попыток.
